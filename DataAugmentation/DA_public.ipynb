{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938fcb43-4b9e-4aeb-90f3-bb862f8e72ca",
   "metadata": {},
   "source": [
    "# Data Augmentation Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f1b6c-da66-488a-a24a-1272eb35e18f",
   "metadata": {},
   "source": [
    "Project for Data Augmentation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f750e-a3bd-495f-bbca-b2e65c492c87",
   "metadata": {},
   "source": [
    "## Data Augmentation Order\n",
    "\n",
    "STEP 1 - Domain Data Preparation\n",
    "1. Domain data labeling check\n",
    "2. Dimensionality Reduction\n",
    "3. Regression analysis\n",
    "\n",
    "\n",
    "STEP 2 - Data Augmentation\n",
    "1. Domain data check\n",
    "02. Public Data Supplement\n",
    "03. Data filtering (1st)\n",
    "04. Dimensionality Reduction\n",
    "05. Label Spreading (semi-supervised learning based)\n",
    "06. Data Filtering (2nd)\n",
    "07. Regression analytsis\n",
    "08. Data Filtering (3rd)\n",
    "09. Data Augmentation\n",
    "10. Model Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0274c5b-defe-43a0-8d24-a70b30719312",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497d4fd-c9e6-4e5e-bf71-131c7297d426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965875d6-0f07-499f-b2aa-8661abe35981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b695956-4d10-46fb-9ec3-7b62968fb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "\n",
    "scaler = MinMaxScaler() #set the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351042d-f129-427a-a321-1f5f3fbcb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam ,RMSprop\n",
    "from keras import  backend as K\n",
    "from keras.optimizers import SGD\n",
    "# from tensorflow.keras import utils as np_utils\n",
    "# from tensorflow.keras.metrics import binary_focal_crossentropy\n",
    "from sklearn import decomposition, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "# from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, Normalizer\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from torch.autograd import Variable\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.callbacks import Callback\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c1d22-f249-4b98-96d8-b6d4f5309be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "884086bf-c312-4af8-bec9-6c903212ae3f",
   "metadata": {},
   "source": [
    "## 01. Domain Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bf23a-4667-4b76-a85c-d0d3d4956a30",
   "metadata": {},
   "source": [
    "* SMC dataset - depression research on 100 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68af2e8-cfe5-4b72-82b4-e5289a652d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HRV numerical dataset\n",
    "domain_ori = pd.read_csv('E:/RESEARCH/Datasets/HRV/HRV_REV_all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb654f-8f3d-4d7d-9c8e-5a044e4523c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data shape, variables check\n",
    "print(\"The shape of the domain dataset is:\",domain_ori.shape)\n",
    "# print(domain.columns)\n",
    "domain_ori.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04607d97-ddd4-43d0-b7c4-887195b17689",
   "metadata": {},
   "source": [
    "* HAMD 점수에 따라서 새롭게 IndexH 라고 라벨링용 변수 만들어주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d28c96-1a8a-478b-8fab-9cfbf47d738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking lables for the data\n",
    "domain_ori.loc[domain_ori['HAMD']<=7, 'IndexH'] = \"normal\" ##healthy control\n",
    "domain_ori.loc[(domain_ori['HAMD']>7) & (domain_ori['HAMD']<=16), 'IndexH'] = \"mild\" ##mild depression\n",
    "domain_ori.loc[domain_ori['HAMD']>16, 'IndexH'] = \"severe\" ##mod-severe depression\n",
    "domain_y = domain_ori.loc[:,'IndexH']\n",
    "# domain_y = domain.loc[:,'disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9e545-62d5-40dc-9edd-925e022de608",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_ori['IndexH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0650eac-e318-4fc1-882b-3e5ae0aa5d62",
   "metadata": {},
   "source": [
    "* 그리고 안쓸 변수들은 제거해주자. (HRV 관련 변수만 쓸 것임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f061d7c-b18c-4d8a-abfe-a28f0dfb2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "### deleting unnecessary data columns\n",
    "domain = domain_ori.drop(['sub','age','gender','VISIT','disorder','HAMD', 'HAMA','PDSS','ASI','APPQ','PSWQ','SPI','PSS','BIS','SSI'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac908b3-ef7f-4fd8-a5bf-133cff76cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check the domain data columns again\n",
    "print(domain.columns)\n",
    "print(domain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb2a97-4144-4464-9128-de68912c32f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31eeba65-1203-4aea-803d-c796a46f8cfa",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597230c-bbf3-4a41-8a34-de6f8d1479fd",
   "metadata": {},
   "source": [
    "* Domain data variable selection for the right task\n",
    "> baseline, stress, rest phase로 나눠진 데이터를 각각 쪼개주는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95402874-c583-46a2-839f-d974d0112bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_b1 = domain.loc[:, ['b1RMSSD', 'b1HR', 'b1PNN50', 'b1VLF', 'b1LF', 'b1HF', 'b1LF/HF']]\n",
    "domain_b2 = domain.loc[:, ['b1RMSSD', 'b1HR', 'b1PNN50', 'b1VLF', 'b1LF', 'b1HF', 'b1LF/HF']]\n",
    "domain_b3 = domain.loc[:, ['b1RMSSD', 'b1HR', 'b1PNN50', 'b1VLF', 'b1LF', 'b1HF', 'b1LF/HF']]\n",
    "domain_b1_index = domain.loc[:, ['b1RMSSD', 'b1HR', 'b1PNN50', 'b1VLF', 'b1LF', 'b1HF', 'b1LF/HF', 'IndexH']]\n",
    "domain_b2_index = domain.loc[:, ['b1RMSSD', 'b1HR', 'b1PNN50', 'b1VLF', 'b1LF', 'b1HF', 'b1LF/HF', 'IndexH']]\n",
    "domain_b3_index = domain.loc[:, ['b1RMSSD', 'b1HR', 'b1PNN50', 'b1VLF', 'b1LF', 'b1HF', 'b1LF/HF', 'IndexH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8f48f-091e-41a9-8ecc-a74b1f95fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_b1.columns = ['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']\n",
    "domain_b2.columns = ['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']\n",
    "domain_b3.columns = ['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662201d-3fb1-4cfc-9126-b1674e1462ae",
   "metadata": {},
   "source": [
    "* domain_s 는 stress phase에 있는 애들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e7a1b-c352-4018-a568-e5fb6988d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_s = domain.loc[:, ['sRMSSD','sHR', 'sPNN50', 'sVLF', 'sLF', 'sHF', 'sLF/HF']]\n",
    "domain_s_index = domain.loc[:, ['sRMSSD','sHR', 'sPNN50', 'sVLF', 'sLF', 'sHF', 'sLF/HF', 'IndexH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7a6de-63cb-4228-a52e-19359ecf1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_s.columns = ['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6895b5c-c0e6-4b6e-9d4b-a28879c1273c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a49693-ad68-44e1-88db-51018958e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardization\n",
    "domain_b1[:] = scaler.fit_transform(domain_b1[:])\n",
    "domain_b2[:] = scaler.fit_transform(domain_b2[:])\n",
    "domain_b3[:] = scaler.fit_transform(domain_b3[:])\n",
    "domain_s[:] = scaler.fit_transform(domain_s[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f083dd-fad7-476a-9ccc-a273378d0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_b1['Index'] = domain_b1_index['IndexH']\n",
    "# domain_b2['Index'] = domain_b2_index['IndexH']\n",
    "# domain_b3['Index'] = domain_b3_index['IndexH']\n",
    "# domain_s['Index'] = domain_s_index['IndexH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6e92f-67b5-4a6a-b7e3-8deb50be72db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9baa0e-a449-40b4-8507-0aaca0e7de97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49d3464-d618-4e24-99ea-2da2321ae32e",
   "metadata": {},
   "source": [
    "Later you can select the dataset that you want to analyze. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2bbf41-5372-42e5-b4ea-4f3873306587",
   "metadata": {},
   "source": [
    "ex) if you want to augment the stress phase dataset, choose domain_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f995a-a054-477f-b1fc-3171d7b13c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8730ac20-7b1a-4ede-8173-ebbae0aba036",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd866b-24cb-41b4-a4db-9dd2352e70e2",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84f59f-4447-43ad-bb06-e00c57a0ce29",
   "metadata": {},
   "source": [
    "## 02. Public Data Supplement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae38185-405e-4b24-a36f-306f0c0ad8e5",
   "metadata": {},
   "source": [
    "* Using public dataset (SWELL-HRV) for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ad1f2-8f4b-404e-9618-0f83b5801f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In our medical case, we adopt HRV dataset from SWEEL HRV research\n",
    "### Using public data must be very careful, and researchers should only use them for training data supplement.\n",
    "\n",
    "public = pd.read_csv('E:/RESEARCH/Datasets/HRV/HRV_Public/SWELL_hrv/data/final/train.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ed78e-ce72-4346-b7e0-cd01af236a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data shape, variables check\n",
    "print(\"The shape of the public SWELL dataset is:\",public.shape)\n",
    "# print(public.columns)\n",
    "public.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfdead3-01c6-4126-9742-8b7f2ba930c0",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee9128-b642-404a-89c1-cd17dfd8e2b0",
   "metadata": {},
   "source": [
    "* preprocess our data to fit into domain data\n",
    "> 실제 사용하는 domain(삼성병원)데이터는 3phase를 가지지만 public에서는 baseline이랑 stress를 나눠본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b171441-8ff0-4965-bff6-1f7cb046edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the variables same as domain dataset\n",
    "public_b = public[public['condition'] == 'no stress']\n",
    "public_s1 = public[public['condition'] == 'interruption']\n",
    "public_s2 = public[public['condition'] == 'time pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16298307-6e60-4c67-8b60-ecb2ae39a61a",
   "metadata": {},
   "source": [
    "* checking the number of baseline and stress phase dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997207-94be-469d-a24f-25bc13e1100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check the number of each phase dataset\n",
    "print(public_b.shape)\n",
    "print(public_s1.shape)\n",
    "print(public_s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6d4b8-e955-4fba-b7ef-63043c617298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now select the common(repeated) variables from the domain data and save\n",
    "public = public.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]\n",
    "public_b = public_b.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]\n",
    "public_s1 = public_s1.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]\n",
    "public_s2 = public_s2.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c61b6-bf5d-493f-85c5-70d56d913621",
   "metadata": {},
   "source": [
    "* 마찬가지로 scaler 적용해서 standardization 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca644b-3242-48c1-ba3b-14e2af8f0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### standardization on supplemented dataset\n",
    "public_b[:] = scaler.fit_transform(public_b[:])\n",
    "public_s1[:] = scaler.fit_transform(public_s1[:])\n",
    "public_s2[:] = scaler.fit_transform(public_s2[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b31e9-efd7-47e6-9d24-797919f50fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### round up the variable values for fifth decimal points\n",
    "public_b = public_b.round(decimals=5)\n",
    "public_s1 = public_s1.round(decimals=5)\n",
    "public_s2 = public_s2.round(decimals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b5442-1503-4f4b-9758-95e47df023bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_b_index = public_b.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]\n",
    "public_s1_index = public_s1.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]\n",
    "public_s2_index = public_s2.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fc3d5-11f5-47b1-98c8-2c5cef2e5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "## put index as 0, for future augmentation prediction value\n",
    "public_b_index['IndexH'] = 0\n",
    "public_s1_index['IndexH'] = 0\n",
    "public_s2_index['IndexH'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857331a-b2ed-4de4-b92a-3d7e7e2cb37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869349c-85c0-436a-8102-82cf93c300b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9998448-46dc-4b91-80f1-9c5b2073b928",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52868f-f63b-4ff8-aed0-a3269d29cc3c",
   "metadata": {},
   "source": [
    "## 03. Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59596e02-ecf4-4d30-b944-41ebc22232f6",
   "metadata": {},
   "source": [
    "### 3-1) Data Mergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c689c2-1134-4e77-9e4d-2dc2a87a3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First select the data phase (maybe not necessary for some dataset)\n",
    "### Then, check the number of data in each domain and public dataset\n",
    "### Here we are going to use baseline phase\n",
    "\n",
    "print(\"Shape of the domain dataset for the training is\", domain_s.shape)\n",
    "print(\"Shape of the public dataset for the training is\", public_s1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f547dec-d05f-4ce9-a299-464df9b27473",
   "metadata": {},
   "outputs": [],
   "source": [
    "### select the proper amount of dataset for each\n",
    "domain_resized = domain_s.sample(frac=1) ##sampling 뽑을거도 없이 전체 다 쓰면 되고.\n",
    "public_resized = public_s1.sample(n=920)\n",
    "print(domain_resized.shape)\n",
    "print(public_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235d8d5-4aa6-4c02-9ec6-02508d372576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_resized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b379d7b-d379-4297-bce0-6a05731aea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_resized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255c0c4-b6c5-461b-8be8-7c7b6f5f0608",
   "metadata": {},
   "source": [
    "* training이라는 이름으로 stress phase에서의 두 데이터를 합치자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d6b5b-9956-4139-935b-ab87e1421e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.concat((domain_resized, public_resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d7600-03d5-47eb-bb84-b4ac14a276f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check the finalized first augmented dataset size/shape\n",
    "print(\"Shape of the firstly augmented dataset for the training is\", training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89721dd0-51ac-4710-bb6d-509191e65419",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6638b-703b-40d7-bae8-f295e4b07d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f25bb-28e8-4524-8840-dc204a192b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1079424e-1603-4750-a0af-f5d448d95bee",
   "metadata": {},
   "source": [
    "## 04. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d17db1-9b50-408e-ba5e-3f3e60617848",
   "metadata": {},
   "source": [
    "* 현재 domain이랑 public에서 사용되는 공용 변수는 7개.('RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF')\n",
    "* 군집화하기 위해서 차원축소를 해도 각 데이터의 설명력이 떨어지지 않는 지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3d1db-f264-4381-9e16-9f8137166e44",
   "metadata": {},
   "source": [
    "* 먼저 Domain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff28f66-1d77-45c1-aa10-4b28bc6d8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To put the labels on domain dataset and use them for labeling, index must be included\n",
    "### 3 component dimensionality reduction on merged dataset\n",
    "dom_pca_3 = decomposition.PCA(n_components=3)\n",
    "dom_pca_3.fit(domain_s.iloc[:])\n",
    "dom_pca_3_result = dom_pca_3.fit_transform(domain_s)\n",
    "dom_pca_3_df = pd.DataFrame(dom_pca_3_result, columns = ['PCA0', 'PCA1', 'PCA2'])\n",
    "dom_3 = dom_pca_3.explained_variance_ratio_.sum()*100 #explained ratio\n",
    "\n",
    "### check the representativeness of the reduced dimension by PCA\n",
    "print('Explained variation per principal component: {}'.format(dom_pca_3.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(np.sum(dom_pca_3.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f1169-ec0d-46f1-a43a-8c0b9e8a564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom_pca_3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fda2c-6365-4ca6-a50e-64be946b59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_s\n",
    "# domain_s_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be4f66-68a6-47f6-9c7c-23e0c66d6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the target info from domain_s dataset\n",
    "dom_pca_3_df['target'] = domain_s_index['IndexH'] ## HAMD를 사용해서 새로 만든 IndexH 라벨에 따른 시각화 비교.\n",
    "# dom_pca_3_df['target'] = domain_ori['disorder'] ## 만약에 HAMD기반해서 새로 만든 라벨 말고 오리지널 disorder를 쓴다면..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776c628-050c-46f4-bb94-0a8b39e1adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_s_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33169712-f5fa-42d5-90a2-a85ed9730333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initialisation\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.title('PCA 3 result from Domain Dataset', fontsize=11)\n",
    "\n",
    "## get the target info from domain_s dataset\n",
    "dom_pca_3_df['target'] = domain_s_index['IndexH']\n",
    "# dom_pca_3_df['target'] = domain_ori['disorder']\n",
    "\n",
    "## seperate by target values\n",
    "dom_pca_0 = dom_pca_3_df[dom_pca_3_df['target']=='normal']\n",
    "dom_pca_1 = dom_pca_3_df[dom_pca_3_df['target']=='mild']\n",
    "dom_pca_2 = dom_pca_3_df[dom_pca_3_df['target']=='severe']\n",
    "\n",
    "ax.scatter(dom_pca_0['PCA0'], dom_pca_0['PCA1'], dom_pca_0['PCA2'], color = 'orange', alpha = 0.7)\n",
    "ax.scatter(dom_pca_1['PCA0'], dom_pca_1['PCA1'], dom_pca_1['PCA2'], color = 'red', alpha = 0.7)\n",
    "ax.scatter(dom_pca_2['PCA0'], dom_pca_2['PCA1'], dom_pca_2['PCA2'], color = 'green', alpha = 0.7)\n",
    "\n",
    "# plt.savefig('pca_result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e88f0f-d5e9-4f2a-bf8b-3f2da5de7e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa55a67-dec2-4b57-bc72-76940624c63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "188b9f00-7ab1-42b5-b398-ddc0d9cad39e",
   "metadata": {},
   "source": [
    "* What if using 2dimensionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e649c5b-ae1e-4416-88dc-2e2bc51250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To put the labels on domain dataset and use them for labeling, index must be included\n",
    "### 3 component dimensionality reduction on merged dataset\n",
    "dom_pca_2 = decomposition.PCA(n_components=2)\n",
    "dom_pca_2.fit(domain_s)\n",
    "dom_pca_2_result = dom_pca_2.fit_transform(domain_s)\n",
    "dom_pca_2_df = pd.DataFrame(dom_pca_2_result, columns = ['PCA0', 'PCA1'])\n",
    "dom_2 = dom_pca_2.explained_variance_ratio_.sum()*100 #explained ratio\n",
    "\n",
    "### check the representativeness of the reduced dimension by PCA\n",
    "print('Explained variation per principal component: {}'.format(dom_pca_2.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(dom_pca_2.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38838587-cdad-467a-a2a3-3a0d9ee5df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class target 정보 불러오기 \n",
    "dom_pca_2_df['target'] = domain_s_index['IndexH']\n",
    "# dom_pca_2_df['target'] = domain_ori['disorder']\n",
    "\n",
    "## seperate by target values\n",
    "dom_pca_0 = dom_pca_2_df[dom_pca_3_df['target']=='normal']\n",
    "dom_pca_1 = dom_pca_2_df[dom_pca_3_df['target']=='mild']\n",
    "dom_pca_2 = dom_pca_2_df[dom_pca_3_df['target']=='severe']\n",
    "\n",
    "# target 별 시각화\n",
    "plt.scatter(dom_pca_0['PCA0'], dom_pca_0['PCA1'], color = 'orange', alpha = 0.7)\n",
    "plt.scatter(dom_pca_1['PCA0'], dom_pca_1['PCA1'], color = 'red', alpha = 0.7)\n",
    "plt.scatter(dom_pca_2['PCA0'], dom_pca_2['PCA1'], color = 'green', alpha = 0.7)\n",
    "\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8510096-318d-4b53-9fdf-a2bc789ad381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ff9ea-7522-475f-aa42-eb8a4ba1a39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b799f-24b1-47b4-bbe6-62173f9604e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42933ee4-843a-449f-b0b5-6616108399cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To put the labels on domain dataset and use them for labeling, index must be included\n",
    "### 3 component dimensionality reduction on merged dataset\n",
    "dom_pca_3 = decomposition.PCA(n_components=3)\n",
    "dom_pca_3_result = dom_pca_3.fit_transform(domain_resized)\n",
    "dom_3 = dom_pca_3.explained_variance_ratio_.sum()*100 #explained ratio\n",
    "\n",
    "### check the representativeness of the reduced dimension by PCA\n",
    "print('Explained variation per principal component: {}'.format(dom_pca_3.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(np.sum(dom_pca_3.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ae719-4f9b-478b-916d-f7501061f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom_pca_3_result\n",
    "dom_pca_3_result.shape ##reduced dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d4e2b9-7dec-4a55-99d9-9b5b83f1cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_resized.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c243cf-0de2-4379-b57d-75cbb4a6d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDATA_reduced = pd.DataFrame(dom_pca_3_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c17a4f-ca62-42f5-ad80-30d1c4ad842e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de269f6-e507-4cd2-aea6-822fa002ffd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad722d0e-731f-4e69-888c-4607b923e15a",
   "metadata": {},
   "source": [
    "* 그림으로 그려서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814071b-cbcf-475b-9174-f4050e58fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_result3 = pd.DataFrame(dom_pca_3.transform(domain_resized), columns = ['PCA%i' % i for i in range(3)], index = domain_resized.index)\n",
    "# dom_result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab84fc-de64-4621-a9e0-c75ab56f9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initialisation\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.title('PCA 3 result from Domain Dataset', fontsize=11, fontweight='bold')\n",
    "ax.scatter(dom_result3['PCA0'], dom_result3['PCA1'], dom_result3['PCA2'])\n",
    "# plt.savefig('pca_result.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecccba-287d-4bc9-9ad3-2ceb6c3ebb91",
   "metadata": {},
   "source": [
    "* Silhouette score를 이용한 분석으로 몇개의 cluster로 나누는 것이 합리적인지 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1f87a-0210-4dc8-b60e-0ddd7ccfd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate values for our number of cluster\n",
    "parameters = [2, 3, 4, 5, 6]\n",
    "\n",
    "# instantiating ParameterGrid, pass number of clusters as input\n",
    "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
    "best_score = -1\n",
    "kmeans_model = KMeans()     # instantiating KMeans model\n",
    "silhouette_scores = []\n",
    "\n",
    "# evaluation based on silhouette_score\n",
    "for p in parameter_grid:\n",
    "    kmeans_model.set_params(**p)  # set current hyper parameter\n",
    "    kmeans_model.fit(domain_resized)     # fit model on dataset, this will find clusters based on parameter p\n",
    "    ss = metrics.silhouette_score(domain_resized, kmeans_model.labels_)   # calculate silhouette_score\n",
    "    silhouette_scores += [ss]       # store all the scores\n",
    "    print('Parameter:', p, 'Score', ss)\n",
    "    # check p which has the best score\n",
    "    if ss > best_score:\n",
    "        best_score = ss\n",
    "        best_grid = p\n",
    "        \n",
    "# plotting silhouette score\n",
    "plt.bar(range(len(silhouette_scores)), list(silhouette_scores), align='center', color='#849ef7', width=0.5)\n",
    "plt.xticks(range(len(silhouette_scores)), list(parameters))\n",
    "plt.title('Domain Dataset silhouette score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1e41a-85ae-4faf-b019-795424f27ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f8f4b-0778-426a-a41a-571c36fe859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5bf71b-6f5a-41cd-be5c-afd13de59cf0",
   "metadata": {},
   "source": [
    "* 다음으로 Public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5da76-1d6a-45ac-b637-890ddf731573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06dda1-5746-4a35-87aa-b0c28b417ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 component dimensionality reduction on merged dataset\n",
    "pub_pca_3 = decomposition.PCA(n_components=3)\n",
    "pub_pca_3_result = pub_pca_3.fit_transform(public_resized)\n",
    "pub_3 = pub_pca_3.explained_variance_ratio_.sum()*100\n",
    "\n",
    "### check the representativeness of the reduced dimension by PCA\n",
    "print('Explained variation per principal component: {}'.format(pub_pca_3.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pub_pca_3.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89782e9e-48d3-411b-8ee0-7b20320d478c",
   "metadata": {},
   "source": [
    "* 마찬가지로 그림으로 그려서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffe69f-99c3-47ea-b8a2-ddffe74c1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_result3 = pd.DataFrame(pub_pca_3.transform(public_resized), columns = ['PCA%i' % i for i in range(3)], index = public_resized.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326b640-359f-455e-bdcc-eb72acc8d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initialisation\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.title('PCA 3 result from Public Dataset', fontsize=11, fontweight='bold')\n",
    "ax.scatter(pub_result3['PCA0'], pub_result3['PCA1'], pub_result3['PCA2'])\n",
    "# plt.savefig('pca_result.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44b2be-be62-4d5a-9acc-16a6aff1bf6b",
   "metadata": {},
   "source": [
    "* Silhouette score to check optimal cluster count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443be35-521b-4f24-b354-dca858d9f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate values for our number of cluster\n",
    "parameters = [2, 3, 4, 5, 6]\n",
    "\n",
    "# instantiating ParameterGrid, pass number of clusters as input\n",
    "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
    "best_score = -1\n",
    "kmeans_model = KMeans()     # instantiating KMeans model\n",
    "silhouette_scores = []\n",
    "\n",
    "# evaluation based on silhouette_score\n",
    "for p in parameter_grid:\n",
    "    kmeans_model.set_params(**p)  # set current hyper parameter\n",
    "    kmeans_model.fit(public_resized)     # fit model on dataset, this will find clusters based on parameter p\n",
    "    ss = metrics.silhouette_score(public_resized, kmeans_model.labels_)   # calculate silhouette_score\n",
    "    silhouette_scores += [ss]       # store all the scores\n",
    "    print('Parameter:', p, 'Score', ss)\n",
    "    # check p which has the best score\n",
    "    if ss > best_score:\n",
    "        best_score = ss\n",
    "        best_grid = p\n",
    "        \n",
    "# plotting silhouette score\n",
    "plt.bar(range(len(silhouette_scores)), list(silhouette_scores), align='center', color='#849ef7', width=0.5)\n",
    "plt.xticks(range(len(silhouette_scores)), list(parameters))\n",
    "plt.title('Public Dataset silhouette score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc33d3-8b19-4311-a30e-7d92321e55ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1be71be-7394-4eb1-a52d-0e10d0868271",
   "metadata": {},
   "source": [
    "* 마지막으로 합쳐진 데이터셋에대한 그림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7295b00-b819-49a2-8c8b-a4cd0f67b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7db125-770a-41a8-bfda-ccd60d26bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 component dimensionality reduction on merged dataset\n",
    "concat_pca_3 = decomposition.PCA(n_components=3)\n",
    "concat_pca_3_result = concat_pca_3.fit_transform(training)\n",
    "concat_3 = concat_pca_3.explained_variance_ratio_.sum()*100\n",
    "\n",
    "### check the representativeness of the reduced dimension by PCA\n",
    "print('Explained variation per principal component: {}'.format(concat_pca_3.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(np.sum(concat_pca_3.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31739c-1157-4c98-ac87-1cbda6b84bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pca_3_result = pd.DataFrame(concat_pca_3.transform(training), columns = ['PCA%i' % i for i in range(3)], index = training.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb0a4d-2489-474a-9b10-79d479b83b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initialisation\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.title('PCA 3 result from Concatenated Dataset', fontsize=11, fontweight='bold')\n",
    "ax.scatter(concat_pca_3_result['PCA0'], concat_pca_3_result['PCA1'], concat_pca_3_result['PCA2'])\n",
    "# plt.savefig('pca_result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d58425-b57a-4896-8e57-998d4906ad64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c56cb-51fd-4df5-a424-c3c6d693fe00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627fc38-9f6f-4d0c-9a93-548a89e6043a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9a46d5a-3621-4b0c-b6cf-d0b9fcb29f60",
   "metadata": {},
   "source": [
    "## 05. Data Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc89da-fb0c-450a-8f90-ae0d573087e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804656c-c8c8-499f-a7e1-192f9cc9bb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f5452-9d79-4c4d-bcd9-0327357c0398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18804fe6-b033-4230-b17f-54c4f1d4243d",
   "metadata": {},
   "source": [
    "## 06. Unlabeled data labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f244c-f1cf-49d6-befa-c9007b431328",
   "metadata": {},
   "source": [
    "* 여기서 RDATA는 Real dataset이고 PDATA는 augmentation을 위한 public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de72f66-6d64-4ebf-bb4c-eebd3a45adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDATA = domain_s\n",
    "PDATA = public_s1.sample(n=1000)\n",
    "label = domain_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a50a2-0c00-48f0-b031-cadc12229cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "798f6f63-02d4-4b40-944e-54381ec7780d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc00e4-30e2-49e9-8172-131277b944a3",
   "metadata": {},
   "source": [
    "* Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96afaa5-b8cf-41de-86a7-0eaa09e32355",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1, R2, R3 = np.split(RDATA, [int(.1*len(RDATA)), int(.5*len(RDATA))])\n",
    "Y1, Y2, Y3 = np.split(label, [int(.1*len(label)), int(.5*len(label))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0444129-dc9f-415a-93f7-1e2b77d2af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984300dc-3284-4037-a183-af9f0b1c01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y12 = np.concatenate((Y1, Y2.apply(lambda x: -1)))\n",
    "R12 = np.concatenate((R1,R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c799dfb-b8a0-49ba-aeb4-201227bb06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9fb29-754c-450e-ac66-d2eddbd1d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Algorithm', 'ROC AUC']\n",
    "results = pd.DataFrame(columns=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5e0df-46ca-4a51-8570-b9b4a5a651c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=710674, class_weight='balanced')\n",
    "logreg.fit(R1, Y1)\n",
    "results = results.append(\n",
    "    pd.Series(['Multiple Logistic Regression', roc_auc_score(Y3, logreg.predict_proba(R3), multi_class='ovr')], \n",
    "                                   index=index), ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0aefb-5bec-4775-80ea-0da215fa6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prop_test(kernel, params_list, X_train, X_test, y_train, y_test):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    n, g = 0, 0\n",
    "    roc_scores = []\n",
    "    if kernel == 'rbf':\n",
    "        for g in params_list:\n",
    "            lp = LabelPropagation(kernel=kernel, n_neighbors=n, gamma=g, max_iter=100000, tol=0.0001)\n",
    "            lp.fit(X_train, y_train)\n",
    "            roc_scores.append(roc_auc_score(y_test, lp.predict_proba(X_test), multi_class='ovr'))\n",
    "    if kernel == 'knn':\n",
    "        for n in params_list:\n",
    "            lp = LabelPropagation(kernel=kernel, n_neighbors=n, gamma=g, max_iter=100000, tol=0.0001)\n",
    "            lp.fit(X_train, y_train)\n",
    "            roc_scores.append(roc_auc_score(y_test, lp.predict_proba(X_test), multi_class='ovr'))\n",
    "    plt.figure(figsize=(16,8));\n",
    "    plt.plot(params_list, roc_scores)\n",
    "    plt.title('Label Propagation ROC AUC with ' + kernel + ' kernel')\n",
    "    plt.show()\n",
    "    print('Best metrics value is at {}'.format(params_list[np.argmax(roc_scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38207c-58c1-4a79-82dd-ae9e37f66f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [9e-6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5, 6e-5, 7e-5, 8e-5, 9e-5]\n",
    "label_prop_test('rbf', gammas, R12, R3, Y12, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff1818-d1a1-4bf2-9147-ab26b4c9a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.arange(50,60)\n",
    "label_prop_test('knn', ns, R12, R3, Y12, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598c3a5-c415-421f-b045-694c5665a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_rbf = LabelPropagation(kernel='rbf', gamma=9e-6, max_iter=100000, tol=0.0001)\n",
    "lp_rbf.fit(R12, Y12)\n",
    "results = results.append(pd.Series(['Label Propagation RBF', \n",
    "                                    roc_auc_score(Y3, lp_rbf.predict_proba(R3), multi_class='ovr')], index=index), ignore_index=True)\n",
    "\n",
    "lp_knn = LabelPropagation(kernel='knn', n_neighbors=52, max_iter=100000, tol=0.0001)\n",
    "lp_knn.fit(R12, Y12)\n",
    "results = results.append(pd.Series(['Label Propagation KNN', \n",
    "                                    roc_auc_score(Y3, lp_knn.predict_proba(R3), multi_class='ovr')], index=index), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3296e7f-4fcb-4453-9bad-9cb5b482c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6656d-c3df-4b54-87e1-c11077534c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_spread_test(kernel, hyperparam, alphas, X_train, X_test, y_train, y_test):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    n, g = 0, 0\n",
    "    roc_scores = []\n",
    "    if kernel == 'rbf':\n",
    "        g = hyperparam\n",
    "    if kernel == 'knn':\n",
    "        n = hyperparam\n",
    "    for alpha in alphas:\n",
    "        ls = LabelSpreading(kernel=kernel, n_neighbors=n, gamma=g, alpha=alpha, max_iter=1000, tol=0.001)\n",
    "        ls.fit(X_train, y_train)\n",
    "        roc_scores.append(roc_auc_score(y_test, ls.predict_proba(X_test), multi_class = 'ovr'))\n",
    "    plt.figure(figsize=(16,8));\n",
    "    plt.plot(alphas, roc_scores);\n",
    "    plt.title('Label Spreading ROC AUC with ' + kernel + ' kernel')\n",
    "    plt.show();\n",
    "    print('Best metrics value is at {}'.format(alphas[np.argmax(roc_scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27ca6b-145f-45be-bded-b866ead4de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  \n",
    "labels_spread_test('rbf', 1e-5, alphas, R12, R3, Y12, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141eefd-11d5-4879-9b1c-5b266b532920",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]  \n",
    "labels_spread_test('knn', 51, alphas, R12, R3, Y12, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60b1fd-f0be-4b17-b76b-9d25f5fa2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rbf = LabelSpreading(kernel='rbf', gamma=9e-6, alpha=0.1, max_iter=1000, tol=0.001)\n",
    "ls_rbf.fit(R12, Y12)\n",
    "results = results.append(pd.Series(['Label Spreading RBF', \n",
    "                                    roc_auc_score(Y3, ls_rbf.predict_proba(R3), multi_class='ovr')], index=index), ignore_index=True)\n",
    "ls_knn = LabelSpreading(kernel='knn', n_neighbors=53, alpha=0.09, max_iter=1000, tol=0.001)\n",
    "ls_knn.fit(R12, Y12)\n",
    "results = results.append(pd.Series(['Label Spreading KNN', \n",
    "                                    roc_auc_score(Y3, ls_knn.predict_proba(R3), multi_class='ovr')], index=index), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23dc356-5dcc-4ec3-94da-cf2c134735a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca19a574-bd3f-4e20-93cd-e00d4fc2e302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae4719-27b4-48df-b9cd-4c1995eca201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18f44d0-9108-4b96-baa7-d5433f9ae5c4",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e25310-ae8e-403a-9c58-dccb797bd042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121a008-6854-4a36-9da8-bcbadb69b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDATA\n",
    "# PDATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79be33-ec06-40f6-b715-e326ae7c0c1a",
   "metadata": {},
   "source": [
    "* 일단 PDATA는 unlabeled data 상태이기에 -1로 라벨값 만들어주고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3eaff1-8a12-48d8-ac8c-06367f715c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDATA['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2e255-e940-452c-b5e2-09efebe29e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDATA.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b65b0-dfb7-4c90-a854-12d5fab6fa40",
   "metadata": {},
   "source": [
    "* Regression 돌리기 위해서 test, train 나눠보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841aeec-93b8-4fb1-9194-3a0578807ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled datapoints and following labels.\n",
    "train_x, test_x, train_y, test_y = train_test_split(RDATA, label, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad33b5-4f3f-4782-8177-a2a1621600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of training dataset x is:\", train_x.shape)\n",
    "print(\"The shape of test dataset x is:\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66f49b-c293-4aad-abf3-0710a972787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlabeled datapoints and following labels.\n",
    "train_x2 = PDATA.loc[:,['RMSSD', 'HR', 'PNN50', 'VLF', 'LF', 'HF', 'LF_HF']]\n",
    "train_y2 = PDATA['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be4dfc-b793-4425-8321-d662737b530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of public training dataset x is:\", train_x2.shape)\n",
    "print(\"The shape of public test dataset x is:\", train_y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82af41f-1e1c-4dda-9f61-a18becda4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "x = np.concatenate((train_x, train_x2))\n",
    "y = np.concatenate((train_y, train_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48334505-6a8d-4f58-abd0-6603dcacfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of Total training dataset x is:\", x.shape)\n",
    "print(\"The shape of Total test dataset x is:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a13630-bbb8-465a-bc53-4f02abc1245f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c833c0d5-6cc6-4ae6-becd-f1e15aade444",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5ec5d-efbc-496d-a9dc-ba2073f2d642",
   "metadata": {},
   "source": [
    "* Logistic regression 돌려서 변수간 연관성 및 함수를 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad0284-f0af-4265-8bfa-1ee548da56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Analysis Method', 'ROC AUC']\n",
    "results = pd.DataFrame(columns = index) ## result 라고 데이터프레임 하나 만들어놓고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235b217-b696-43fa-936a-0a2104d867f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'newton-cg', random_state = 710674, C = 1.0, max_iter = 20000)\n",
    "logreg.fit(train_x, train_y)\n",
    "results = results.append(\n",
    "    pd.Series(['Multiple Logistic Regression', roc_auc_score(test_y, logreg.predict_proba(test_x), multi_class='ovr')],\n",
    "              index=index), ignore_index=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b5348-d464-4de1-acee-3e851ae2623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e621b9-8619-470f-a991-db469861eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(test_x)\n",
    "acc_score = accuracy_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44c673-b31a-4b63-8527-7c646e3c6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d10889-90aa-410b-9eeb-17ad74ab1a80",
   "metadata": {},
   "source": [
    "* 각 라벨별 변수에 대한 계수(coefficient)를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383728f-31c6-4bbc-a29d-fbbbb2d451bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cbd5f-5c4f-42e1-ab9c-080253bc8c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cdd3c-acc1-4027-8189-e68ec518da8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05de05c-3c8d-4ab1-a8da-324b07a1a3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3644ce2-a110-4afc-94a3-79746c073d3f",
   "metadata": {},
   "source": [
    "### Label propagation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a9c9f-7748-4186-a53b-2d05de98dc44",
   "metadata": {},
   "source": [
    "* Label propagation (generating probablistic transition matrix for unlabeled datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eee636-f5bb-4abf-8549-d1d9c7a14661",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDATA = domain_s\n",
    "PDATA = public_s1.sample(n=1000)\n",
    "label = domain_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4063a-9cae-415e-ab64-9628a2768356",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d71834-8c41-4cfb-99e8-587ec6e2b66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8a019-b743-494b-b7b4-d6667520c888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f947973-bb3a-4406-a1eb-57979f23d3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f495032-3bf1-4602-a7ae-8147714f4df1",
   "metadata": {},
   "source": [
    "## 07. Performance comparison (using DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165089f-9677-43eb-bd34-4ab12caac8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=80\n",
    "    bs=64\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes=3\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "# np.random.seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e82bd5-346c-4d97-843a-2a0229d6379b",
   "metadata": {},
   "source": [
    "* domain_b1, domain_b2, domain_b3, domain_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ae498-bc29-4554-8b84-2f7fc5501a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88669abb-1454-4b4c-97e9-c85ed5d78f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_s_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21968a0c-359b-41b0-b471-e7e7aa54b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = domain_s_index.loc[:,['IndexH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ccabe-6b36-4675-9c23-cba4ce171d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data_y\n",
    "label = label.replace({'normal': 0})\n",
    "label = label.replace({'mild': 1})\n",
    "label = label.replace({'severe': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c0bcd-206a-4a23-a76c-51f0499ebb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = domain_s\n",
    "y = to_categorical((label), 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67853ada-7c21-4beb-81d2-b34433a8289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bd785-fabf-49ab-ae9e-97892c587de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of training dataset is:\", x_train.shape[0])\n",
    "print(\"The size of test dataset is:\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29661d-9d35-4615-9585-8efe8344c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((x_train, x_test), axis = 0)\n",
    "targets = np.concatenate((y_train, y_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2f061-1a0f-4c8e-a893-c8e55e5aba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1939c-10fa-4eaa-830a-4281ff7d6d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c39170a2-a106-42c4-b74d-56ea04cc4579",
   "metadata": {},
   "source": [
    "* Applying 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029135df-8158-4042-826f-ff8e21494e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "split_num = 5\n",
    "opt = keras.optimizers.SGD(learning_rate = args.lr, decay = 1e-5, momentum = args.momentum)\n",
    "kfold = KFold(n_splits = split_num, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d41b0-b441-47e1-8108-a4c0f97dc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c348ce-dddb-4bda-be35-f79446b03736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {1:1.8, 2: 2.5 , 0:1.2}\n",
    "# class_weight = {1:1.0, 2:1.5, 0:1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d74e5-7fad-44eb-86f9-9bf06b111dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_fold = []\n",
    "loss_per_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e695b-25e0-414a-a806-2ec7b22c0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in kfold.split(inputs, targets):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim = x_train.shape[1], activation = 'relu'))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    # model.add(Dense(1024, activation = 'relu'))\n",
    "#     model.add(Dropout(0.5)) #drop out\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dense(1024, activation = 'relu')) \n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dense(256, activation = 'relu'))\n",
    "    # model.add(Dense(128, activation = 'relu')) # added\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(args.num_classes, activation = 'softmax'))\n",
    "    \n",
    "    ## model compile\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print(f'Training or fold {fold_num} ... ')\n",
    "    \n",
    "    ## fit data to model\n",
    "    history = model.fit(inputs[train], targets[train], batch_size = args.bs, epochs = args.epochs, verbose = 0, class_weight = class_weight)\n",
    "    \n",
    "    ## generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test])\n",
    "    print(f'Score for fold {fold_num}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    ## increasing fold number\n",
    "    fold_num = fold_num + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "## Summarizing the results\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'>> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'>>> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'>>> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320762c-5eb2-4254-a4c2-4beef962b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train, test in kfold.split(inputs, targets):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(32, input_dim = x_train.shape[1], activation = 'relu'))\n",
    "#     model.add(Dense(64, activation = 'relu'))\n",
    "#     model.add(Dense(64, activation = 'relu'))\n",
    "#     model.add(Dense(64, activation = 'relu'))\n",
    "#     model.add(Dense(32, activation = 'relu'))\n",
    "#     model.add(Dense(args.num_classes, activation = 'softmax'))\n",
    "    \n",
    "#     ## model compile\n",
    "#     model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "#     print('----------------------------------------')\n",
    "#     print(f'Training or fold {fold_num} ... ')\n",
    "    \n",
    "#     ## fit data to model\n",
    "#     history = model.fit(inputs[train], targets[train], batch_size = args.bs, epochs = args.epochs, verbose = 0, class_weight = class_weight)\n",
    "    \n",
    "#     ## generate generalization metrics\n",
    "#     scores = model.evaluate(inputs[test], targets[test])\n",
    "#     print(f'Score for fold {fold_num}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "#     print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "#     acc_per_fold.append(scores[1] * 100)\n",
    "#     loss_per_fold.append(scores[0])\n",
    "    \n",
    "#     ## increasing fold number\n",
    "#     fold_num = fold_num + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "# ## Summarizing the results\n",
    "# print('------------------------------------------------------------------------')\n",
    "# print('Score per fold')\n",
    "# for i in range(0, len(acc_per_fold)):\n",
    "#     print('------------------------------------------------------------------------')\n",
    "#     print(f'>> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "# print('------------------------------------------------------------------------')\n",
    "# print('Average scores for all folds:')\n",
    "# print(f'>>> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "# print(f'>>> Loss: {np.mean(loss_per_fold)}')\n",
    "# print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c34107-36f0-476d-8efe-87cce01ed1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "y_predict = np.argmax(y_predict, axis = 1)\n",
    "y_test = np.argmax(y_test, axis = 1)\n",
    "\n",
    "result = confusion_matrix(y_test, y_predict, normalize = 'pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc7c00-38f8-4311-8c0b-352423095ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predict.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe4dfd-025e-497b-8586-4a74472353d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9645283-0d5a-4c71-ba60-202e6baf262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(result, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3af009-404d-42c1-bda2-978d382b1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "precision = metrics.precision_score(y_test, y_predict, average = 'macro')\n",
    "recall = metrics.recall_score(y_test, y_predict, average = 'micro')\n",
    "f1 = metrics.f1_score(y_test, y_predict, average = 'weighted')\n",
    "\n",
    "print(\"=============================================\")\n",
    "print(\"The overall accuracy is:\", round(accuracy, 4))\n",
    "print(\"The precision score is:\", round(precision, 4))\n",
    "print(\"The recall score is:\", round(recall, 4))\n",
    "print(\"The f1 score is:\", round(f1, 4))\n",
    "print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bd416-a036-466c-a24a-32b3fa54a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
