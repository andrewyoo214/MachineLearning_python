{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbe02d2-f07c-42d0-b8d7-44e074cf1919",
   "metadata": {},
   "source": [
    "# Data Augmentation for Medical Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8bacf-9e16-435f-80a4-52d2a2975e90",
   "metadata": {},
   "source": [
    "> This script is for medical data augmentation. \\\n",
    "> All algorithms are final organized version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337a3c0-e993-4e33-8d8b-88ec3abefa6a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f868f5-8196-43c3-8727-96dd6c8bd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import shap ## for XAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94289e47-e290-45de-8ccd-78fb1d3ca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from keras.layers import Dense , Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam ,RMSprop\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, train_test_split, ParameterGrid\n",
    "from sklearn import decomposition, metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Lambda, Conv1D, Flatten, Reshape, UpSampling1D, MaxPooling1D, concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.losses import mse, MeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a06fd5-2429-47b5-a08c-56b4424fbffa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a6e46-b365-4056-8fc6-81821fb6a031",
   "metadata": {},
   "source": [
    "> We will apply various types/domains medical dataset. \\\n",
    "> It includes public dataset and even own collected medical datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb766456-20cd-4775-8312-09b6b1dd2da8",
   "metadata": {},
   "source": [
    "###  dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be33413-57d8-4f08-90e4-e5077476721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_csv('E:/RESEARCH/Datasets/bio_data/breast_cancer_wisconsin/data.csv') ## Wisconsin breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9603c-7c7b-4cde-b88d-a5bbd65edfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the original dataset is: {data_ori.shape}\")\n",
    "data_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e057401-5dcc-49ee-9822-973d036a942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing_rows = data_ori.isna().any(axis=1).sum()\n",
    "print(f\"The number of rows that contains at least one missing value: {num_missing_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75c66d-4af3-41b0-bd65-9d6b2ea2221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da323b-5b45-48cc-8af3-3c143dfc6549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a2b9ad-b630-4750-90f3-569b1ba3a033",
   "metadata": {},
   "source": [
    "### Unnecessary feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91477bc-28f8-4f10-8364-61e6e80f7f37",
   "metadata": {},
   "source": [
    "> Unnecessary features varies according to the analysis purpose. \\\n",
    "> Researcher should provide research goal or purpose of the augmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ec94e-fc01-442a-a1ed-e3740e461d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Unnamed: 32','id']\n",
    "data_ori_ = data_ori.drop(drop_list, axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a4e67-e9b2-4242-b45b-8963c641dee9",
   "metadata": {},
   "source": [
    "### Scaler on x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4ce02-96e5-4751-892a-d8047892783e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccf6ed-650c-4bfc-8c75-1a47f769cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_only[:] = (scaler.fit_transform(x_only[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6edf97-3b5c-43eb-8194-b571f6a3af09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cf3c4a-69b5-407c-acf9-4ffbaad2b811",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521ea15-8361-4286-90c2-aa615ad5009f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a03db-1c4e-4442-9d07-01a0892f106a",
   "metadata": {},
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd75462-3abe-481f-b094-cf4822e61aa4",
   "metadata": {},
   "source": [
    "## dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d713e98-ed10-430a-991f-d5040682527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ori_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a758f4c-df78-4a97-b408-0094b45ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_B = data[data['diagnosis']==\"B\"]\n",
    "data_M = data[data['diagnosis']==\"M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa73b4-4e9e-4a92-b53a-6b778453f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = data.copy()\n",
    "# data_vae = data_B.copy()\n",
    "# data_vae = data_M.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5723f-0d43-4bcf-a000-14c3fe529ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1da8dd-69d1-4516-ad9a-59df37e69066",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd81d3-df02-446f-b229-fffb9c0eb911",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01046e74-c2be-4fdc-b6de-2b422a45e02b",
   "metadata": {},
   "source": [
    "> Applying scaler on dataset prevents \"nan loss\" issue during VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b66518-9502-4605-806c-e48e18a6dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_vae.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_vae.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595bb3c-e233-4538-8084-ef5592895ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)\n",
    "\n",
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc1c20-e7e3-433f-a993-e01598479bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f7c1f-2bc3-4a19-9a02-3bb3e45d502a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0174eefd-facc-400e-ac44-95b77fb53457",
   "metadata": {},
   "source": [
    "### VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0965f-e92c-4550-b26c-aae6ae989c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    ## VAE model for data augmentation\n",
    "    def build_model(self):\n",
    "        ####################################################\n",
    "        ## defining encoder section\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # shallow model use\n",
    "        h = Dense(16, activation='relu')(inputs)\n",
    "\n",
    "        # # deeper model use\n",
    "        # h = Dense(32, activation='relu')(inputs)\n",
    "        # h = Dense(16, activation='relu')(h)\n",
    "        # h = Dense(8, activation='relu')(h)\n",
    "\n",
    "        # calculating mean/var\n",
    "        z_mean = Dense(self.latent_dim)(h)\n",
    "        z_log_var = Dense(self.latent_dim)(h)\n",
    "\n",
    "        ## sampling section (for gaussian distribution)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        ####################################################\n",
    "        ## defining decoder section\n",
    "\n",
    "        # shallower model use\n",
    "        decoder_h = Dense(16, activation='relu')\n",
    "        decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "        h_decoded = decoder_h(z)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "        \n",
    "        # # deeper model use\n",
    "        # decoder_h1 = Dense(8, activation='relu')\n",
    "        # decoder_h2 = Dense(16, activation='relu')\n",
    "        # decoder_h3 = Dense(32, activation='relu')\n",
    "        # decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "\n",
    "        # h_decoded = decoder_h1(z)\n",
    "        # h_decoded = decoder_h2(h_decoded)\n",
    "        # h_decoded = decoder_h3(h_decoded)\n",
    "        # x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining VAE model\n",
    "        self.vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining loss function (reconstruction + KL divergence)\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var + K.epsilon()), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.vae.add_loss(vae_loss)\n",
    "\n",
    "        ####################################################\n",
    "        ## compiling model\n",
    "        self.vae.compile(optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "        ####################################################\n",
    "        ## Extracting encoder and decoder separately\n",
    "        ## encoder section\n",
    "        self.encoder = Model(inputs, z_mean)\n",
    "\n",
    "        ## decoder section\n",
    "        #### shallow model\n",
    "        decoder_input = Input(shape=(self.latent_dim,))\n",
    "        _h_decoded = decoder_h(decoder_input)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "        \n",
    "        # #### deeper model\n",
    "        # decoder_input = Input(shape=(self.latent_dim,))\n",
    "        # _h_decoded = decoder_h1(decoder_input)\n",
    "        # _h_decoded = decoder_h2(_h_decoded)\n",
    "        # _h_decoded = decoder_h3(_h_decoded)\n",
    "        # _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        # self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    ####################################################\n",
    "    ####################################################\n",
    "    ## Model training\n",
    "    def train(self, data, epochs, batch_size, validation_split):\n",
    "        self.vae.fit(data, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def encode(self, data):\n",
    "        return self.encoder.predict(data)\n",
    "\n",
    "    def decode(self, latent_points):\n",
    "        return self.decoder.predict(latent_points)\n",
    "\n",
    "    def generate_synthetic_data(self, num_samples=1):\n",
    "        latent_samples = np.random.normal(size=(num_samples, self.latent_dim)) ## sampling from latent space\n",
    "        return self.decode(latent_samples) ## synthetic data generation with decoder section\n",
    "\n",
    "    def visualize_latent_space(self, data, labels):\n",
    "        ## encode the dataset into latent space\n",
    "        encoded_data = self.encode(data)\n",
    "        ## latent space visualization with t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=710674)\n",
    "        encoded_data_tsne = tsne.fit_transform(encoded_data)\n",
    "        ## Visualize\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(encoded_data_tsne[:, 0], encoded_data_tsne[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"t-SNE component 1\")\n",
    "        plt.ylabel(\"t-SNE component 2\")\n",
    "        plt.title(\"t-SNE visualization of the latent space\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519795-e972-46dd-aa18-6485cd044e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = VAE(input_dim=data_x.shape[1], latent_dim=4)\n",
    "vae.train(data_x, epochs = 150, batch_size = 32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8c330-9e7d-4339-be91-ca88cde8972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36b183c8-c3f8-4bce-9a15-11b14d797578",
   "metadata": {},
   "source": [
    "#### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e235a2-97e6-477d-a78e-2d0ab44bd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.visualize_latent_space(data_x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4121a-9bdd-4b39-b752-ad3afe3b8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dee2b39-8f2a-41c4-912d-5424bed7f9f7",
   "metadata": {},
   "source": [
    "#### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5426c-d4eb-4ab9-883d-96f33923df28",
   "metadata": {},
   "source": [
    "* For mdd class augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7123e-ea0d-4769-99e2-dc96df52bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_mdd = vae.generate_synthetic_data(num_samples=10)\n",
    "gen_mdd = synthetic_mdd.copy()\n",
    "gen_mdd = pd.DataFrame(gen_mdd, columns = data_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd28762-125c-4921-8230-dc7e679eab43",
   "metadata": {},
   "source": [
    "* For pd class augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c7a7f-6c9c-4442-920a-6e0652087fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_pd = vae.generate_synthetic_data(num_samples=10)\n",
    "gen_pd = synthetic_pd.copy()\n",
    "gen_pd = pd.DataFrame(gen_pd, columns = data_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0874f02-b8c4-422b-aaf3-edae1c828e77",
   "metadata": {},
   "source": [
    "* For con class augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b272a4-1cab-45c3-b1d2-7e9ee2e8e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_con = vae.generate_synthetic_data(num_samples=10)\n",
    "gen_con = synthetic_con.copy()\n",
    "gen_con = pd.DataFrame(gen_con, columns = data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94fe50-7e42-4142-a814-01afea560bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac2fdc-52cc-4238-a42f-e2f708fe5ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946000dd-9bb1-4695-9557-e4e4ec45bc07",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8502d-7ac1-471a-bd42-c49fb6c07cd5",
   "metadata": {},
   "source": [
    "## Conditional Variational Autoencoder (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338ef91-b6b7-4110-9753-317c1e116492",
   "metadata": {},
   "source": [
    "> Advantage of CVAE is adding label or more information to increase explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3359e5-1dc6-462e-8747-68036d0cf353",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9b28b-67b6-4349-85d3-151f39b31ece",
   "metadata": {},
   "source": [
    "> Applying scaler on dataset prevents \"nan loss\" issue during VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560281f-294a-4613-9d4d-6db797001d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042caa1-7f0a-4fd4-b8d3-4ead4df3afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_vae.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_vae.diagnosis\n",
    "# c = data_vae.loc[:,[\"diagnosis\"]]\n",
    "c = data_vae.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e808c9-7fa5-4d69-a9a8-6da6db93ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)\n",
    "\n",
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding\n",
    "data_c = c.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd46fdc-40f8-40e0-a7bf-65b0cefff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)\n",
    "print(\"The size of c dataset is:\", data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed957c6-c08e-43be-8f2b-49b95bdb297a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20cb35d-e150-4967-9e40-1ccd5b2f41e5",
   "metadata": {},
   "source": [
    "### CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d867cef-4c46-4926-a999-921c1380c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE:\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        ## initializing data demensionality\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        ## generating encoder and decoder section\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.cvae = self.build_cvae()\n",
    "    \n",
    "    ## defining encoder function\n",
    "    def build_encoder(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        combined_input = layers.Concatenate()([x_input, c_input])\n",
    "        \n",
    "        h = layers.Dense(16, activation='relu')(combined_input)\n",
    "        # h = layers.Dense(32, activation='relu')(h)\n",
    "        \n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(h)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(h)\n",
    "        \n",
    "        ## sampling (reparameterization)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        z = layers.Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        return Model([x_input, c_input], [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    ## defining decoder function\n",
    "    def build_decoder(self):\n",
    "        z_input = layers.Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        combined_input = layers.Concatenate()([z_input, c_input])\n",
    "        \n",
    "        h = layers.Dense(16, activation='relu')(combined_input)\n",
    "        # h = layers.Dense(64, activation='relu')(h)\n",
    "        x_decoded = layers.Dense(self.input_dim, activation='sigmoid', name='output')(h)\n",
    "        \n",
    "        return Model([z_input, c_input], x_decoded, name='decoder')\n",
    "    \n",
    "    ## build and compile the CVAE model\n",
    "    def build_cvae(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        z_mean, z_log_var, z = self.encoder([x_input, c_input])\n",
    "        x_decoded = self.decoder([z, c_input])\n",
    "        \n",
    "        cvae = Model([x_input, c_input], x_decoded, name='cvae')\n",
    "        \n",
    "        reconstruction_loss = mse(x_input, x_decoded)\n",
    "        reconstruction_loss *= self.input_dim\n",
    "        \n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        cvae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        cvae.add_loss(cvae_loss)\n",
    "        cvae.compile(optimizer='adam')\n",
    "        \n",
    "        return cvae\n",
    "    \n",
    "    ## model training\n",
    "    def train(self, x_data, c_data, epochs, batch_size, validation_split):\n",
    "        self.cvae.fit([x_data, c_data], epochs=epochs, batch_size=batch_size, validation_split = validation_split)\n",
    "    \n",
    "    ## synthetic data generation (for data augmentation)\n",
    "    def generate_synthetic_data(self, condition, n_samples=1):\n",
    "        z_samples = np.random.normal(size=(n_samples, self.latent_dim))\n",
    "        synthetic_data = self.decoder.predict([z_samples, condition])\n",
    "        return synthetic_data\n",
    "        \n",
    "    def visualize_latent_space(self, x_data, c_data, labels, n_samples):\n",
    "        n_samples = min(n_samples, len(x_data), len(c_data))\n",
    "        indices = np.random.choice(len(x_data), n_samples, replace=False) ## data sampling for n\n",
    "        x_sample = x_data.iloc[indices]\n",
    "        c_sample = c_data[indices]\n",
    "        z_mean, _, _ = self.encoder.predict([x_sample, c_sample]) ## calculating latent vector z\n",
    "        tsne = TSNE(n_components=2, random_state=710674) #tsne visualization\n",
    "        z_tsne = tsne.fit_transform(z_mean)\n",
    "\n",
    "        ## visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if labels is not None:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=labels[indices], cmap='viridis')\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1])\n",
    "        plt.title('t-SNE visualization of the latent space')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8304bfd-0864-4e0b-9907-d6b173681020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvae = CVAE(input_dim = data_x.shape[1], condition_dim = data_y.shape[1], latent_dim=2)\n",
    "cvae.train(data_x, data_y, epochs=150, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f9954-ff47-4540-95dd-2b36d47267f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f6a483-cc46-4409-bf2b-663533e530b2",
   "metadata": {},
   "source": [
    "#### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d998d21-df5d-4f35-8ea4-7cf41ea2de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.visualize_latent_space(data_x, data_y, label, n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ffab2-1981-4b86-9d7d-90c7242349a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02c368b-2778-4898-9811-bcd4b716c458",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709c151-d091-44f9-818a-a87b41fd025c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c716af4-99d6-405d-87c1-ab246aa72a2e",
   "metadata": {},
   "source": [
    "# Classification performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b1a65-7ad2-4a26-947f-0fd9614370b8",
   "metadata": {},
   "source": [
    "## Original dataset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b74071-5d41-4b00-8a3f-83bb161e2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dnn = data_ori_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfa146-97d1-4ed7-8623-a5d3d550cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dnn.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_dnn.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf1d28-bd56-4b86-b847-75e5925c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b148481-23f4-4070-b6b5-ac50fd174804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc2359-e87c-4272-9067-4ecc900f1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e61f20-b3f8-4478-a949-01ff9f1a0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0:1, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c53b12-54fa-4916-921a-c986656c83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf06b45-c38d-4970-aa25-14b0db1993f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da47566c-3dfe-48b6-b810-a04809da4ceb",
   "metadata": {},
   "source": [
    "## Augmented dataset add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431a963-6f2b-49e3-aaff-f31eaff1d44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28edc61a-0500-45b9-ab76-22b06a20b11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656d04ba-c534-479a-a64d-5aa6372f9ddf",
   "metadata": {},
   "source": [
    "## Simple DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf53eee-2152-48a2-b32a-72fcb1735300",
   "metadata": {},
   "source": [
    "> To compare classification performance, we design simple structure of DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b03d7-14df-4e9d-9300-0186b78f56e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a094a-9a9d-49ab-8bab-bc9ba25bb7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd8edf5-17b3-4621-b44b-2989de45222d",
   "metadata": {},
   "source": [
    "### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36842d63-1554-4616-a932-a7996e0b5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDNN:\n",
    "    def __init__(self, input_dim, layer_configs, output_units=2, output_activation='softmax'):\n",
    "        self.input_dim = input_dim ## input data dimensionality (# variables)\n",
    "        self.layer_configs = layer_configs ## hidden layer/sequential layer lists (units, activation, batch_norm, dropout_rate)\n",
    "        self.output_units = output_units ## output unit no.\n",
    "        self.output_activation = output_activation ## activation function for output layer\n",
    "        self.model = self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], activation=self.layer_configs[0]['activation'], input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            ## AddLearningRateScheduler callback\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "        \n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_split):\n",
    "        return self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split,\n",
    "                             callbacks=self.callbacks)\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc70963-5d03-4f73-b145-85fc7e2bf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_learning_rate(epoch, mode='cyclic', base_lr=0.001, max_lr=0.006, step_size=2000, gamma=0.99994):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    \n",
    "    if mode == 'cyclic' or mode == 'triangular':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n",
    "    elif mode == 'triangular2':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) / (2 ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) * (gamma ** epoch)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose from 'cyclic', 'triangular', 'triangular2', or 'exp_range'.\")\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3d4c9-3840-4f68-a1ab-b437d121a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = dynamic_learning_rate(epoch=1000, mode='cyclic')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular2')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='exp_range', gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f71e88-1de2-4a55-ae0e-8f264e690ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 16, 'activation': 'relu', 'batch_norm': False, 'dropout_rate': 0.3},\n",
    "    {'units': 8, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2}\n",
    "]\n",
    "\n",
    "model = SimpleDNN(input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a3972-a921-44ab-9678-af5d3a845e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "model.compile_model(optimizer = opt, lr_scheduler=scheduler)\n",
    "\n",
    "## model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "# loss, accuracy = mlp.evaluate_model(x_test, y_test)\n",
    "\n",
    "# 예측\n",
    "# predictions = model.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c3ac7-d734-4e26-a711-00e4d6d4895b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## model training on training dataset\n",
    "history = model.fit_model(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264564c1-fc3d-4908-853f-a674413d7ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f38ffd4-a2ef-41e6-97ae-b7b66527b99a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c23e4-80ba-4b2f-aa19-05967c1f5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_test, y_test):\n",
    "\n",
    "    ## predict on model\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_predict_classes = np.argmax(y_predict, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    ## calculate confusion matrix and visualize\n",
    "    cm = confusion_matrix(y_test_classes, y_predict_classes, normalize='pred')\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, fmt='.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    ## evaluation metrics\n",
    "    accuracy = metrics.accuracy_score(y_test_classes, y_predict_classes)\n",
    "    precision = metrics.precision_score(y_test_classes, y_predict_classes, average='macro')\n",
    "    recall = metrics.recall_score(y_test_classes, y_predict_classes, average='micro')\n",
    "    f1 = metrics.f1_score(y_test_classes, y_predict_classes, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_predict, multi_class='ovr')\n",
    "    \n",
    "    ## Results\n",
    "    print(\"=============================================\")\n",
    "    print(f\"The overall accuracy is: {accuracy:.4f}\")\n",
    "    print(f\"The precision score is: {precision:.4f}\")\n",
    "    print(f\"The recall score is: {recall:.4f}\")\n",
    "    print(f\"The F1 score is: {f1:.4f}\")\n",
    "    print(f\"The AUC score is: {auc:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    ## Print out the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc6ea3-e541-4c4c-93d4-37222ecdaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0423ecf-9cec-472c-981b-59519084ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb36adb-09e5-4889-9477-d68b667b2c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbc26a-36fe-4997-8a6e-e091f1deb9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113854e-76a1-4efb-a2b0-97afb7b17ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
