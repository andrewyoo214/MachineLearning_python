{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbe02d2-f07c-42d0-b8d7-44e074cf1919",
   "metadata": {},
   "source": [
    "# Data Augmentation for Medical Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8bacf-9e16-435f-80a4-52d2a2975e90",
   "metadata": {},
   "source": [
    "> This script is for medical data augmentation. \\\n",
    "> All algorithms are final organized version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337a3c0-e993-4e33-8d8b-88ec3abefa6a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f868f5-8196-43c3-8727-96dd6c8bd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import shap ## for XAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94289e47-e290-45de-8ccd-78fb1d3ca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from keras.layers import Dense , Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam ,RMSprop\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, train_test_split, ParameterGrid\n",
    "from sklearn import decomposition, metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Lambda, Conv1D, Flatten, Reshape, UpSampling1D, MaxPooling1D, concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.losses import mse, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1008403-ca19-4eb7-a7ad-a8306641badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ca588-7405-403e-8124-8de67f1f8eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a06fd5-2429-47b5-a08c-56b4424fbffa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a6e46-b365-4056-8fc6-81821fb6a031",
   "metadata": {},
   "source": [
    "> We will apply various types/domains medical dataset. \\\n",
    "> It includes public dataset and even own collected medical datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb766456-20cd-4775-8312-09b6b1dd2da8",
   "metadata": {},
   "source": [
    "###  dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be33413-57d8-4f08-90e4-e5077476721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_csv('E:/RESEARCH/Datasets/bio_data/breast_cancer_wisconsin/data.csv') ## Wisconsin breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9603c-7c7b-4cde-b88d-a5bbd65edfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the original dataset is: {data_ori.shape}\")\n",
    "data_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e057401-5dcc-49ee-9822-973d036a942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing_rows = data_ori.isna().any(axis=1).sum()\n",
    "print(f\"The number of rows that contains at least one missing value: {num_missing_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75c66d-4af3-41b0-bd65-9d6b2ea2221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da323b-5b45-48cc-8af3-3c143dfc6549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a2b9ad-b630-4750-90f3-569b1ba3a033",
   "metadata": {},
   "source": [
    "### Unnecessary feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91477bc-28f8-4f10-8364-61e6e80f7f37",
   "metadata": {},
   "source": [
    "> Unnecessary features varies according to the analysis purpose. \\\n",
    "> Researcher should provide research goal or purpose of the augmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ec94e-fc01-442a-a1ed-e3740e461d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Unnamed: 32','id']\n",
    "data_ori_ = data_ori.drop(drop_list, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6edf97-3b5c-43eb-8194-b571f6a3af09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cf3c4a-69b5-407c-acf9-4ffbaad2b811",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521ea15-8361-4286-90c2-aa615ad5009f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a03db-1c4e-4442-9d07-01a0892f106a",
   "metadata": {},
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd75462-3abe-481f-b094-cf4822e61aa4",
   "metadata": {},
   "source": [
    "## dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d713e98-ed10-430a-991f-d5040682527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ori_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a758f4c-df78-4a97-b408-0094b45ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_B = data[data['diagnosis']==\"B\"]\n",
    "data_M = data[data['diagnosis']==\"M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa73b4-4e9e-4a92-b53a-6b778453f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = data.copy()\n",
    "# data_vae = data_B.copy()\n",
    "# data_vae = data_M.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5723f-0d43-4bcf-a000-14c3fe529ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1da8dd-69d1-4516-ad9a-59df37e69066",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106f443-4bac-4394-a95e-33ff466f812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_vae:\n",
    "    # arugments\n",
    "    epochs=130\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_vae = Args_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd81d3-df02-446f-b229-fffb9c0eb911",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01046e74-c2be-4fdc-b6de-2b422a45e02b",
   "metadata": {},
   "source": [
    "> Applying scaler on dataset prevents \"nan loss\" issue during VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b66518-9502-4605-806c-e48e18a6dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_vae.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_vae.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595bb3c-e233-4538-8084-ef5592895ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc1c20-e7e3-433f-a993-e01598479bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf50cc5-06b3-40d1-b398-cabe7e66063f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0174eefd-facc-400e-ac44-95b77fb53457",
   "metadata": {},
   "source": [
    "### VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0965f-e92c-4550-b26c-aae6ae989c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    ## VAE model for data augmentation\n",
    "    def build_model(self):\n",
    "        ####################################################\n",
    "        ## defining encoder section\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # shallow model use\n",
    "        h = Dense(16, activation='relu')(inputs)\n",
    "\n",
    "        # deeper model use\n",
    "        # h = Dense(16, activation='relu')(inputs)\n",
    "        # h = Dense(8, activation='relu')(h)\n",
    "        # h = Dense(8, activation='relu')(h)\n",
    "\n",
    "        # calculating mean/var\n",
    "        z_mean = Dense(self.latent_dim)(h)\n",
    "        z_log_var = Dense(self.latent_dim)(h)\n",
    "\n",
    "        ## sampling section (for gaussian distribution)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        ####################################################\n",
    "        ## defining decoder section\n",
    "\n",
    "        # shallower model use\n",
    "        decoder_h = Dense(16, activation='relu')\n",
    "        decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "        h_decoded = decoder_h(z)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "        \n",
    "        # deeper model use\n",
    "        # decoder_h1 = Dense(8, activation='relu')\n",
    "        # decoder_h2 = Dense(16, activation='relu')\n",
    "        # decoder_h3 = Dense(32, activation='relu')\n",
    "        # decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "\n",
    "        # h_decoded = decoder_h1(z)\n",
    "        # h_decoded = decoder_h2(h_decoded)\n",
    "        # h_decoded = decoder_h3(h_decoded)\n",
    "        # x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining VAE model\n",
    "        self.vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining loss function (reconstruction + KL divergence)\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var + K.epsilon()), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.vae.add_loss(vae_loss)\n",
    "\n",
    "        ####################################################\n",
    "        ## compiling model\n",
    "        self.vae.compile(optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "        ####################################################\n",
    "        ## Extracting encoder and decoder separately\n",
    "        ## encoder section\n",
    "        self.encoder = Model(inputs, z_mean)\n",
    "        \n",
    "        ## decoder section\n",
    "        ### shallow model\n",
    "        decoder_input = Input(shape=(self.latent_dim,))\n",
    "        _h_decoded = decoder_h(decoder_input)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "        ### deeper model\n",
    "        # decoder_input = Input(shape=(self.latent_dim,))\n",
    "        # _h_decoded = decoder_h1(decoder_input)\n",
    "        # _h_decoded = decoder_h2(_h_decoded)\n",
    "        # _h_decoded = decoder_h3(_h_decoded)\n",
    "        # _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        # self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    ## Model training\n",
    "    def train(self, data, epochs, batch_size, validation_split):\n",
    "        self.vae.fit(data, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def encode(self, data):\n",
    "        return self.encoder.predict(data)\n",
    "\n",
    "    def decode(self, latent_points):\n",
    "        return self.decoder.predict(latent_points)\n",
    "\n",
    "    def generate_synthetic_data(self, n_samples):\n",
    "        latent_samples = np.random.normal(size=(n_samples, self.latent_dim)) ## sampling from latent space\n",
    "        return self.decode(latent_samples) ## synthetic data generation with decoder section\n",
    "\n",
    "    def visualize_latent_space(self, data, labels):\n",
    "        ## encode the dataset into latent space\n",
    "        encoded_data = self.encode(data)\n",
    "        ## latent space visualization with t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=710674)\n",
    "        encoded_data_tsne = tsne.fit_transform(encoded_data)\n",
    "        ## Visualize\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(encoded_data_tsne[:, 0], encoded_data_tsne[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"t-SNE component 1\")\n",
    "        plt.ylabel(\"t-SNE component 2\")\n",
    "        plt.title(\"t-SNE visualization of the latent space\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519795-e972-46dd-aa18-6485cd044e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = VAE(input_dim=data_x.shape[1], latent_dim=args_vae.latent_dim)\n",
    "vae.train(data_x, epochs = args_vae.epochs, batch_size = args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8c330-9e7d-4339-be91-ca88cde8972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36b183c8-c3f8-4bce-9a15-11b14d797578",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e235a2-97e6-477d-a78e-2d0ab44bd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.visualize_latent_space(data_x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4121a-9bdd-4b39-b752-ad3afe3b8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0861292d-50de-4516-971c-e0e3523beb3e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee2b39-8f2a-41c4-912d-5424bed7f9f7",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465470b-43f5-493a-9c38-270148e64e33",
   "metadata": {},
   "source": [
    "> We use two different synthetic data generation functions.\n",
    "> 1. Adopting single VAE model(trained on all data classes)\n",
    "> 2. Adopting individual VAE model for each data classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dbcd7-9fe9-482a-abf0-7c617ac452c3",
   "metadata": {},
   "source": [
    "#### single VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cc22a-1632-4adb-be4c-4a8b7e36fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_for_classes(vae_model, latent_dim, class_labels, num_samples_per_class):\n",
    "    synthetic_data = {}\n",
    "    for class_label in class_labels:\n",
    "        num_samples = num_samples_per_class[class_label]\n",
    "        ## sampling from latent space\n",
    "        z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "        ## use sampled data to generate synthetic dataset\n",
    "        generated_samples = vae_model.decoder.predict(z_samples)\n",
    "        ## save synthetic dataset with class label\n",
    "        synthetic_data[class_label] = generated_samples\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7835c7-80b2-47c3-949a-5a81f6a1b999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 생성할 클래스 라벨들과 각 클래스별 생성할 샘플 수\n",
    "num_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "# 각 클래스별 synthetic data 생성\n",
    "synthetic_data = generate_synthetic_data_for_classes(vae, args_vae.latent_dim, label, num_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e251b81-d4f1-4c79-97fc-9c95ab0e9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스에 대한 데이터프레임화\n",
    "gen_B = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_M = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "print(f\"The synthetic data size is... \\nSynthetic for B class: {gen_B.shape[0]} \\nSynthetic for M class: {gen_M.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b747742-97e6-4605-af90-b8a93cd7ef9e",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded5443-440f-4ab1-9263-a7243f37b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23cb37c5-29d6-42f3-8e1d-84faf8cf7858",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d65f9d-5b1b-4e78-aa73-78d00749ebf3",
   "metadata": {},
   "source": [
    "#### individual VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce9692-9794-48c4-9dfe-87bcb7f9d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_vae(vae_model, latent_dim, num_samples):\n",
    "    ## sampling from latent space\n",
    "    z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "    ## use sampled latent space vector to generate synthetic dataset\n",
    "    synthetic_data = vae_model.decoder.predict(z_samples)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e516a0-0667-4cde-9b53-b19577cdfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b, data_m = (data_B.drop(\"diagnosis\", axis=1), data_M.drop(\"diagnosis\", axis=1))\n",
    "data_b[:] = (scaler.fit_transform(data_b[:])).round(decimals=6)\n",
    "data_m[:] = (scaler.fit_transform(data_m[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce85933-2807-438b-9832-d8b0937e8df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 클래스 B에 대한 VAE 모델 학습\n",
    "vae_B = VAE(data_b.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_B.train(data_b, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)\n",
    "\n",
    "# 클래스 M에 대한 VAE 모델 학습\n",
    "vae_M = VAE(data_m.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_M.train(data_m, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9a269-5a81-4140-91c7-a82bfea06018",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "num_samples_B = num_samples_per_class[1]\n",
    "synthetic_data_B = generate_class_synthetic_data_vae(vae_B, args_vae.latent_dim, num_samples_B)\n",
    "\n",
    "num_samples_M = num_samples_per_class[0]\n",
    "synthetic_data_M = generate_class_synthetic_data_vae(vae_M, args_vae.latent_dim, num_samples_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abe0d1-b678-4307-b31d-d781681a3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_B = pd.DataFrame(synthetic_data_B, columns=data_x.columns)\n",
    "gen_M = pd.DataFrame(synthetic_data_M, columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37577001-7229-461d-be3c-381df0537343",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d3851-908f-4687-bcfa-e38b4f1a052e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946000dd-9bb1-4695-9557-e4e4ec45bc07",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8502d-7ac1-471a-bd42-c49fb6c07cd5",
   "metadata": {},
   "source": [
    "## Conditional Variational Autoencoder (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338ef91-b6b7-4110-9753-317c1e116492",
   "metadata": {},
   "source": [
    "> Advantage of CVAE is adding label or more information to increase explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4aeda-7bfa-4089-a0b0-ac53e6dc6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_cvae:\n",
    "    # arugments\n",
    "    epochs=130\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_cvae = Args_cvae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3359e5-1dc6-462e-8747-68036d0cf353",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9b28b-67b6-4349-85d3-151f39b31ece",
   "metadata": {},
   "source": [
    "> Applying scaler on dataset prevents \"nan loss\" issue during VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560281f-294a-4613-9d4d-6db797001d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cvae = data_ori_.copy()\n",
    "data_cvae.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042caa1-7f0a-4fd4-b8d3-4ead4df3afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_cvae.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_cvae.diagnosis\n",
    "# c = data_cvae.loc[:,[\"diagnosis\"]]\n",
    "c = data_cvae.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e808c9-7fa5-4d69-a9a8-6da6db93ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding\n",
    "data_c = c.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70248eec-352f-41b7-b88e-040cf39bca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd46fdc-40f8-40e0-a7bf-65b0cefff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)\n",
    "print(\"The size of c dataset is:\", data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed957c6-c08e-43be-8f2b-49b95bdb297a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20cb35d-e150-4967-9e40-1ccd5b2f41e5",
   "metadata": {},
   "source": [
    "### CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d867cef-4c46-4926-a999-921c1380c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE:\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        ## initializing data demensionality\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        ## generating encoder and decoder section\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.cvae = self.build_cvae()\n",
    "    \n",
    "    ## defining encoder function\n",
    "    def build_encoder(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        combined_input = layers.Concatenate()([x_input, c_input])\n",
    "        \n",
    "        h = layers.Dense(16, activation='relu')(combined_input)\n",
    "        # h = layers.Dense(32, activation='relu')(h)\n",
    "        \n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(h)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(h)\n",
    "        \n",
    "        ## sampling (reparameterization)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        z = layers.Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        return Model([x_input, c_input], [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    ## defining decoder function\n",
    "    def build_decoder(self):\n",
    "        z_input = layers.Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        combined_input = layers.Concatenate()([z_input, c_input])\n",
    "        \n",
    "        h = layers.Dense(16, activation='relu')(combined_input)\n",
    "        # h = layers.Dense(64, activation='relu')(h)\n",
    "        x_decoded = layers.Dense(self.input_dim, activation='sigmoid', name='output')(h)\n",
    "        \n",
    "        return Model([z_input, c_input], x_decoded, name='decoder')\n",
    "    \n",
    "    ## build and compile the CVAE model\n",
    "    def build_cvae(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        z_mean, z_log_var, z = self.encoder([x_input, c_input])\n",
    "        x_decoded = self.decoder([z, c_input])\n",
    "        \n",
    "        cvae = Model([x_input, c_input], x_decoded, name='cvae')\n",
    "        \n",
    "        reconstruction_loss = mse(x_input, x_decoded)\n",
    "        reconstruction_loss *= self.input_dim\n",
    "        \n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        cvae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        cvae.add_loss(cvae_loss)\n",
    "        cvae.compile(optimizer='adam')\n",
    "        \n",
    "        return cvae\n",
    "    \n",
    "    ## model training\n",
    "    def train(self, x_data, c_data, epochs, batch_size, validation_split):\n",
    "        self.cvae.fit([x_data, c_data], epochs=epochs, batch_size=batch_size, validation_split = validation_split)\n",
    "    \n",
    "    ## synthetic data generation (for data augmentation)\n",
    "    def generate_synthetic_data(self, condition, n_samples=1):\n",
    "        z_samples = np.random.normal(size=(n_samples, self.latent_dim))\n",
    "        synthetic_data = self.decoder.predict([z_samples, condition])\n",
    "        return synthetic_data\n",
    "        \n",
    "    def visualize_latent_space(self, x_data, c_data, labels, n_samples):\n",
    "        n_samples = min(n_samples, len(x_data), len(c_data))\n",
    "        indices = np.random.choice(len(x_data), n_samples, replace=False) ## data sampling for n\n",
    "        x_sample = x_data.iloc[indices]\n",
    "        c_sample = c_data[indices]\n",
    "        z_mean, _, _ = self.encoder.predict([x_sample, c_sample]) ## calculating latent vector z\n",
    "        tsne = TSNE(n_components=2, random_state=710674) #tsne visualization\n",
    "        z_tsne = tsne.fit_transform(z_mean)\n",
    "\n",
    "        ## visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if labels is not None:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=labels[indices], cmap='viridis')\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1])\n",
    "        plt.title('t-SNE visualization of the latent space')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8304bfd-0864-4e0b-9907-d6b173681020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvae = CVAE(input_dim = data_x.shape[1], condition_dim = data_y.shape[1], latent_dim=args_cvae.latent_dim)\n",
    "cvae.train(data_x, data_y, epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f9954-ff47-4540-95dd-2b36d47267f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f6a483-cc46-4409-bf2b-663533e530b2",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d998d21-df5d-4f35-8ea4-7cf41ea2de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.visualize_latent_space(data_x, data_y, label, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac4b83-44fb-4f9f-89a0-8f94958ebce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b15c46-3725-4edb-96ac-1b375b14c61e",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e445fd4-5c8a-415e-90bb-a03bee57017a",
   "metadata": {},
   "source": [
    "#### single CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25957117-b48c-4422-ac2c-bfd6706a2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data(cvae_model, c_data, n_samples_per_class, labels, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## filtering the condition for each class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "        class_conditions = c_data[class_indices]\n",
    "        \n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            indices = np.random.choice(len(class_conditions), n_samples, replace=True)\n",
    "            class_new_conditions = class_conditions[indices]\n",
    "        ## calculate mean and variance from origianl condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = np.mean(class_conditions, axis=0)\n",
    "            std = np.std(class_conditions, axis=0)\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    # return synthetic_data, new_conditions\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a4aad-7e91-419e-bdfd-0b142902e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "## synthetic data generation (augmentation)\n",
    "synthetic_data = generate_class_synthetic_data(cvae, data_y, n_samples_per_class, label, method='random')\n",
    "# synthetic_data = generate_class_synthetic_data(cvae, data_y, n_samples_per_class, label, method='gaussian')\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0 - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1 - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a5596-5f0c-4a64-a217-4e1bc781b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataframe(synthetic_data, data_x):\n",
    "    dataframes = {}\n",
    "    for class_label, data in synthetic_data.items():\n",
    "        data_copy = data.copy()\n",
    "        df = pd.DataFrame(data_copy, columns=data_x.columns)\n",
    "        dataframes[class_label] = df\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f88ee1-8931-4234-9ec2-eb1dd450a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = synthetic_dataframe(synthetic_data, data_x)\n",
    "\n",
    "# 클래스 0과 클래스 1에 대한 데이터프레임 추출\n",
    "gen_B = dataframes[0]\n",
    "gen_M = dataframes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119eb56b-a07a-42cc-8e6d-6689d597c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed88c5b-9878-4e0c-98bd-4221ed6f2412",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828aeae-e8ca-4fd8-94bb-6afd21c1a0c1",
   "metadata": {},
   "source": [
    "#### individual CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dac422-dc3d-486d-b1a7-2c3822930986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## we should train individual CVAE model for each class\n",
    "cvae_models = {}\n",
    "\n",
    "for class_label in np.unique(label):\n",
    "    ## data filtering to find dataset with that class label\n",
    "    class_indices = np.where(label == class_label)[0]\n",
    "    class_data = data_x.iloc[class_indices]\n",
    "    \n",
    "    ## initialize and train CVAE model\n",
    "    cvae = CVAE(input_dim=data_x.shape[1], latent_dim=args_cvae.latent_dim, condition_dim=data_y.shape[1])\n",
    "    cvae.train(class_data, data_y[class_indices], epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)\n",
    "    \n",
    "    ## save the trained model\n",
    "    cvae_models[class_label] = cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b15a3-8baf-4122-bcaf-dd8ce2712e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_seperately(cvae_models, c_data, n_samples_per_class, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## loading trained CVAE model for each class\n",
    "        cvae_model = cvae_models[class_label]\n",
    "        \n",
    "        ## filtering the condition for the class\n",
    "        class_indices = np.where(label == class_label)[0]\n",
    "        class_conditions = c_data[class_indices]\n",
    "\n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            indices = np.random.choice(len(class_conditions), n_samples, replace=True)\n",
    "            class_new_conditions = class_conditions[indices]\n",
    "        ## calculate mean and variance from origianl condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = np.mean(class_conditions, axis=0)\n",
    "            std = np.std(class_conditions, axis=0)\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    # return synthetic_data, new_conditions\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731dfec-d4ad-45df-bf48-4eb6b620fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_y, n_samples_per_class, method=\"random\")\n",
    "# synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_y, n_samples_per_class, method=\"gaussian\")\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0 - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1 - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb930a4e-2890-4378-ad47-d612d08340a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0b213-4d81-4130-a0dd-12459cdcbed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02c368b-2778-4898-9811-bcd4b716c458",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709c151-d091-44f9-818a-a87b41fd025c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c716af4-99d6-405d-87c1-ab246aa72a2e",
   "metadata": {},
   "source": [
    "# Classification performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b1a65-7ad2-4a26-947f-0fd9614370b8",
   "metadata": {},
   "source": [
    "## Original dataset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b74071-5d41-4b00-8a3f-83bb161e2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dnn = data_ori_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfa146-97d1-4ed7-8623-a5d3d550cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dnn.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_dnn.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf1d28-bd56-4b86-b847-75e5925c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b148481-23f4-4070-b6b5-ac50fd174804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc2359-e87c-4272-9067-4ecc900f1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c1c17-c014-4f39-8682-1f15e330908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## definition for computing class weight (to solve class imbalance issue)\n",
    "def compute_class_weights(y):\n",
    "    class_counts = np.bincount(y) ## calculating data point for each class\n",
    "    total_samples = len(y) ## total data points\n",
    "    ## computing each class weight\n",
    "    class_weights = {i: total_samples / (len(class_counts) * class_count) \n",
    "                     for i, class_count in enumerate(class_counts)}\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596145a-e853-45aa-9efa-84c199748154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weights(label)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf06b45-c38d-4970-aa25-14b0db1993f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da47566c-3dfe-48b6-b810-a04809da4ceb",
   "metadata": {},
   "source": [
    "## Augmented dataset add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39a1af-6662-42a5-b5f1-de5a52427c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cvae_data_preparation:\n",
    "    def __init__(self, original_data, gen_data, label_col, id_cols=None, drop_cols=None):\n",
    "        \"\"\"\n",
    "        original_data (pandas.DataFrame): original dataset\n",
    "        gen_data (dict): data dictionary for synthetic dataset (ex: {0: gen_A, 1: gen_B})\n",
    "        label_col (str): label column (target variable)\n",
    "        id_cols (list): column list to drop (예: ['Unnamed: 32', 'id'])\n",
    "        drop_cols (list): column list to drop from original dataset\n",
    "        \"\"\"\n",
    "        self.original_data = original_data\n",
    "        self.gen_data = gen_data\n",
    "        self.label_col = label_col\n",
    "        self.id_cols = id_cols if id_cols is not None else []\n",
    "        self.drop_cols = drop_cols if drop_cols is not None else []\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        ## preprocessing original dataset\n",
    "        data_dnn = self.original_data.copy()\n",
    "        for col in self.id_cols:\n",
    "            if col in data_dnn.columns:\n",
    "                data_dnn = data_dnn.drop(col, axis=1)\n",
    "        \n",
    "        data_dnn = data_dnn.drop(self.drop_cols, axis=1)\n",
    "        data_classes = {label: data_dnn[data_dnn[self.label_col] == label] for label in data_dnn[self.label_col].unique()}\n",
    "        \n",
    "        ## preprocessing synthetic generated dataset\n",
    "        for label, df in self.gen_data.items():\n",
    "            df[self.label_col] = label\n",
    "            if self.drop_cols:\n",
    "                df = df.drop(self.drop_cols, axis=1)\n",
    "            self.gen_data[label] = df\n",
    "        \n",
    "        ## concat original dataset and generated dataset\n",
    "        ori_df_concat = pd.concat(list(data_classes.values()), ignore_index=True)\n",
    "        gen_df_concat = pd.concat(list(self.gen_data.values()), ignore_index=True)\n",
    "        \n",
    "        ## separating feature and label(target)\n",
    "        ori_x = ori_df_concat.drop([self.label_col], axis=1)\n",
    "        ori_y = ori_df_concat[[self.label_col]]\n",
    "        \n",
    "        gen_x = gen_df_concat.drop([self.label_col], axis=1)\n",
    "        gen_y = gen_df_concat[[self.label_col]]\n",
    "        \n",
    "        ## filling missing values and scaling\n",
    "        ori_x = ori_x.fillna(ori_x.mean())\n",
    "        ori_x[:] = (scaler.fit_transform(ori_x[:])).round(decimals=6) ## scaling x values\n",
    "        \n",
    "        # 레이블 인코딩 및 원-핫 인코딩\n",
    "        ori_y = ori_y.replace({self.label_col: {label: idx for idx, label in enumerate(ori_y[self.label_col].unique())}})\n",
    "        y_ori = to_categorical(ori_y, num_classes=len(ori_y[self.label_col].unique()))\n",
    "        \n",
    "        gen_y = gen_y.replace({self.label_col: {label: idx for idx, label in enumerate(gen_y[self.label_col].unique())}})\n",
    "        y_gen = to_categorical(gen_y, num_classes=len(gen_y[self.label_col].unique()))\n",
    "        \n",
    "        ## separating training and test dataset\n",
    "        x_trainset, x_test, y_trainset, y_test = train_test_split(ori_x, y_ori, test_size=0.35, random_state=710674)\n",
    "        \n",
    "        ## add generated dataset to training dataset\n",
    "        x_train_concat = pd.concat([x_trainset, gen_x], ignore_index=True)\n",
    "        y_train_concat = np.concatenate([y_trainset, y_gen])\n",
    "        \n",
    "        ## separating training dataset and validation dataset\n",
    "        x_train, x_vali, y_train, y_vali = train_test_split(x_train_concat, y_train_concat, test_size=0.2, random_state=710674)\n",
    "        \n",
    "        return x_train, x_vali, x_test, y_train, y_vali, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028bb49-b690-4e65-b597-22f94d7d1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a dictionary for generated dataset\n",
    "gen_data = {\"B\": gen_B, \"M\": gen_M }\n",
    "\n",
    "## data_preparation class initialize and data prepare\n",
    "data_prep = cvae_data_preparation(data_ori, gen_data, label_col='diagnosis', id_cols=['Unnamed: 32', 'id'])\n",
    "x_train, x_vali, x_test, y_train, y_vali, y_test = data_prep.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c21197-c2a6-42f1-b6f3-78f0544f09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The final datset size is... \\nTraining dataset: {x_train.shape[0]} \\nValidation dataset: {x_vali.shape[0]} \\nTest dataset: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea230a58-039b-463c-b96f-898074d86474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656d04ba-c534-479a-a64d-5aa6372f9ddf",
   "metadata": {},
   "source": [
    "## Simple DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf53eee-2152-48a2-b32a-72fcb1735300",
   "metadata": {},
   "source": [
    "> To compare classification performance, we design simple structure of DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b03d7-14df-4e9d-9300-0186b78f56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_dnn:\n",
    "    # arugments\n",
    "    epochs=30\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    seed=710674\n",
    "\n",
    "args_dnn = Args_dnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a465b-88c6-422b-9bb9-51da409a6ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd8edf5-17b3-4621-b44b-2989de45222d",
   "metadata": {},
   "source": [
    "### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36842d63-1554-4616-a932-a7996e0b5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDNN:\n",
    "    def __init__(self, input_dim, layer_configs, output_units=2, output_activation='softmax'):\n",
    "        self.input_dim = input_dim ## input data dimensionality (# variables)\n",
    "        self.layer_configs = layer_configs ## hidden layer/sequential layer lists (units, activation, batch_norm, dropout_rate)\n",
    "        self.output_units = output_units ## output unit no.\n",
    "        self.output_activation = output_activation ## activation function for output layer\n",
    "        self.model = self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], activation=self.layer_configs[0]['activation'], input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            ## AddLearningRateScheduler callback\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "        \n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_split=None, class_weight=None, validation_data=None):\n",
    "        return self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split,\n",
    "                             class_weight=class_weight, validation_data = validation_data, callbacks=self.callbacks)\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc70963-5d03-4f73-b145-85fc7e2bf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_learning_rate(epoch, mode='cyclic', base_lr=0.001, max_lr=0.006, step_size=2000, gamma=0.99994):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    \n",
    "    if mode == 'cyclic' or mode == 'triangular':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n",
    "    elif mode == 'triangular2':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) / (2 ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) * (gamma ** epoch)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose from 'cyclic', 'triangular', 'triangular2', or 'exp_range'.\")\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3d4c9-3840-4f68-a1ab-b437d121a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = dynamic_learning_rate(epoch=1000, mode='cyclic')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular2')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='exp_range', gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f71e88-1de2-4a55-ae0e-8f264e690ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    # {'units': 32, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 16, 'activation': 'relu', 'batch_norm': False, 'dropout_rate': 0.3},\n",
    "    {'units': 8, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2}\n",
    "]\n",
    "\n",
    "model = SimpleDNN(input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a3972-a921-44ab-9678-af5d3a845e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "model.compile_model(optimizer = opt, lr_scheduler=scheduler)\n",
    "\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c3ac7-d734-4e26-a711-00e4d6d4895b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## model training on training dataset\n",
    "# history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, validation_split=0.2)\n",
    "# history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, validation_data=(x_vali, y_vali))\n",
    "history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, class_weight = class_weight, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264564c1-fc3d-4908-853f-a674413d7ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f38ffd4-a2ef-41e6-97ae-b7b66527b99a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c23e4-80ba-4b2f-aa19-05967c1f5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_test, y_test):\n",
    "\n",
    "    ## predict on model\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_predict_classes = np.argmax(y_predict, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    ## calculate confusion matrix and visualize\n",
    "    cm = confusion_matrix(y_test_classes, y_predict_classes, normalize='pred')\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, fmt='.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    ## evaluation metrics\n",
    "    accuracy = metrics.accuracy_score(y_test_classes, y_predict_classes)\n",
    "    precision = metrics.precision_score(y_test_classes, y_predict_classes, average='macro')\n",
    "    recall = metrics.recall_score(y_test_classes, y_predict_classes, average='micro')\n",
    "    f1 = metrics.f1_score(y_test_classes, y_predict_classes, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_predict, multi_class='ovr')\n",
    "    \n",
    "    ## Results\n",
    "    print(\"=============================================\")\n",
    "    print(f\"The overall accuracy is: {accuracy:.4f}\")\n",
    "    print(f\"The precision score is: {precision:.4f}\")\n",
    "    print(f\"The recall score is: {recall:.4f}\")\n",
    "    print(f\"The F1 score is: {f1:.4f}\")\n",
    "    print(f\"The AUC score is: {auc:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    ## Print out the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc6ea3-e541-4c4c-93d4-37222ecdaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0423ecf-9cec-472c-981b-59519084ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb36adb-09e5-4889-9477-d68b667b2c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbc26a-36fe-4997-8a6e-e091f1deb9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
