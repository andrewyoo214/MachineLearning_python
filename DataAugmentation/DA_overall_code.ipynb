{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbe02d2-f07c-42d0-b8d7-44e074cf1919",
   "metadata": {},
   "source": [
    "# Data Augmentation for Medical Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8bacf-9e16-435f-80a4-52d2a2975e90",
   "metadata": {},
   "source": [
    "> This script is for medical data augmentation. \\\n",
    "> All algorithms are final organized version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337a3c0-e993-4e33-8d8b-88ec3abefa6a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f868f5-8196-43c3-8727-96dd6c8bd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import shap ## for XAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94289e47-e290-45de-8ccd-78fb1d3ca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from keras.layers import Dense , Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam ,RMSprop\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, train_test_split, ParameterGrid\n",
    "from sklearn import decomposition, metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Lambda, Conv1D, Flatten, Reshape, UpSampling1D, MaxPooling1D, concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.losses import mse, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1008403-ca19-4eb7-a7ad-a8306641badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)\n",
    "# scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ca588-7405-403e-8124-8de67f1f8eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a06fd5-2429-47b5-a08c-56b4424fbffa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a6e46-b365-4056-8fc6-81821fb6a031",
   "metadata": {},
   "source": [
    "> We will apply various types/domains medical dataset. \\\n",
    "> It includes public dataset and even own collected medical datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb766456-20cd-4775-8312-09b6b1dd2da8",
   "metadata": {},
   "source": [
    "###  dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3be33413-57d8-4f08-90e4-e5077476721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_csv('E:/RESEARCH/Datasets/bio_data/breast_cancer_wisconsin/data.csv') ## Wisconsin breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0f9603c-7c7b-4cde-b88d-a5bbd65edfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the original dataset is: (569, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The shape of the original dataset is: {data_ori.shape}\")\n",
    "data_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edd72eaf-0729-4616-8d44-af16bff35db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e057401-5dcc-49ee-9822-973d036a942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows that contains at least one missing value: 569\n"
     ]
    }
   ],
   "source": [
    "num_missing_rows = data_ori.isna().any(axis=1).sum()\n",
    "print(f\"The number of rows that contains at least one missing value: {num_missing_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b75c66d-4af3-41b0-bd65-9d6b2ea2221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis\n",
       "B    357\n",
       "M    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ori['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da323b-5b45-48cc-8af3-3c143dfc6549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a2b9ad-b630-4750-90f3-569b1ba3a033",
   "metadata": {},
   "source": [
    "### Unnecessary feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91477bc-28f8-4f10-8364-61e6e80f7f37",
   "metadata": {},
   "source": [
    "> Unnecessary features varies according to the analysis purpose. \\\n",
    "> Researcher should provide research goal or purpose of the augmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba7ec94e-fc01-442a-a1ed-e3740e461d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Unnamed: 32','id']\n",
    "data_ori = data_ori.drop(drop_list, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a758f4c-df78-4a97-b408-0094b45ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_B = data_ori[data_ori['diagnosis']==\"B\"]\n",
    "data_M = data_ori[data_ori['diagnosis']==\"M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "34878d6f-342c-42b8-80da-4e83e914a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_B = data_B.sample(n=350, random_state=710674)\n",
    "sample_M = data_M.sample(n=30, random_state=710674)\n",
    "\n",
    "# 두 샘플을 합쳐서 최종 데이터프레임 구성\n",
    "data_sampled = pd.concat([sample_B, sample_M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6edf97-3b5c-43eb-8194-b571f6a3af09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cf3c4a-69b5-407c-acf9-4ffbaad2b811",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521ea15-8361-4286-90c2-aa615ad5009f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a03db-1c4e-4442-9d07-01a0892f106a",
   "metadata": {},
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd75462-3abe-481f-b094-cf4822e61aa4",
   "metadata": {},
   "source": [
    "## dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d713e98-ed10-430a-991f-d5040682527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccaa73b4-4e9e-4a92-b53a-6b778453f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = data.copy()\n",
    "# data_vae = data_B.copy()\n",
    "# data_vae = data_M.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5723f-0d43-4bcf-a000-14c3fe529ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1da8dd-69d1-4516-ad9a-59df37e69066",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6106f443-4bac-4394-a95e-33ff466f812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_vae:\n",
    "    # arugments\n",
    "    epochs=130\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_vae = Args_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd81d3-df02-446f-b229-fffb9c0eb911",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01046e74-c2be-4fdc-b6de-2b422a45e02b",
   "metadata": {},
   "source": [
    "> Applying scaler on dataset prevents \"nan loss\" issue during VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83b66518-9502-4605-806c-e48e18a6dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_vae.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_vae.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2595bb3c-e233-4538-8084-ef5592895ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72cc1c20-e7e3-433f-a993-e01598479bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of x dataset is: (569, 30)\n",
      "The size of y dataset is: (569, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf50cc5-06b3-40d1-b398-cabe7e66063f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0174eefd-facc-400e-ac44-95b77fb53457",
   "metadata": {},
   "source": [
    "### VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0965f-e92c-4550-b26c-aae6ae989c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    ## VAE model for data augmentation\n",
    "    def build_model(self):\n",
    "        ####################################################\n",
    "        ## defining encoder section\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # shallow model use\n",
    "        h = Dense(16, activation='relu')(inputs)\n",
    "\n",
    "        # deeper model use\n",
    "        # h = Dense(16, activation='relu')(inputs)\n",
    "        # h = Dense(8, activation='relu')(h)\n",
    "        # h = Dense(8, activation='relu')(h)\n",
    "\n",
    "        # calculating mean/var\n",
    "        z_mean = Dense(self.latent_dim)(h)\n",
    "        z_log_var = Dense(self.latent_dim)(h)\n",
    "\n",
    "        ## sampling section (for gaussian distribution)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        ####################################################\n",
    "        ## defining decoder section\n",
    "\n",
    "        # shallower model use\n",
    "        decoder_h = Dense(16, activation='relu')\n",
    "        decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "        h_decoded = decoder_h(z)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "        \n",
    "        # deeper model use\n",
    "        # decoder_h1 = Dense(8, activation='relu')\n",
    "        # decoder_h2 = Dense(16, activation='relu')\n",
    "        # decoder_h3 = Dense(32, activation='relu')\n",
    "        # decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "\n",
    "        # h_decoded = decoder_h1(z)\n",
    "        # h_decoded = decoder_h2(h_decoded)\n",
    "        # h_decoded = decoder_h3(h_decoded)\n",
    "        # x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining VAE model\n",
    "        self.vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining loss function (reconstruction + KL divergence)\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var + K.epsilon()), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.vae.add_loss(vae_loss)\n",
    "\n",
    "        ####################################################\n",
    "        ## compiling model\n",
    "        self.vae.compile(optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "        ####################################################\n",
    "        ## Extracting encoder and decoder separately\n",
    "        ## encoder section\n",
    "        self.encoder = Model(inputs, z_mean)\n",
    "        \n",
    "        ## decoder section\n",
    "        ### shallow model\n",
    "        decoder_input = Input(shape=(self.latent_dim,))\n",
    "        _h_decoded = decoder_h(decoder_input)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "        ### deeper model\n",
    "        # decoder_input = Input(shape=(self.latent_dim,))\n",
    "        # _h_decoded = decoder_h1(decoder_input)\n",
    "        # _h_decoded = decoder_h2(_h_decoded)\n",
    "        # _h_decoded = decoder_h3(_h_decoded)\n",
    "        # _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        # self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    ## Model training\n",
    "    def train(self, data, epochs, batch_size, validation_split):\n",
    "        self.vae.fit(data, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def encode(self, data):\n",
    "        return self.encoder.predict(data)\n",
    "\n",
    "    def decode(self, latent_points):\n",
    "        return self.decoder.predict(latent_points)\n",
    "\n",
    "    def generate_synthetic_data(self, n_samples):\n",
    "        latent_samples = np.random.normal(size=(n_samples, self.latent_dim)) ## sampling from latent space\n",
    "        return self.decode(latent_samples) ## synthetic data generation with decoder section\n",
    "\n",
    "    def visualize_latent_space(self, data, labels):\n",
    "        ## encode the dataset into latent space\n",
    "        encoded_data = self.encode(data)\n",
    "        ## latent space visualization with t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=710674)\n",
    "        encoded_data_tsne = tsne.fit_transform(encoded_data)\n",
    "        ## Visualize\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(encoded_data_tsne[:, 0], encoded_data_tsne[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"t-SNE component 1\")\n",
    "        plt.ylabel(\"t-SNE component 2\")\n",
    "        plt.title(\"t-SNE visualization of the latent space\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519795-e972-46dd-aa18-6485cd044e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = VAE(input_dim=data_x.shape[1], latent_dim=args_vae.latent_dim)\n",
    "vae.train(data_x, epochs = args_vae.epochs, batch_size = args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8c330-9e7d-4339-be91-ca88cde8972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36b183c8-c3f8-4bce-9a15-11b14d797578",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e235a2-97e6-477d-a78e-2d0ab44bd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.visualize_latent_space(data_x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4121a-9bdd-4b39-b752-ad3afe3b8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0861292d-50de-4516-971c-e0e3523beb3e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee2b39-8f2a-41c4-912d-5424bed7f9f7",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465470b-43f5-493a-9c38-270148e64e33",
   "metadata": {},
   "source": [
    "> We use two different synthetic data generation functions.\n",
    "> 1. Adopting single VAE model(trained on all data classes)\n",
    "> 2. Adopting individual VAE model for each data classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dbcd7-9fe9-482a-abf0-7c617ac452c3",
   "metadata": {},
   "source": [
    "#### single VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cc22a-1632-4adb-be4c-4a8b7e36fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_for_classes(vae_model, latent_dim, class_labels, num_samples_per_class):\n",
    "    synthetic_data = {}\n",
    "    for class_label in class_labels:\n",
    "        num_samples = num_samples_per_class[class_label]\n",
    "        ## sampling from latent space\n",
    "        z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "        ## use sampled data to generate synthetic dataset\n",
    "        generated_samples = vae_model.decoder.predict(z_samples)\n",
    "        ## save synthetic dataset with class label\n",
    "        synthetic_data[class_label] = generated_samples\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7835c7-80b2-47c3-949a-5a81f6a1b999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 생성할 클래스 라벨들과 각 클래스별 생성할 샘플 수\n",
    "num_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "# 각 클래스별 synthetic data 생성\n",
    "synthetic_data = generate_synthetic_data_for_classes(vae, args_vae.latent_dim, label, num_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e251b81-d4f1-4c79-97fc-9c95ab0e9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스에 대한 데이터프레임화\n",
    "gen_B = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_M = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "print(f\"The synthetic data size is... \\n\\nSynthetic for B class: {gen_B.shape[0]} \\nSynthetic for M class: {gen_M.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b747742-97e6-4605-af90-b8a93cd7ef9e",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded5443-440f-4ab1-9263-a7243f37b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23cb37c5-29d6-42f3-8e1d-84faf8cf7858",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d65f9d-5b1b-4e78-aa73-78d00749ebf3",
   "metadata": {},
   "source": [
    "#### individual VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce9692-9794-48c4-9dfe-87bcb7f9d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_vae(vae_model, latent_dim, num_samples):\n",
    "    ## sampling from latent space\n",
    "    z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "    ## use sampled latent space vector to generate synthetic dataset\n",
    "    synthetic_data = vae_model.decoder.predict(z_samples)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e516a0-0667-4cde-9b53-b19577cdfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b, data_m = (data_B.drop(\"diagnosis\", axis=1), data_M.drop(\"diagnosis\", axis=1))\n",
    "data_b[:] = (scaler.fit_transform(data_b[:])).round(decimals=6)\n",
    "data_m[:] = (scaler.fit_transform(data_m[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce85933-2807-438b-9832-d8b0937e8df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 클래스 B에 대한 VAE 모델 학습\n",
    "vae_B = VAE(data_b.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_B.train(data_b, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)\n",
    "\n",
    "# 클래스 M에 대한 VAE 모델 학습\n",
    "vae_M = VAE(data_m.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_M.train(data_m, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9a269-5a81-4140-91c7-a82bfea06018",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "num_samples_B = num_samples_per_class[1]\n",
    "synthetic_data_B = generate_class_synthetic_data_vae(vae_B, args_vae.latent_dim, num_samples_B)\n",
    "\n",
    "num_samples_M = num_samples_per_class[0]\n",
    "synthetic_data_M = generate_class_synthetic_data_vae(vae_M, args_vae.latent_dim, num_samples_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abe0d1-b678-4307-b31d-d781681a3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_B = pd.DataFrame(synthetic_data_B, columns=data_x.columns)\n",
    "gen_M = pd.DataFrame(synthetic_data_M, columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37577001-7229-461d-be3c-381df0537343",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d3851-908f-4687-bcfa-e38b4f1a052e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946000dd-9bb1-4695-9557-e4e4ec45bc07",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8502d-7ac1-471a-bd42-c49fb6c07cd5",
   "metadata": {},
   "source": [
    "## Conditional Variational Autoencoder (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338ef91-b6b7-4110-9753-317c1e116492",
   "metadata": {},
   "source": [
    "> Advantage of CVAE is adding label or more information to increase explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4aeda-7bfa-4089-a0b0-ac53e6dc6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_cvae:\n",
    "    # arugments\n",
    "    epochs=130\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_cvae = Args_cvae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3359e5-1dc6-462e-8747-68036d0cf353",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9b28b-67b6-4349-85d3-151f39b31ece",
   "metadata": {},
   "source": [
    "> Applying scaler on dataset prevents \"nan loss\" issue during VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560281f-294a-4613-9d4d-6db797001d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cvae = data_ori_.copy()\n",
    "data_cvae.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042caa1-7f0a-4fd4-b8d3-4ead4df3afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_cvae.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_cvae.diagnosis\n",
    "# c = data_cvae.loc[:,[\"diagnosis\"]]\n",
    "c = data_cvae.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e808c9-7fa5-4d69-a9a8-6da6db93ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding\n",
    "data_c = c.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70248eec-352f-41b7-b88e-040cf39bca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd46fdc-40f8-40e0-a7bf-65b0cefff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)\n",
    "print(\"The size of c dataset is:\", data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed957c6-c08e-43be-8f2b-49b95bdb297a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20cb35d-e150-4967-9e40-1ccd5b2f41e5",
   "metadata": {},
   "source": [
    "### CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d867cef-4c46-4926-a999-921c1380c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE:\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        ## initializing data demensionality\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        ## generating encoder and decoder section\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.cvae = self.build_cvae()\n",
    "    \n",
    "    ## defining encoder function\n",
    "    def build_encoder(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        combined_input = layers.Concatenate()([x_input, c_input])\n",
    "        \n",
    "        h = layers.Dense(16, activation='relu')(combined_input)\n",
    "        # h = layers.Dense(32, activation='relu')(h)\n",
    "        \n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(h)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(h)\n",
    "        \n",
    "        ## sampling (reparameterization)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        z = layers.Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        return Model([x_input, c_input], [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    ## defining decoder function\n",
    "    def build_decoder(self):\n",
    "        z_input = layers.Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        combined_input = layers.Concatenate()([z_input, c_input])\n",
    "        \n",
    "        h = layers.Dense(16, activation='relu')(combined_input)\n",
    "        # h = layers.Dense(64, activation='relu')(h)\n",
    "        x_decoded = layers.Dense(self.input_dim, activation='sigmoid', name='output')(h)\n",
    "        \n",
    "        return Model([z_input, c_input], x_decoded, name='decoder')\n",
    "    \n",
    "    ## build and compile the CVAE model\n",
    "    def build_cvae(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        z_mean, z_log_var, z = self.encoder([x_input, c_input])\n",
    "        x_decoded = self.decoder([z, c_input])\n",
    "        \n",
    "        cvae = Model([x_input, c_input], x_decoded, name='cvae')\n",
    "        \n",
    "        reconstruction_loss = mse(x_input, x_decoded)\n",
    "        reconstruction_loss *= self.input_dim\n",
    "        \n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        cvae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        cvae.add_loss(cvae_loss)\n",
    "        cvae.compile(optimizer='adam')\n",
    "        \n",
    "        return cvae\n",
    "    \n",
    "    ## model training\n",
    "    def train(self, x_data, c_data, epochs, batch_size, validation_split):\n",
    "        self.cvae.fit([x_data, c_data], epochs=epochs, batch_size=batch_size, validation_split = validation_split)\n",
    "    \n",
    "    ## synthetic data generation (for data augmentation)\n",
    "    def generate_synthetic_data(self, condition, n_samples=1):\n",
    "        z_samples = np.random.normal(size=(n_samples, self.latent_dim))\n",
    "        synthetic_data = self.decoder.predict([z_samples, condition])\n",
    "        return synthetic_data\n",
    "        \n",
    "    def visualize_latent_space(self, x_data, c_data, labels, n_samples):\n",
    "        n_samples = min(n_samples, len(x_data), len(c_data))\n",
    "        indices = np.random.choice(len(x_data), n_samples, replace=False) ## data sampling for n\n",
    "        x_sample = x_data.iloc[indices]\n",
    "        c_sample = c_data[indices]\n",
    "        z_mean, _, _ = self.encoder.predict([x_sample, c_sample]) ## calculating latent vector z\n",
    "        tsne = TSNE(n_components=2, random_state=710674) #tsne visualization\n",
    "        z_tsne = tsne.fit_transform(z_mean)\n",
    "\n",
    "        ## visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if labels is not None:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=labels[indices], cmap='viridis')\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1])\n",
    "        plt.title('t-SNE visualization of the latent space')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8304bfd-0864-4e0b-9907-d6b173681020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvae = CVAE(input_dim = data_x.shape[1], condition_dim = data_y.shape[1], latent_dim=args_cvae.latent_dim)\n",
    "cvae.train(data_x, data_y, epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f9954-ff47-4540-95dd-2b36d47267f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f6a483-cc46-4409-bf2b-663533e530b2",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d998d21-df5d-4f35-8ea4-7cf41ea2de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.visualize_latent_space(data_x, data_y, label, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac4b83-44fb-4f9f-89a0-8f94958ebce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b15c46-3725-4edb-96ac-1b375b14c61e",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3c41d-238b-47ee-9078-56a4a72fd445",
   "metadata": {},
   "source": [
    "* This section is to generate synthetic data using trained CVAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e445fd4-5c8a-415e-90bb-a03bee57017a",
   "metadata": {},
   "source": [
    "#### single CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25957117-b48c-4422-ac2c-bfd6706a2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data(cvae_model, c_data, n_samples_per_class, labels, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## filtering the condition for each class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "        class_conditions = c_data[class_indices]\n",
    "        \n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            indices = np.random.choice(len(class_conditions), n_samples, replace=True)\n",
    "            class_new_conditions = class_conditions[indices]\n",
    "        ## calculate mean and variance from origianl condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = np.mean(class_conditions, axis=0)\n",
    "            std = np.std(class_conditions, axis=0)\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    # return synthetic_data, new_conditions\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a4aad-7e91-419e-bdfd-0b142902e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "## synthetic data generation (augmentation)\n",
    "synthetic_data = generate_class_synthetic_data(cvae, data_y, n_samples_per_class, label, method='random')\n",
    "# synthetic_data = generate_class_synthetic_data(cvae, data_y, n_samples_per_class, label, method='gaussian')\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0 - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1 - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a5596-5f0c-4a64-a217-4e1bc781b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataframe(synthetic_data, data_x):\n",
    "    dataframes = {}\n",
    "    for class_label, data in synthetic_data.items():\n",
    "        data_copy = data.copy()\n",
    "        df = pd.DataFrame(data_copy, columns=data_x.columns)\n",
    "        dataframes[class_label] = df\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f88ee1-8931-4234-9ec2-eb1dd450a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = synthetic_dataframe(synthetic_data, data_x)\n",
    "\n",
    "# 클래스 0과 클래스 1에 대한 데이터프레임 추출\n",
    "gen_B = dataframes[0]\n",
    "gen_M = dataframes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119eb56b-a07a-42cc-8e6d-6689d597c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed88c5b-9878-4e0c-98bd-4221ed6f2412",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828aeae-e8ca-4fd8-94bb-6afd21c1a0c1",
   "metadata": {},
   "source": [
    "#### individual CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dac422-dc3d-486d-b1a7-2c3822930986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## we should train individual CVAE model for each class\n",
    "cvae_models = {}\n",
    "\n",
    "for class_label in np.unique(label):\n",
    "    ## data filtering to find dataset with that class label\n",
    "    class_indices = np.where(label == class_label)[0]\n",
    "    class_data = data_x.iloc[class_indices]\n",
    "    \n",
    "    ## initialize and train CVAE model\n",
    "    cvae = CVAE(input_dim=data_x.shape[1], latent_dim=args_cvae.latent_dim, condition_dim=data_y.shape[1])\n",
    "    cvae.train(class_data, data_y[class_indices], epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)\n",
    "    \n",
    "    ## save the trained model\n",
    "    cvae_models[class_label] = cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b15a3-8baf-4122-bcaf-dd8ce2712e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_seperately(cvae_models, c_data, n_samples_per_class, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## loading trained CVAE model for each class\n",
    "        cvae_model = cvae_models[class_label]\n",
    "        \n",
    "        ## filtering the condition for the class\n",
    "        class_indices = np.where(label == class_label)[0]\n",
    "        class_conditions = c_data[class_indices]\n",
    "\n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            indices = np.random.choice(len(class_conditions), n_samples, replace=True)\n",
    "            class_new_conditions = class_conditions[indices]\n",
    "        ## calculate mean and variance from origianl condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = np.mean(class_conditions, axis=0)\n",
    "            std = np.std(class_conditions, axis=0)\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    # return synthetic_data, new_conditions\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731dfec-d4ad-45df-bf48-4eb6b620fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0:212, 1:357}\n",
    "\n",
    "synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_y, n_samples_per_class, method=\"random\")\n",
    "# synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_y, n_samples_per_class, method=\"gaussian\")\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0 - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1 - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb930a4e-2890-4378-ad47-d612d08340a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0b213-4d81-4130-a0dd-12459cdcbed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02c368b-2778-4898-9811-bcd4b716c458",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709c151-d091-44f9-818a-a87b41fd025c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c716af4-99d6-405d-87c1-ab246aa72a2e",
   "metadata": {},
   "source": [
    "# Classification performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b1a65-7ad2-4a26-947f-0fd9614370b8",
   "metadata": {},
   "source": [
    "## Original dataset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05b74071-5d41-4b00-8a3f-83bb161e2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dnn = data_ori.copy()\n",
    "data_dnn = data_sampled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6dcfa146-97d1-4ed7-8623-a5d3d550cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dnn.drop(\"diagnosis\", axis=1)\n",
    "x = x.fillna(x.mean()) ## filling na values with mean values (just drop the rows is also a possible option)\n",
    "y = data_dnn.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2daf1d28-bd56-4b86-b847-75e5925c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'B':0})\n",
    "label = label.replace({'M': 1})\n",
    "data_y = to_categorical(label, 2) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b148481-23f4-4070-b6b5-ac50fd174804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d3c1c17-c014-4f39-8682-1f15e330908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## definition for computing class weight (to solve class imbalance issue)\n",
    "def compute_class_weights(y):\n",
    "    class_counts = np.bincount(y) ## calculating data point for each class\n",
    "    total_samples = len(y) ## total data points\n",
    "    ## computing each class weight\n",
    "    class_weights = {i: total_samples / (len(class_counts) * class_count) \n",
    "                     for i, class_count in enumerate(class_counts)}\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0596145a-e853-45aa-9efa-84c199748154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5428571428571428, 1: 6.333333333333333}\n"
     ]
    }
   ],
   "source": [
    "class_weight = compute_class_weights(label)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf06b45-c38d-4970-aa25-14b0db1993f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da47566c-3dfe-48b6-b810-a04809da4ceb",
   "metadata": {},
   "source": [
    "## Augmented dataset add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39a1af-6662-42a5-b5f1-de5a52427c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cvae_data_preparation:\n",
    "    def __init__(self, original_data, gen_data, label_col, id_cols=None, drop_cols=None):\n",
    "        \"\"\"\n",
    "        original_data (pandas.DataFrame): original dataset\n",
    "        gen_data (dict): data dictionary for synthetic dataset (ex: {0: gen_A, 1: gen_B})\n",
    "        label_col (str): label column (target variable)\n",
    "        id_cols (list): column list to drop (예: ['Unnamed: 32', 'id'])\n",
    "        drop_cols (list): column list to drop from original dataset\n",
    "        \"\"\"\n",
    "        self.original_data = original_data\n",
    "        self.gen_data = gen_data\n",
    "        self.label_col = label_col\n",
    "        self.id_cols = id_cols if id_cols is not None else []\n",
    "        self.drop_cols = drop_cols if drop_cols is not None else []\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        ## preprocessing original dataset\n",
    "        data_dnn = self.original_data.copy()\n",
    "        for col in self.id_cols:\n",
    "            if col in data_dnn.columns:\n",
    "                data_dnn = data_dnn.drop(col, axis=1)\n",
    "        \n",
    "        data_dnn = data_dnn.drop(self.drop_cols, axis=1)\n",
    "        data_classes = {label: data_dnn[data_dnn[self.label_col] == label] for label in data_dnn[self.label_col].unique()}\n",
    "        \n",
    "        ## preprocessing synthetic generated dataset\n",
    "        for label, df in self.gen_data.items():\n",
    "            df[self.label_col] = label\n",
    "            if self.drop_cols:\n",
    "                df = df.drop(self.drop_cols, axis=1)\n",
    "            self.gen_data[label] = df\n",
    "        \n",
    "        ## concat original dataset and generated dataset\n",
    "        ori_df_concat = pd.concat(list(data_classes.values()), ignore_index=True)\n",
    "        gen_df_concat = pd.concat(list(self.gen_data.values()), ignore_index=True)\n",
    "        \n",
    "        ## separating feature and label(target)\n",
    "        ori_x = ori_df_concat.drop([self.label_col], axis=1)\n",
    "        ori_y = ori_df_concat[[self.label_col]]\n",
    "        \n",
    "        gen_x = gen_df_concat.drop([self.label_col], axis=1)\n",
    "        gen_y = gen_df_concat[[self.label_col]]\n",
    "        \n",
    "        ## filling missing values and scaling\n",
    "        ori_x = ori_x.fillna(ori_x.mean())\n",
    "        ori_x[:] = (scaler.fit_transform(ori_x[:])).round(decimals=6) ## scaling x values\n",
    "        \n",
    "        # 레이블 인코딩 및 원-핫 인코딩\n",
    "        ori_y = ori_y.replace({self.label_col: {label: idx for idx, label in enumerate(ori_y[self.label_col].unique())}})\n",
    "        y_ori = to_categorical(ori_y, num_classes=len(ori_y[self.label_col].unique()))\n",
    "        \n",
    "        gen_y = gen_y.replace({self.label_col: {label: idx for idx, label in enumerate(gen_y[self.label_col].unique())}})\n",
    "        y_gen = to_categorical(gen_y, num_classes=len(gen_y[self.label_col].unique()))\n",
    "        \n",
    "        ## separating training and test dataset\n",
    "        x_trainset, x_test, y_trainset, y_test = train_test_split(ori_x, y_ori, test_size=0.35, random_state=710674)\n",
    "        \n",
    "        ## add generated dataset to training dataset\n",
    "        x_train_concat = pd.concat([x_trainset, gen_x], ignore_index=True)\n",
    "        y_train_concat = np.concatenate([y_trainset, y_gen])\n",
    "        \n",
    "        ## separating training dataset and validation dataset\n",
    "        x_train, x_vali, y_train, y_vali = train_test_split(x_train_concat, y_train_concat, test_size=0.2, random_state=710674)\n",
    "        \n",
    "        return x_train, x_vali, x_test, y_train, y_vali, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028bb49-b690-4e65-b597-22f94d7d1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a dictionary for generated dataset\n",
    "gen_data = {\"B\": gen_B, \"M\": gen_M }\n",
    "\n",
    "## data_preparation class initialize and data prepare\n",
    "data_prep = cvae_data_preparation(data_ori, gen_data, label_col='diagnosis', id_cols=['Unnamed: 32', 'id'])\n",
    "x_train, x_vali, x_test, y_train, y_vali, y_test = data_prep.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c21197-c2a6-42f1-b6f3-78f0544f09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The final datset size is... \\nTraining dataset: {x_train.shape[0]} \\nValidation dataset: {x_vali.shape[0]} \\nTest dataset: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea230a58-039b-463c-b96f-898074d86474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656d04ba-c534-479a-a64d-5aa6372f9ddf",
   "metadata": {},
   "source": [
    "## Simple DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf53eee-2152-48a2-b32a-72fcb1735300",
   "metadata": {},
   "source": [
    "> To compare classification performance, we design simple structure of DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d63b03d7-14df-4e9d-9300-0186b78f56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_dnn:\n",
    "    # arugments\n",
    "    epochs=50\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    seed=710674\n",
    "\n",
    "args_dnn = Args_dnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a465b-88c6-422b-9bb9-51da409a6ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd8edf5-17b3-4621-b44b-2989de45222d",
   "metadata": {},
   "source": [
    "### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "36842d63-1554-4616-a932-a7996e0b5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDNN:\n",
    "    def __init__(self, input_dim, layer_configs, output_units, output_activation='softmax'):\n",
    "        self.input_dim = input_dim ## input data dimensionality (# variables)\n",
    "        self.layer_configs = layer_configs ## hidden layer/sequential layer lists (units, activation, batch_norm, dropout_rate)\n",
    "        self.output_units = output_units ## output unit no.\n",
    "        self.output_activation = output_activation ## activation function for output layer\n",
    "        self.model = self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], activation=self.layer_configs[0]['activation'], input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None, class_weight=None):\n",
    "        if lr_scheduler:\n",
    "            ## AddLearningRateScheduler callback\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "        \n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_split=None, class_weight=None, validation_data=None):\n",
    "        return self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split,\n",
    "                             class_weight=class_weight, validation_data = validation_data, callbacks=self.callbacks)\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7dc70963-5d03-4f73-b145-85fc7e2bf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_learning_rate(epoch, mode='cyclic', base_lr=0.001, max_lr=0.006, step_size=2000, gamma=0.99994):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    \n",
    "    if mode == 'cyclic' or mode == 'triangular':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n",
    "    elif mode == 'triangular2':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) / (2 ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) * (gamma ** epoch)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose from 'cyclic', 'triangular', 'triangular2', or 'exp_range'.\")\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bbc3d4c9-3840-4f68-a1ab-b437d121a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = dynamic_learning_rate(epoch=1000, mode='cyclic')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular2')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='exp_range', gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40d768eb-5aec-4a80-8bb1-32af09b17f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 30)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60f71e88-1de2-4a55-ae0e-8f264e690ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 16, 'activation': 'relu', 'batch_norm': False, 'dropout_rate': 0.3},\n",
    "    {'units': 8, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2}\n",
    "]\n",
    "\n",
    "model = SimpleDNN(output_units=args_dnn.num_classes, input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb8a3972-a921-44ab-9678-af5d3a845e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 32)                992       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,834\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "model.compile_model(optimizer = opt, lr_scheduler=scheduler)\n",
    "\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a0c3ac7-d734-4e26-a711-00e4d6d4895b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 1.2209 - accuracy: 0.3457 - val_loss: 0.6625 - val_accuracy: 0.8852 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.0301 - accuracy: 0.3909 - val_loss: 0.6249 - val_accuracy: 0.9016 - lr: 0.0013\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.8330 - accuracy: 0.4979 - val_loss: 0.5723 - val_accuracy: 0.9016 - lr: 0.0016\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.7123 - accuracy: 0.6008 - val_loss: 0.5139 - val_accuracy: 0.9016 - lr: 0.0020\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5790 - accuracy: 0.6831 - val_loss: 0.4637 - val_accuracy: 0.9016 - lr: 0.0023\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5363 - accuracy: 0.7572 - val_loss: 0.4230 - val_accuracy: 0.9016 - lr: 0.0026\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4822 - accuracy: 0.8395 - val_loss: 0.3882 - val_accuracy: 0.9016 - lr: 0.0029\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4076 - accuracy: 0.8930 - val_loss: 0.3617 - val_accuracy: 0.9016 - lr: 0.0032\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3167 - accuracy: 0.9300 - val_loss: 0.3415 - val_accuracy: 0.9016 - lr: 0.0036\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2916 - accuracy: 0.9506 - val_loss: 0.3240 - val_accuracy: 0.9016 - lr: 0.0039\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.2695 - accuracy: 0.9300 - val_loss: 0.3104 - val_accuracy: 0.9016 - lr: 0.0042\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2745 - accuracy: 0.9012 - val_loss: 0.2994 - val_accuracy: 0.9016 - lr: 0.0045\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2155 - accuracy: 0.9424 - val_loss: 0.2897 - val_accuracy: 0.9016 - lr: 0.0048\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2096 - accuracy: 0.9342 - val_loss: 0.2808 - val_accuracy: 0.9016 - lr: 0.0052\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2426 - accuracy: 0.9383 - val_loss: 0.2709 - val_accuracy: 0.9016 - lr: 0.0055\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1828 - accuracy: 0.9383 - val_loss: 0.2613 - val_accuracy: 0.9016 - lr: 0.0058\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1912 - accuracy: 0.9342 - val_loss: 0.2553 - val_accuracy: 0.9016 - lr: 0.0061\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2168 - accuracy: 0.9300 - val_loss: 0.2542 - val_accuracy: 0.9016 - lr: 0.0064\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1223 - accuracy: 0.9712 - val_loss: 0.2518 - val_accuracy: 0.9016 - lr: 0.0068\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2105 - accuracy: 0.9342 - val_loss: 0.2433 - val_accuracy: 0.9016 - lr: 0.0071\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1559 - accuracy: 0.9342 - val_loss: 0.2405 - val_accuracy: 0.9016 - lr: 0.0074\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1638 - accuracy: 0.9424 - val_loss: 0.2401 - val_accuracy: 0.9016 - lr: 0.0077\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1452 - accuracy: 0.9630 - val_loss: 0.2259 - val_accuracy: 0.9016 - lr: 0.0080\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1275 - accuracy: 0.9588 - val_loss: 0.2102 - val_accuracy: 0.9016 - lr: 0.0084\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1433 - accuracy: 0.9547 - val_loss: 0.2011 - val_accuracy: 0.9016 - lr: 0.0087\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1722 - accuracy: 0.9465 - val_loss: 0.1822 - val_accuracy: 0.9344 - lr: 0.0090\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1441 - accuracy: 0.9465 - val_loss: 0.1750 - val_accuracy: 0.9344 - lr: 0.0087\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1476 - accuracy: 0.9465 - val_loss: 0.1831 - val_accuracy: 0.9344 - lr: 0.0084\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1335 - accuracy: 0.9630 - val_loss: 0.1827 - val_accuracy: 0.9344 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1607 - accuracy: 0.9465 - val_loss: 0.1950 - val_accuracy: 0.9344 - lr: 0.0077\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1540 - accuracy: 0.9383 - val_loss: 0.2018 - val_accuracy: 0.9344 - lr: 0.0074\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1804 - accuracy: 0.9300 - val_loss: 0.1942 - val_accuracy: 0.9344 - lr: 0.0071\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1149 - accuracy: 0.9671 - val_loss: 0.1846 - val_accuracy: 0.9344 - lr: 0.0068\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1226 - accuracy: 0.9630 - val_loss: 0.1810 - val_accuracy: 0.9344 - lr: 0.0064\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1033 - accuracy: 0.9671 - val_loss: 0.1830 - val_accuracy: 0.9344 - lr: 0.0061\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1411 - accuracy: 0.9547 - val_loss: 0.1825 - val_accuracy: 0.9344 - lr: 0.0058\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1281 - accuracy: 0.9630 - val_loss: 0.1793 - val_accuracy: 0.9344 - lr: 0.0055\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0879 - accuracy: 0.9712 - val_loss: 0.1715 - val_accuracy: 0.9508 - lr: 0.0052\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1166 - accuracy: 0.9547 - val_loss: 0.1586 - val_accuracy: 0.9508 - lr: 0.0048\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1266 - accuracy: 0.9424 - val_loss: 0.1452 - val_accuracy: 0.9508 - lr: 0.0045\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1151 - accuracy: 0.9547 - val_loss: 0.1368 - val_accuracy: 0.9508 - lr: 0.0042\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0839 - accuracy: 0.9712 - val_loss: 0.1286 - val_accuracy: 0.9508 - lr: 0.0039\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0796 - accuracy: 0.9712 - val_loss: 0.1254 - val_accuracy: 0.9508 - lr: 0.0036\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0828 - accuracy: 0.9671 - val_loss: 0.1208 - val_accuracy: 0.9508 - lr: 0.0032\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1209 - accuracy: 0.9506 - val_loss: 0.1220 - val_accuracy: 0.9508 - lr: 0.0029\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1168 - accuracy: 0.9506 - val_loss: 0.1203 - val_accuracy: 0.9508 - lr: 0.0026\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1726 - accuracy: 0.9424 - val_loss: 0.1178 - val_accuracy: 0.9508 - lr: 0.0023\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0893 - accuracy: 0.9712 - val_loss: 0.1171 - val_accuracy: 0.9508 - lr: 0.0020\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0830 - accuracy: 0.9753 - val_loss: 0.1177 - val_accuracy: 0.9508 - lr: 0.0016\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1001 - accuracy: 0.9753 - val_loss: 0.1186 - val_accuracy: 0.9508 - lr: 0.0013\n"
     ]
    }
   ],
   "source": [
    "## model training on training dataset\n",
    "history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, validation_split=0.2)\n",
    "# history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, class_weight = class_weight, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264564c1-fc3d-4908-853f-a674413d7ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f38ffd4-a2ef-41e6-97ae-b7b66527b99a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "242c23e4-80ba-4b2f-aa19-05967c1f5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_test, y_test):\n",
    "\n",
    "    ## predict on model\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_predict_classes = np.argmax(y_predict, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    ## calculate confusion matrix and visualize\n",
    "    cm = confusion_matrix(y_test_classes, y_predict_classes, normalize='pred')\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, fmt='.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    ## evaluation metrics\n",
    "    accuracy = metrics.accuracy_score(y_test_classes, y_predict_classes)\n",
    "    precision = metrics.precision_score(y_test_classes, y_predict_classes, average='macro')\n",
    "    recall = metrics.recall_score(y_test_classes, y_predict_classes, average='micro')\n",
    "    f1 = metrics.f1_score(y_test_classes, y_predict_classes, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_predict, multi_class='ovr')\n",
    "    \n",
    "    ## Results\n",
    "    print(\"=============================================\")\n",
    "    print(f\"The overall accuracy is: {accuracy:.4f}\")\n",
    "    print(f\"The precision score is: {precision:.4f}\")\n",
    "    print(f\"The recall score is: {recall:.4f}\")\n",
    "    print(f\"The F1 score is: {f1:.4f}\")\n",
    "    print(f\"The AUC score is: {auc:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    ## Print out the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "20cc6ea3-e541-4c4c-93d4-37222ecdaad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "The overall accuracy is: 0.9868\n",
      "The precision score is: 0.9933\n",
      "The recall score is: 0.9868\n",
      "The F1 score is: 0.9847\n",
      "The AUC score is: 1.0000\n",
      "=============================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        74\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.99        76\n",
      "   macro avg       0.99      0.75      0.83        76\n",
      "weighted avg       0.99      0.99      0.98        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_performance(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0423ecf-9cec-472c-981b-59519084ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb36adb-09e5-4889-9477-d68b667b2c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ebfc2-0396-4ee4-b750-5e7215f8e973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17d29db-da6a-4ebc-8e05-1d96de43b927",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881e049-558a-4561-bd96-d1e628e2b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def adjust_synthetic_data_distribution(original_data, synthetic_data):\n",
    "    # 원래 데이터의 평균과 분산을 계산\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_data)\n",
    "    \n",
    "    # 원래 데이터의 평균과 분산\n",
    "    original_mean = scaler.mean_\n",
    "    original_std = scaler.scale_\n",
    "    \n",
    "    # synthetic data의 평균과 분산을 계산\n",
    "    synthetic_scaler = StandardScaler()\n",
    "    synthetic_scaler.fit(synthetic_data)\n",
    "    \n",
    "    # synthetic data를 원래 데이터의 평균과 분산으로 조정\n",
    "    adjusted_synthetic_data = synthetic_scaler.transform(synthetic_data)\n",
    "    adjusted_synthetic_data = adjusted_synthetic_data * original_std + original_mean\n",
    "    \n",
    "    return adjusted_synthetic_data\n",
    "\n",
    "# 예시 데이터\n",
    "data_x = np.random.rand(1000, 20)  # 원래 데이터 (예: 1000개의 샘플, 20개의 피쳐)\n",
    "synthetic_data = np.random.rand(500, 20)  # synthetic 데이터 (예: 500개의 샘플, 20개의 피쳐)\n",
    "\n",
    "# 데이터 분포 조정\n",
    "adjusted_synthetic_data = adjust_synthetic_data_distribution(data_x, synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bc813-2b85-4b77-9a13-5c877efa8d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760457e-c4a2-4843-9ca3-381fd34ccea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
