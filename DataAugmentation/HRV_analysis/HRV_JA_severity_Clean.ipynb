{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a20b02d-6ef3-4a6e-a45e-69cddd886137",
   "metadata": {},
   "source": [
    "# Data Augmentation- VAE, Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6a98c-2f59-452a-b8b1-ee762e22acf9",
   "metadata": {},
   "source": [
    "> This script adopts HRV dataset for 3-class disease classification. \\\n",
    "> Major Depressive Disorder, Panic Disorder, Control. \\\n",
    "> Also contains various psychological scales (HAMD, HAMA, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbef65-77a1-43fe-989a-0a0c7d4ead5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import shap ## for XAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882e3a3-161d-4d3e-bef3-b83010ca2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from keras.layers import Dense , Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam ,RMSprop\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, ParameterGrid\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, confusion_matrix, roc_auc_score, classification_report, precision_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Lambda, Conv1D, Flatten, Reshape, UpSampling1D, MaxPooling1D, concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.losses import mse, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf01643-b981-4d57-ae21-6822cf681a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173afa80-706e-4320-b3c5-cabc098f7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfaf0e-1296-43a9-a547-8dba9168a6ba",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baaf49-19f6-4559-8906-1d8fb2d49ae7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf5bb1-4eca-4da1-b06e-5c62e8549841",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecc614-ad90-4684-b320-e30dc72b4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_csv('E:/RESEARCH/Datasets/HRV/JA/HRV_prep_features_and_core_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c10540-7820-4eb9-b55f-bc1e56ff585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = data_ori[(data_ori.srd >=0.8) & (data_ori.srd <= 1.0)] ## srd outlier removed - there were 1424 outlier data\n",
    "data_ori = data_ori[(data_ori.abnormal_hr) <= 5] ## abnormal heart rate outlier removed - there were 175 outlier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4403115-0081-4087-b098-c3c3c484b367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7cb3a-dd1e-43a6-9f71-e3d97900f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data shape, variables check\n",
    "print(f\"The shape of the original dataset is: {data_ori.shape}\")\n",
    "# print(public.columns)\n",
    "data_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0edc4b1-37db-4c95-ad24-e143b4094011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값이 하나라도 있는 행의 개수 확인\n",
    "num_missing_rows = data_ori.isna().any(axis=1).sum()\n",
    "print(f\"The number of rows of the original dataset is: {data_ori.shape[0]}\")\n",
    "print(f\"The number of rows that contains at least one missing value: {num_missing_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b4f18-73bf-4bf3-87aa-87484a359d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d57267d7-1656-4f5a-aed2-22c4fc92c88c",
   "metadata": {},
   "source": [
    "### target column generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a991786-c830-4ea6-890f-4df0cfe0bdac",
   "metadata": {},
   "source": [
    "> combine HAMD, BDI to depression, and HAMA, BAI to anxious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e66c3-70f0-4c31-baa0-24ad66b40a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori['depression'] = 0\n",
    "data_ori['anxious'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a9729-de63-4e50-89ec-11a9d5a5ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99675c6-e5de-4f79-88c3-fb86c000ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori.loc[(data_ori['HAMD_']==\"normal\") & (data_ori['BDI_']==\"normal\"), 'depression' ] = 'normal'\n",
    "data_ori.loc[(data_ori['HAMD_']==\"mild\") & (data_ori['BDI_']==\"mild\"), 'depression' ] = 'mild'\n",
    "data_ori.loc[(data_ori['HAMD_']==\"moderate\") & (data_ori['BDI_']==\"moderate\"), 'depression' ] = 'moderate'\n",
    "data_ori.loc[(data_ori['HAMD_']==\"severe\") & (data_ori['BDI_']==\"severe\"), 'depression' ] = 'severe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae30d2-d4b9-4976-9a23-5c74317940a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori.loc[(data_ori['HAMA_']==\"normal\") & (data_ori['BAI_']==\"normal\"), 'anxious' ] = 'normal'\n",
    "data_ori.loc[(data_ori['HAMA_']==\"mild\") & (data_ori['BAI_']==\"mild\"), 'anxious' ] = 'mild'\n",
    "data_ori.loc[(data_ori['HAMA_']==\"moderate\") & (data_ori['BAI_']==\"moderate\"), 'anxious' ] = 'moderate'\n",
    "data_ori.loc[(data_ori['HAMA_']==\"severe\") & (data_ori['BAI_']==\"severe\"), 'anxious' ] = 'severe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb78c4-7bf5-4aab-9739-652b85544879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea33a5-66ea-48aa-90dd-13d917828404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = data_ori.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329ad79-9f86-48ac-9436-d5068daeafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_empty = data_ori[data_ori['depression']==0].index\n",
    "anxious_empty = data_ori[data_ori['anxious']==0].index\n",
    "\n",
    "data_dep = data_ori.drop(depression_empty)\n",
    "data_anx = data_ori.drop(anxious_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e995314-ce75-4c43-864a-414b415d01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dep.depression.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1733b-0702-4817-a210-f2275a9d9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anx.anxious.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8c844-ccdf-4e31-a745-512c1afe732f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d0d266c-c4f4-41e2-a73d-b8539c403dec",
   "metadata": {},
   "source": [
    "### Variables drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342d987-312d-4388-9e04-0fcba04e73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6981a7e-29ff-44e0-b2df-5047d1463e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = ['gender', 'age']\n",
    "scale_list = ['HAMD_', 'HAMA_', 'BDI_', 'BAI_']\n",
    "target_list = ['depression', 'anxious']\n",
    "drop_list = info_list + scale_list + target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661893e-3c72-4264-89d8-39a34bffeead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f99886c-d984-4989-a955-acfd917dbd27",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7f908-d2a5-4939-b109-2f2f784b0bce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Distribution check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37422c91-54a5-48d7-a3b4-7b9403b7f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vis = data_ori.copy()\n",
    "data_vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de08029-7af1-485c-af10-7ed88ba7eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_ = ['sub', 'VISIT', 'age', 'gender', 'disorder_mdd']\n",
    "# info_ = ['sub', 'VISIT', 'age', 'gender', 'disorder', 'disorder_mdd']\n",
    "scale_ = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "drop_ = info_ + scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49d31d-d3ea-49b5-841b-ab40fdbc47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = data_vis.drop(drop_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8cb63-fac8-4b72-9c4d-884142dce35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2ecae-f6ae-44eb-bbba-490845d1c19e",
   "metadata": {},
   "source": [
    "#### KDE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc513b78-1111-4303-a85b-981be282841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in list(x.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93b026-aed9-4b00-9038-f2f9c0672650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdeplot_features(data, feature, title):\n",
    "    \n",
    "    df0 = data[data['disorder']==\"mdd\"]\n",
    "    df1 = data[data['disorder']==\"pd\"]\n",
    "    df2 = data[data['disorder']==\"con\"]\n",
    "\n",
    "    df0 = df0.drop(['disorder'], axis=1)\n",
    "    df1 = df1.drop(['disorder'], axis=1)\n",
    "    df2 = df2.drop(['disorder'], axis=1)\n",
    "    \n",
    "    df0_values = df0[feature].to_numpy()\n",
    "    df1_values = df1[feature].to_numpy()\n",
    "    df2_values = df2[feature].to_numpy()\n",
    "     \n",
    "    plt.figure(figsize = (6, 2.5))\n",
    "    \n",
    "    sns.kdeplot(df0_values, color = 'y')\n",
    "    sns.kdeplot(df1_values, color = 'b')\n",
    "    sns.kdeplot(df2_values, color = 'g')\n",
    "    \n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    \n",
    "    # del values_train , values_test\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f370a-e7e2-43d8-a852-9d2eecbbc57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in features:\n",
    "    kdeplot_features(x_, feature=var, title = var + \"distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc4e77-3523-4066-986d-bb6f9b24ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a965d6-b485-4eb2-9bab-eb817409832e",
   "metadata": {},
   "source": [
    "#### JSD calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06c5f9-2a6d-4e6c-be28-8e10c7186e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list to append distributional differences\n",
    "variable = []\n",
    "diff01 = []\n",
    "diff02 = []\n",
    "diff03 = []\n",
    "diff12 = []\n",
    "diff13 = []\n",
    "diff23 = []\n",
    "\n",
    "for feature in features:\n",
    "    ## deleting any NA values in each feature column, for distance calculation\n",
    "    data = data_vis.dropna(subset=[feature])\n",
    "\n",
    "    ## split the dataset into each classes\n",
    "    df0 = data[data['target_info']==\"control\"]\n",
    "    df1 = data[data['target_info']==\"mdd\"]\n",
    "    df2 = data[data['target_info']==\"bpi\"]\n",
    "    df3 = data[data['target_info']==\"bpii\"]\n",
    "\n",
    "    df0 = df0.drop(['target_info'], axis=1)\n",
    "    df1 = df1.drop(['target_info'], axis=1)\n",
    "    df2 = df2.drop(['target_info'], axis=1)\n",
    "    df3 = df3.drop(['target_info'], axis=1)\n",
    "    \n",
    "    df0_values = df0[feature]\n",
    "    df1_values = df1[feature]\n",
    "    df2_values = df2[feature]\n",
    "    df3_values = df3[feature]\n",
    "\n",
    "    ## sampling based on the minimum size of the target info.\n",
    "    sample_size = (min(df0_values.shape[0], df1_values.shape[0], df2_values.shape[0], df3_values.shape[0]))\n",
    "    df0_sample = df0_values.sample(n=(sample_size))\n",
    "    df1_sample = df1_values.sample(n=(sample_size))\n",
    "    df2_sample = df2_values.sample(n=(sample_size))\n",
    "    df3_sample = df3_values.sample(n=(sample_size))\n",
    "\n",
    "    ## jensen-shannon divergence calculation and append\n",
    "    variable.append(feature)\n",
    "    diff01.append(jensenshannon(df0_sample, df1_sample))\n",
    "    diff02.append(jensenshannon(df0_sample, df2_sample))\n",
    "    diff03.append(jensenshannon(df0_sample, df3_sample))\n",
    "    diff12.append(jensenshannon(df1_sample, df2_sample))\n",
    "    diff13.append(jensenshannon(df1_sample, df3_sample))\n",
    "    diff23.append(jensenshannon(df2_sample, df3_sample)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76991d9-f00c-4eff-be6e-239c519eaca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990b52e-f0f1-4e37-8e78-52ad9acdc13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a790933-91fd-4c5e-81b8-7fbf723533cf",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cd5d4-ff8e-4c15-b42f-276833e80bec",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a5c67-d878-405a-a9f8-82423bbc0f86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72688524-49ea-430b-b106-45a383ae0483",
   "metadata": {},
   "source": [
    "## dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997cdb8-24cf-4e5c-97af-15df171e525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d7157-98be-41dd-86a7-f5608e50c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CON = data[data['disorder']==\"con\"]\n",
    "data_PD = data[data['disorder']==\"pd\"]\n",
    "data_MDD = data[data['disorder']==\"mdd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b20739-123e-4919-b470-8927a558f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = data.copy()\n",
    "# data_vae = data_con.copy()\n",
    "# data_vae = data_pd.copy()\n",
    "# data_vae = data_mdd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387281-62c2-4293-9bba-99636db90593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22dbbc0-3dd7-4fd0-9041-e53865ff384f",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7aefa-d3f3-4d5f-bb6b-5263c40e8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_vae:\n",
    "    # arugments\n",
    "    epochs=150\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_vae = Args_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e77906-6db1-4e08-a0cf-9f745d2c62d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a9fc98d-616a-4be3-bbce-8e19f3307c9c",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfc6e8-e64f-4aad-a0e9-b42f3f05a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list = ['age', 'gender', 'disorder']\n",
    "drop_list = scale_list + etc_list + demo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a10a29-e8fb-4768-93fe-6168376f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop((drop_list), axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data.disorder# mdd / pd / con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ad571-cd55-43dd-88a8-1520cfd717bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'mdd': 0})\n",
    "label = label.replace({'pd': 1})\n",
    "label = label.replace({'con' : 2})\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce530e4-07f7-49d5-be98-a0692741743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8254ba3-98e7-446b-b9cb-17c28c07934f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0f958b-c75f-46cf-ba90-aabe6e3b242a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d6516-5f26-4afd-a1ca-21efe1021660",
   "metadata": {},
   "source": [
    "### vae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd6248-3ced-4948-a427-8149c8e180d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self.build_model()\n",
    "\n",
    "    ## VAE model for data augmentation\n",
    "    def build_model(self):\n",
    "        ####################################################\n",
    "        ## defining encoder section\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # shallow model use\n",
    "        h = Dense(32, activation='relu')(inputs)\n",
    "\n",
    "        # # deeper model use\n",
    "        # h = Dense(128, activation='relu')(inputs)\n",
    "        # h = Dense(64, activation='relu')(h)\n",
    "        # h = Dense(32, activation='relu')(h)\n",
    "\n",
    "        # calculating mean/var\n",
    "        z_mean = Dense(self.latent_dim)(h)\n",
    "        z_log_var = Dense(self.latent_dim)(h)\n",
    "\n",
    "        ## sampling section (for gaussian distribution)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        ####################################################\n",
    "        ## defining decoder section\n",
    "\n",
    "        # shallower model use\n",
    "        decoder_h = Dense(32, activation='relu')\n",
    "        decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "        h_decoded = decoder_h(z)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "        \n",
    "        # # deeper model use\n",
    "        # decoder_h1 = Dense(32, activation='relu')\n",
    "        # decoder_h2 = Dense(64, activation='relu')\n",
    "        # decoder_h3 = Dense(128, activation='relu')\n",
    "        # decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "\n",
    "        # h_decoded = decoder_h1(z)\n",
    "        # h_decoded = decoder_h2(h_decoded)\n",
    "        # h_decoded = decoder_h3(h_decoded)\n",
    "        # x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining VAE model\n",
    "        self.vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining loss function (reconstruction + KL divergence)\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var + K.epsilon()), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.vae.add_loss(vae_loss)\n",
    "\n",
    "        ####################################################\n",
    "        ## compiling model\n",
    "        self.vae.compile(optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "        ####################################################\n",
    "        ## Extracting encoder and decoder separately\n",
    "        ## encoder section\n",
    "        self.encoder = Model(inputs, z_mean)\n",
    "\n",
    "        ## decoder section\n",
    "        decoder_input = Input(shape=(self.latent_dim,))\n",
    "        _h_decoded = decoder_h(decoder_input)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "        \n",
    "        # #### deeper model\n",
    "        # decoder_input = Input(shape=(self.latent_dim,))\n",
    "        # _h_decoded = decoder_h1(decoder_input)\n",
    "        # _h_decoded = decoder_h2(_h_decoded)\n",
    "        # _h_decoded = decoder_h3(_h_decoded)\n",
    "        # _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        # self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    ####################################################\n",
    "    ####################################################\n",
    "    ## Model training\n",
    "    def train(self, data, epochs, batch_size, validation_split):\n",
    "        self.vae.fit(data, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def encode(self, data):\n",
    "        return self.encoder.predict(data)\n",
    "\n",
    "    def decode(self, latent_points):\n",
    "        return self.decoder.predict(latent_points)\n",
    "\n",
    "    def generate_synthetic_data(self, num_samples=1):\n",
    "        latent_samples = np.random.normal(size=(num_samples, self.latent_dim)) ## sampling from latent space\n",
    "        return self.decode(latent_samples) ## synthetic data generation with decoder section\n",
    "\n",
    "    def visualize_latent_space(self, data, labels):\n",
    "        ## encode the dataset into latent space\n",
    "        encoded_data = self.encode(data)\n",
    "\n",
    "        ## latent space visualization with t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=710674)\n",
    "        encoded_data_tsne = tsne.fit_transform(encoded_data)\n",
    "\n",
    "        ## Visualize\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(encoded_data_tsne[:, 0], encoded_data_tsne[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"t-SNE component 1\")\n",
    "        plt.ylabel(\"t-SNE component 2\")\n",
    "        plt.title(\"t-SNE visualization of the latent space\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519795-e972-46dd-aa18-6485cd044e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = VAE(input_dim=data_x.shape[1], latent_dim=args_vae.latent_dim)\n",
    "vae.train(data_x, epochs = args_vae.epochs, batch_size = args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8c330-9e7d-4339-be91-ca88cde8972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d41ae535-2ade-4963-a709-d96f43475e01",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e235a2-97e6-477d-a78e-2d0ab44bd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.visualize_latent_space(data_x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4121a-9bdd-4b39-b752-ad3afe3b8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8de385-71df-496c-ae62-f99725b6f39e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782e6f7-f737-4dc9-9265-df31bd7a7e15",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc58508-4b08-43b7-8a09-b7576b8e65da",
   "metadata": {},
   "source": [
    "> We use two different synthetic data generation functions.\n",
    "> 1. Adopting single VAE model(trained on all data classes)\n",
    "> 2. Adopting individual VAE model for each data classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60dfc3-5254-40e6-b71a-a0a06bd6f34d",
   "metadata": {},
   "source": [
    "#### single VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e7f06-e243-419a-a18d-843058e1aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori['disorder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cc22a-1632-4adb-be4c-4a8b7e36fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_for_classes(vae_model, latent_dim, class_labels, num_samples_per_class):\n",
    "    synthetic_data = {}\n",
    "    for class_label in class_labels:\n",
    "        num_samples = num_samples_per_class[class_label]\n",
    "        ## sampling from latent space\n",
    "        z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "        ## use sampled data to generate synthetic dataset\n",
    "        generated_samples = vae_model.decoder.predict(z_samples)\n",
    "        ## save synthetic dataset with class label\n",
    "        synthetic_data[class_label] = generated_samples\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7835c7-80b2-47c3-949a-5a81f6a1b999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 생성할 클래스 라벨들과 각 클래스별 생성할 샘플 수\n",
    "num_samples_per_class = {0:164, 1:151, 2: 106} ##Set to total 300 for each class\n",
    "\n",
    "# 각 클래스별 synthetic data 생성\n",
    "synthetic_data = generate_synthetic_data_for_classes(vae, args_vae.latent_dim, label, num_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e251b81-d4f1-4c79-97fc-9c95ab0e9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스에 대한 데이터프레임화\n",
    "gen_mdd = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_pd = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "gen_con = pd.DataFrame(synthetic_data[2], columns=data_x.columns)\n",
    "print(f\"The synthetic data size is... \\n\\nSynthetic for MDD class: {gen_mdd.shape[0]} \\nSynthetic for PD class: {gen_pd.shape[0]} \\nSynthetic for control class: {gen_con.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1688b2b-cc7c-4ee3-bd7b-d25e917fe6ad",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded5443-440f-4ab1-9263-a7243f37b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2dbfadc-32b6-4641-99a5-38cf2e3ca0e0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212692c-5901-4188-aaac-f730acb897b9",
   "metadata": {},
   "source": [
    "#### individual VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce9692-9794-48c4-9dfe-87bcb7f9d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_vae(vae_model, latent_dim, num_samples):\n",
    "    ## sampling from latent space\n",
    "    z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "    ## use sampled latent space vector to generate synthetic dataset\n",
    "    synthetic_data = vae_model.decoder.predict(z_samples)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e516a0-0667-4cde-9b53-b19577cdfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mdd, data_pd, data_con = (data_MDD.drop(drop_list, axis=1), data_PD.drop(drop_list, axis=1), data_CON.drop(drop_list, axis=1))\n",
    "\n",
    "data_mdd[:] = (scaler.fit_transform(data_mdd[:])).round(decimals=6)\n",
    "data_pd[:] = (scaler.fit_transform(data_pd[:])).round(decimals=6)\n",
    "data_con[:] = (scaler.fit_transform(data_con[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce85933-2807-438b-9832-d8b0937e8df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MDD 클래스에 대한 VAE 모델 학습\n",
    "vae_mdd = VAE(data_mdd.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_mdd.train(data_mdd, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)\n",
    "\n",
    "# PD 클래스에 대한 VAE 모델 학습\n",
    "vae_pd = VAE(data_pd.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_pd.train(data_pd, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)\n",
    "\n",
    "# Control 클래스에 대한 VAE 모델 학습\n",
    "vae_con = VAE(data_con.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_con.train(data_con, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9a269-5a81-4140-91c7-a82bfea06018",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = {0:164, 1:151, 2: 106}\n",
    "\n",
    "num_samples_mdd = num_samples_per_class[0]\n",
    "synthetic_data_mdd = generate_class_synthetic_data_vae(vae_mdd, args_vae.latent_dim, num_samples_mdd)\n",
    "\n",
    "num_samples_pd = num_samples_per_class[1]\n",
    "synthetic_data_pd = generate_class_synthetic_data_vae(vae_pd, args_vae.latent_dim, num_samples_pd)\n",
    "\n",
    "num_samples_con = num_samples_per_class[2]\n",
    "synthetic_data_con = generate_class_synthetic_data_vae(vae_con, args_vae.latent_dim, num_samples_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abe0d1-b678-4307-b31d-d781681a3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_mdd = pd.DataFrame(synthetic_data_mdd, columns=data_x.columns)\n",
    "gen_pd = pd.DataFrame(synthetic_data_pd, columns=data_x.columns)\n",
    "gen_con = pd.DataFrame(synthetic_data_con, columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277f2a8-02ad-45f0-bf64-c9e2d5e0f2f2",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d3851-908f-4687-bcfa-e38b4f1a052e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9743ac3-520e-4654-ac30-3534ceff5881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95411c46-079c-4c9c-ab27-ff0efab3ebb8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995646d-680f-4e9b-82de-d2a5c100b308",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e2c7a-98b9-4dd5-bc53-cd541a0abf17",
   "metadata": {},
   "source": [
    "## Conditional VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9b771-ad71-492c-bdc7-ec62558c9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_cvae:\n",
    "    # arugments\n",
    "    epochs=150\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_cvae = Args_cvae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358116f-32bc-47c1-8a9b-e175f214d8e3",
   "metadata": {},
   "source": [
    "> preparing x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7196dd-d531-4896-b879-0c922ff17877",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cvae = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b83362-5a42-434b-b671-92b322f99b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## relabeling data_cvae's disorder variable to use them as an conditional input\n",
    "data_cvae['disorder'] = data_cvae['disorder'].replace({'mdd': 0})\n",
    "data_cvae['disorder'] = data_cvae['disorder'].replace({'pd' : 1})\n",
    "data_cvae['disorder'] = data_cvae['disorder'].replace({'con': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb0103-1f46-46ec-86dd-1a78c5cbbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list = ['age', 'gender', 'disorder']\n",
    "drop_list = scale_list + etc_list + demo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ef456-4692-4992-8cf6-900f097323e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## x: input, y: output, c: conditional inputs\n",
    "x = data_cvae.drop(drop_list, axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_cvae.disorder\n",
    "c = data_cvae.loc[:, ('age', 'gender', 'disorder')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac578c-a68a-4815-bbd0-d37c1557c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6)\n",
    "###################\n",
    "label = y.copy()\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding\n",
    "###################\n",
    "data_c = c.copy()\n",
    "data_c[:] = (scaler.fit_transform(data_c[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b262b-66df-4686-87cf-2422fb53b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)\n",
    "print(\"The size of c dataset is:\", data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78067da8-db50-4a35-b7f1-88bcb5867291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4584f46-9dbc-41bb-9b51-4e8dbcae0010",
   "metadata": {},
   "source": [
    "> CVAE model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5dfbd-6c87-42e2-9d55-f7ab6b71b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE:\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        ## initializing model\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        ## generating encoder and decoder section\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.cvae = self.build_cvae()\n",
    "    \n",
    "    ## defining encoder function\n",
    "    def build_encoder(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        ## defining input layer: put x and condition c together\n",
    "        combined_input = layers.Concatenate()([x_input, c_input])\n",
    "        \n",
    "        # encoder layers defining\n",
    "        h = layers.Dense(64, activation=None)(combined_input)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)\n",
    "        \n",
    "        h = layers.Dense(32, activation=None)(h)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)\n",
    "        \n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(h)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(h)\n",
    "        \n",
    "        ## sampling (reparameterization)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        z = layers.Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        return Model([x_input, c_input], [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    ## defining decoder function\n",
    "    def build_decoder(self):\n",
    "        z_input = layers.Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        ## decoder input defining: put latent space input and conditional c together\n",
    "        combined_input = layers.Concatenate()([z_input, c_input])\n",
    "\n",
    "        # decoder layers defining\n",
    "        h = layers.Dense(32, activation=None)(combined_input)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)  # Dropout with 30% rate\n",
    "        \n",
    "        h = layers.Dense(64, activation=None)(h)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)  # Dropout with 30% rate\n",
    "        \n",
    "        x_decoded = layers.Dense(self.input_dim, activation='sigmoid', name='output')(h)\n",
    "        \n",
    "        return Model([z_input, c_input], x_decoded, name='decoder')\n",
    "    \n",
    "    ## build and compile the CVAE model\n",
    "    def build_cvae(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        z_mean, z_log_var, z = self.encoder([x_input, c_input])\n",
    "        x_decoded = self.decoder([z, c_input])\n",
    "        \n",
    "        cvae = Model([x_input, c_input], x_decoded, name='cvae')\n",
    "        \n",
    "        reconstruction_loss = mse(x_input, x_decoded)\n",
    "        reconstruction_loss *= self.input_dim\n",
    "        \n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        cvae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        cvae.add_loss(cvae_loss)\n",
    "        cvae.compile(optimizer='adam')\n",
    "        \n",
    "        return cvae\n",
    "    \n",
    "    def encode(self, data, condition):\n",
    "        z_mean, z_log_var, z = self.encoder.predict([data, condition])\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "    def decode(self, latent, condition):\n",
    "        return self.decoder.predict([latent, condition])\n",
    "    \n",
    "    ## model training\n",
    "    def train(self, x_train, c_train, epochs, batch_size, validation_split=None):\n",
    "        self.cvae.fit([x_train, c_train], epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "    \n",
    "    ## synthetic data generation (for data augmentation)\n",
    "    def generate_synthetic_data(self, condition, n_samples):\n",
    "        z_samples = np.random.normal(size=(n_samples, self.latent_dim))\n",
    "        synthetic_data = self.decoder.predict([z_samples, condition])\n",
    "        return synthetic_data\n",
    "\n",
    "\n",
    "    def visualize_latent_space(self, x_data, c_data, labels, n_samples):\n",
    "        c_data = np.array(data_c)\n",
    "        n_samples = min(n_samples, len(x_data), len(c_data))\n",
    "        indices = np.random.choice(len(x_data), n_samples, replace=False) ## data sampling for n\n",
    "        x_sample = x_data.iloc[indices]\n",
    "        c_sample = c_data[indices]\n",
    "        z_mean, _, _ = self.encoder.predict([x_sample, c_sample]) ## calculating latent vector z\n",
    "        tsne = TSNE(n_components=2, random_state=710674) #tsne visualization\n",
    "        z_tsne = tsne.fit_transform(z_mean)\n",
    "\n",
    "        ## visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if labels is not None:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=labels[indices], cmap='viridis')\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1])\n",
    "        plt.title('t-SNE visualization of the latent space')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb698af1-02a1-42f9-99b2-10b84c29e61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvae = CVAE(input_dim = data_x.shape[1], condition_dim = data_c.shape[1], latent_dim=args_cvae.latent_dim)\n",
    "cvae.train(data_x, data_c, epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb68103-ec3e-4bbb-97e9-111b287134d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b307b6f-6507-4b5c-9a48-5189c76de85d",
   "metadata": {},
   "source": [
    "> Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a88eb1-3c0a-4ed7-a960-b4f28db3d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.visualize_latent_space(data_x, data_c, label, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89d066-5ef1-43e6-9d62-3721bc8b15d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60811c06-abb8-4ae6-8ffc-3a1d1c034c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "250d704d-a4bc-466d-923b-3ba46eb6adf2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28d859-6da8-4689-9015-64a6ed57f6e6",
   "metadata": {},
   "source": [
    "> Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c22a4-2c6b-4fd8-8a2d-4777c2ea060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd3c54-22b0-451a-b700-7f72ea906575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a859cc-609e-4221-ba57-a36c67cae09e",
   "metadata": {},
   "source": [
    "#### Using single CVAE model (integrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327c748-b69d-48f8-bbf9-b17b8a6fdcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data(cvae_model, c_data, n_samples_per_class, labels, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## filtering the condition for each class\n",
    "        class_indices = labels == class_label\n",
    "        class_conditions = c_data[class_indices]\n",
    "        \n",
    "        if class_conditions.empty:\n",
    "            print(f\"Warning: No data found for class label {class_label}. Skipping this class.\")\n",
    "            continue\n",
    "        \n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            class_new_conditions = class_conditions.sample(n=n_samples, replace=True).values\n",
    "        ## calculate mean and variance from original condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = class_conditions.mean(axis=0).values\n",
    "            std = class_conditions.std(axis=0).values\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953103f1-47aa-4fcc-9149-3741d58ae38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0:164, 1:151, 2: 106}\n",
    "## synthetic data generation (augmentation)\n",
    "# synthetic_data = generate_class_synthetic_data(cvae, data_c, n_samples_per_class, label, method='random')\n",
    "synthetic_data = generate_class_synthetic_data(cvae, data_c, n_samples_per_class, label, method='gaussian')\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0(MDD) - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(PD) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(CON) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0ccfc-39f5-4aac-b8d9-bb2d77da638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_mdd = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_pd = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "gen_con = pd.DataFrame(synthetic_data[2], columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9597c3-20d3-4d2f-92d9-906a63768324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ae357a-e499-4c63-b960-127dc31cc4be",
   "metadata": {},
   "source": [
    "====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79db0e-193b-49f1-903d-efb16df100a7",
   "metadata": {},
   "source": [
    "#### Using individual CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c11fdb-1e09-4d8f-bf39-b719c207c08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## we should train individual CVAE model for each class\n",
    "cvae_models = {}\n",
    "\n",
    "for class_label in np.unique(label):\n",
    "    ## data filtering to find dataset with that class label\n",
    "    class_indices = np.where(label == class_label)[0]\n",
    "    class_data = data_x.iloc[class_indices]\n",
    "    \n",
    "    ## initialize and train CVAE model\n",
    "    cvae = CVAE(input_dim=data_x.shape[1], latent_dim=args_cvae.latent_dim, condition_dim=data_y.shape[1])\n",
    "    cvae.train(class_data, data_y[class_indices], epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)\n",
    "    \n",
    "    ## save the trained model\n",
    "    cvae_models[class_label] = cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a008b0-8826-4d9a-8644-106754f472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_seperately(cvae_models, c_data, n_samples_per_class, labels, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## loading trained CVAE model for each class\n",
    "        cvae_model = cvae_models[class_label]\n",
    "        \n",
    "        ## filtering the condition for the class\n",
    "        class_indices = labels == class_label\n",
    "        class_conditions = c_data[class_indices]\n",
    "\n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            class_new_conditions = class_conditions.sample(n=n_samples, replace=True).values\n",
    "        ## calculate mean and variance from origianl condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = class_conditions.mean(axis=0).values\n",
    "            std = class_conditions.std(axis=0).values\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    # return synthetic_data, new_conditions\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21353-7556-4997-8e54-caa1a82657fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0:164, 1:151, 2: 106}\n",
    "\n",
    "synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_c, n_samples_per_class, label, method=\"random\")\n",
    "# synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_c, n_samples_per_class, label, method=\"gaussian\")\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0(MDD) - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(PD) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(CON) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be256307-b7a2-46f4-b9dc-f802cabed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_mdd = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_pd = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "gen_con = pd.DataFrame(synthetic_data[2], columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915af965-9fdb-4cd5-9c15-3f35767e8b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc6eb8f-52e9-4376-83f1-078ccd4f12f9",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674ff09-5db3-4fed-bc2a-73e3e52014c5",
   "metadata": {},
   "source": [
    "> Reconstruct original dataset using trained model to adopt augmented trainig dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69f6e-c4a0-40b0-a1d3-28a848beb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, z_latent = cvae.encode(data_x, data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff0d62-6fc1-49f3-a942-4de6e40a1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = cvae.decode(z_latent, data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ad27b-0745-49aa-aa0a-321622529aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81749b-fbbc-427d-aa50-42597c245810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159035-db8f-414b-a9d2-d322c21044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8491406-0098-4a01-bfe1-68caacbaefb3",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9b6ef-3771-49a3-b4a5-df9c25e7bb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c356d3-bcaa-4389-a583-4c5fab1dce89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264f9bb-bc66-40a9-90c9-0bb4bdc25965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2aad34-7583-4996-aaee-cd00c2241feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50635f7-443e-45af-8a07-b55c9ced68cf",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd232e5c-c1a7-4d8f-8144-14dc2bf64f50",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8736b-d296-4c8d-aace-9dd5d109a0cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Latent Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788b712-d306-4b80-9017-b89025cd26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_diffusion:\n",
    "    # arugments\n",
    "    epochs=80\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    latent_dim = 4\n",
    "    seed=710674\n",
    "\n",
    "args_diffusion = Args_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e0517-bac5-41f1-9ea5-574aa3a05e4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "* preparing x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51d8f6-f44c-4a88-bda9-5f174523aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diffusion = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234de504-6572-4337-8ecc-ac194e367ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CON = data_diffusion[data_diffusion['disorder']==\"con\"]\n",
    "data_PD = data_diffusion[data_diffusion['disorder']==\"pd\"]\n",
    "data_MDD = data_diffusion[data_diffusion['disorder']==\"mdd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55cb8e-fc72-4bd4-bf13-c6ef1f5e3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list = ['age', 'gender', 'disorder']\n",
    "drop_list = scale_list + etc_list + demo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0577fb-4861-4013-ae60-d300183aa2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_diffusion.drop((drop_list), axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_diffusion.disorder# mdd / pd / con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043a2b4-d4de-4aad-b1ab-3e114940842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'mdd': 0})\n",
    "label = label.replace({'pd': 1})\n",
    "label = label.replace({'con' : 2})\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61450ab-71aa-4ac2-99f1-43d3520a5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ed243-4f13-448b-9c57-3d834db8bc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6323d725-767c-4267-81df-536e5ded49a1",
   "metadata": {},
   "source": [
    "* 1 Dimensional latent diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad250c5-f929-4c28-bb6f-57c3c603bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionVAE:\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.vae = self.build_vae()\n",
    "        \n",
    "    ## sampling - reparameterization\n",
    "    def sampling(self, repara):\n",
    "        z_mean, z_log_var = repara\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        z_mean = layers.Dense(self.latent_dim)(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim)(x)\n",
    "        return Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "    def build_decoder(self):\n",
    "        latent_inputs = Input(shape=(self.latent_dim,))\n",
    "        x = layers.Dense(128, activation='relu')(latent_inputs)\n",
    "        x = layers.Dense(np.prod(self.input_shape), activation='relu')(x)\n",
    "        x = layers.Reshape(self.input_shape)(x)\n",
    "        outputs = layers.Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "        return Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    def build_vae(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = layers.Lambda(self.sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "        outputs = self.decoder(z)\n",
    "\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "        # Define the VAE loss\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, outputs)\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        vae_loss = reconstruction_loss + kl_loss\n",
    "        vae.add_loss(vae_loss)\n",
    "        vae.compile(optimizer='adam')\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def train(self, data_x, epochs, batch_size, validation_split=0.2, verbose=2):\n",
    "        self.vae.fit(data_x, epochs=epochs, validation_split=validation_split, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    def generate(self, latent_sample):\n",
    "        return self.decoder.predict(latent_sample)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming data_x and args_diffusion are already defined\n",
    "\n",
    "input_shape = (data_x.shape[1], 1)\n",
    "latent_dim = args_diffusion.latent_dim\n",
    "\n",
    "vae_model = LatentDiffusionVAE(input_shape, latent_dim)\n",
    "vae_model.train(data_x, epochs=args_diffusion.epochs, batch_size=args_diffusion.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749719a-3384-4aa1-8339-1c6e0f6b98c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49bee9-771d-45ae-80be-214694620a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e4ec70-9398-4957-9e9f-3afcf76cf4c7",
   "metadata": {},
   "source": [
    "--- working code (start) --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe80ac-7856-4761-aa36-e580cf9e024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the latent space\n",
    "def sampling(repara):\n",
    "    z_mean, z_log_var = repara\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define encoder\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    return Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "# Define decoder\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(128, activation='relu')(latent_inputs)\n",
    "    x = Dense(np.prod(output_shape), activation='relu')(x)\n",
    "    x = Reshape(output_shape)(x)\n",
    "    outputs = Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "    return Model(latent_inputs, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dba56-edca-46c7-a855-1d38223010ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (data_x.shape[1], 1)  # Example input shape\n",
    "latent_dim = args_diffusion.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b2e74-1c33-412d-8922-6172b7923bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define the VAE\n",
    "encoder = build_encoder(input_shape, latent_dim)\n",
    "decoder = build_decoder(input_shape, latent_dim)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "z_mean, z_log_var = encoder(inputs)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "outputs = decoder(z)\n",
    "\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "# Define the VAE loss\n",
    "reconstruction_loss = MeanSquaredError()(inputs, outputs)\n",
    "kl_loss = -0.5 * tf.reduce_mean(\n",
    "    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dc914-b23c-427a-b90f-cc5e3a0dfc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb65314-efc8-4b76-b8a9-780e86f5123a",
   "metadata": {},
   "source": [
    "* model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e2eab-5606-4188-9d1b-23fdad6c2c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "vae.fit(data_x, epochs=args_diffusion.epochs, validation_split=0.2, batch_size=args_diffusion.bs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22559155-2211-4879-ad88-819bc3a56e34",
   "metadata": {},
   "source": [
    "--- working code (end) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56313a0-e1e4-41c3-8043-3083af56d4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75487eb-4981-48dc-8057-21a92f2f7285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a3418-2b88-4a91-95b8-52c1292ab49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905fb40-d68d-4a01-b73d-60acfbd944bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff6241-7e4b-4827-a1f6-44ba65133932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb965bcb-a722-454f-b3ad-498d54d08094",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981c57e-13b4-4b5e-aa19-c6cfe91c989a",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b774b0d-996a-492a-8930-56b8498703c4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d74df3-ad0e-4d1e-903f-ba88f4613b8e",
   "metadata": {},
   "source": [
    "# Classification performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ea892-6b25-42dc-a749-143b32efbea8",
   "metadata": {},
   "source": [
    "## Original dataset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32baa1-3aad-4685-adcd-06aebe9b98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dnn = data_ori.copy()\n",
    "data_dnn = data_dep.copy()\n",
    "# data_dnn = data_anx.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9f402-32ba-4a8f-a71e-7be2a1d3f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dnn.depression.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3899c-0c1f-4ac2-8fc2-a0ce231a6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dnn.drop(drop_list, axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_dnn.depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43146e-8e4b-40e8-8e2a-d2af107cc8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb11a2-17f9-4bec-9ec3-c1a82325f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'normal': 0})\n",
    "label = label.replace({'mild': 1})\n",
    "label = label.replace({'moderate': 2})\n",
    "label = label.replace({'severe': 3})\n",
    "data_y = to_categorical(label, 4) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e78f59-c643-476e-9f0c-37c3fec6b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8c93e-3b36-44db-be80-ba7b082b58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## definition for computing class weight (to solve class imbalance issue)\n",
    "def compute_class_weights(y):\n",
    "    class_counts = np.bincount(y) ## calculating data point for each class\n",
    "    total_samples = len(y) ## total data points\n",
    "    ## computing each class weight\n",
    "    class_weights = {i: total_samples / (len(class_counts) * class_count) \n",
    "                     for i, class_count in enumerate(class_counts)}\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385e133-6da1-4443-936c-c26a81b9c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weights(label)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eec954-0f97-458b-8f38-38f0c1430dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa41a9dd-4361-49e2-bd64-ea3cb5c71d67",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaaf7d3-f1da-4632-bb89-c31e45e79ee4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Augmented dataset add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23165eff-832d-482a-b4eb-7915a4c29b26",
   "metadata": {},
   "source": [
    "> we generate synthesized dataset using VAE from above. \\\n",
    "> adopted original datset: data_mdd, data_bpi, data_bpii \\\n",
    "> synthesized into: gen_mdd, gen_bpi, gen_bpii \\\n",
    "> gen_control is not generated since \"control group\" has biggest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daafef-9b75-4899-8111-f44278b6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class synthetic_data_preparation:\n",
    "    def __init__(self, original_data, gen_data, label_col, id_cols=None, drop_cols=None):\n",
    "        \"\"\"\n",
    "        original_data (pandas.DataFrame): original dataset\n",
    "        gen_data (dict): data dictionary for synthetic dataset (ex: {0: gen_A, 1: gen_B})\n",
    "        label_col (str): label column (target variable)\n",
    "        id_cols (list): column list to drop (예: ['Unnamed: 32', 'id'])\n",
    "        drop_cols (list): column list to drop from original dataset\n",
    "        \"\"\"\n",
    "        self.original_data = original_data\n",
    "        self.gen_data = gen_data\n",
    "        self.label_col = label_col\n",
    "        self.id_cols = id_cols if id_cols is not None else []\n",
    "        self.drop_cols = drop_cols if drop_cols is not None else []\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        ## preprocessing original dataset\n",
    "        data_dnn = self.original_data.copy()\n",
    "        for col in self.id_cols:\n",
    "            if col in data_dnn.columns:\n",
    "                data_dnn = data_dnn.drop(col, axis=1)\n",
    "        \n",
    "        data_dnn = data_dnn.drop(self.drop_cols, axis=1)\n",
    "        data_classes = {label: data_dnn[data_dnn[self.label_col] == label] for label in data_dnn[self.label_col].unique()}\n",
    "        \n",
    "        ## preprocessing synthetic generated dataset\n",
    "        for label, df in self.gen_data.items():\n",
    "            df[self.label_col] = label\n",
    "            if self.drop_cols:\n",
    "                df = df.drop(self.drop_cols, axis=1)\n",
    "            self.gen_data[label] = df\n",
    "        \n",
    "        ## concat original dataset and generated dataset\n",
    "        ori_df_concat = pd.concat(list(data_classes.values()), ignore_index=True)\n",
    "        gen_df_concat = pd.concat(list(self.gen_data.values()), ignore_index=True)\n",
    "        \n",
    "        ## separating feature and label(target)\n",
    "        ori_x = ori_df_concat.drop([self.label_col], axis=1)\n",
    "        ori_y = ori_df_concat[[self.label_col]]\n",
    "        \n",
    "        gen_x = gen_df_concat.drop([self.label_col], axis=1)\n",
    "        gen_y = gen_df_concat[[self.label_col]]\n",
    "        \n",
    "        ## filling missing values and scaling\n",
    "        ori_x = ori_x.fillna(ori_x.mean())\n",
    "        ori_x[:] = (scaler.fit_transform(ori_x[:])).round(decimals=6) ## scaling x values\n",
    "        \n",
    "        # 레이블 인코딩 및 원-핫 인코딩\n",
    "        ori_y = ori_y.replace({self.label_col: {label: idx for idx, label in enumerate(ori_y[self.label_col].unique())}})\n",
    "        y_ori = to_categorical(ori_y, num_classes=len(ori_y[self.label_col].unique()))\n",
    "        \n",
    "        gen_y = gen_y.replace({self.label_col: {label: idx for idx, label in enumerate(gen_y[self.label_col].unique())}})\n",
    "        y_gen = to_categorical(gen_y, num_classes=len(gen_y[self.label_col].unique()))\n",
    "        \n",
    "        ## separating training and test dataset\n",
    "        x_trainset, x_test, y_trainset, y_test = train_test_split(ori_x, y_ori, test_size=0.35, random_state=710674)\n",
    "        \n",
    "        ## add generated dataset to training dataset\n",
    "        x_train_concat = pd.concat([x_trainset, gen_x], ignore_index=True)\n",
    "        y_train_concat = np.concatenate([y_trainset, y_gen])\n",
    "        \n",
    "        ## separating training dataset and validation dataset\n",
    "        x_train, x_vali, y_train, y_vali = train_test_split(x_train_concat, y_train_concat, test_size=0.2, random_state=710674)\n",
    "        \n",
    "        return x_train, x_vali, x_test, y_train, y_vali, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9528204-5e13-47a1-9266-dd4488568e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215632f-f314-4022-84a0-1d6769e779ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list_ = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list_ = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list_ = ['age', 'gender']\n",
    "drop_list_ = scale_list_ + etc_list_ + demo_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f4218-b67d-46d3-86ed-47cc98ac34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a dictionary for generated dataset\n",
    "gen_data = {\"mdd\":gen_mdd, \"pd\":gen_pd, \"con\":gen_con}\n",
    "\n",
    "## data_preparation class initialize and data prepare\n",
    "data_prep = synthetic_data_preparation(data_ori, gen_data, label_col='disorder', id_cols=drop_list_)\n",
    "x_train, x_vali, x_test, y_train, y_vali, y_test = data_prep.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc9bd3-3c49-4f07-ac1a-6689570947fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9ddb173-633a-43d0-9f51-32ae7cb6fec5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c2ee7-3221-4217-8794-c19a904f91d5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2bfda-069a-47e9-9ce5-d579238f3abd",
   "metadata": {},
   "source": [
    "## Simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedc655-0b50-4bc7-b3d6-0832d939efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_dnn:\n",
    "    # arugments\n",
    "    epochs=200\n",
    "    bs=32\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_classes= 4\n",
    "    seed=710674\n",
    "\n",
    "args_dnn = Args_dnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bade4d-02ae-4a3b-ae78-3441f5c033b7",
   "metadata": {},
   "source": [
    "* DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859ec14-825f-4665-8dcc-4c4054076f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDNN:\n",
    "    def __init__(self, input_dim, layer_configs, output_units, output_activation='softmax'):\n",
    "        self.input_dim = input_dim ## input data dimensionality (# variables)\n",
    "        self.layer_configs = layer_configs ## hidden layer/sequential layer lists (units, activation, batch_norm, dropout_rate)\n",
    "        self.output_units = output_units ## output unit no.\n",
    "        self.output_activation = output_activation ## activation function for output layer\n",
    "        self.model = self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], activation=self.layer_configs[0]['activation'], input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            ## AddLearningRateScheduler callback\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "        \n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_split=None, class_weight=None, validation_data=None):\n",
    "        return self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split,\n",
    "                             class_weight=class_weight, validation_data = validation_data, callbacks=self.callbacks)\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8723a2-895c-43b4-b031-c869d0f2af3b",
   "metadata": {},
   "source": [
    "* optimization function, model compile, and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28545248-f087-4bc1-9a84-7262b9179abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_learning_rate(epoch, mode='cyclic', base_lr=0.001, max_lr=0.006, step_size=2000, gamma=0.99994):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    \n",
    "    if mode == 'cyclic' or mode == 'triangular':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n",
    "    elif mode == 'triangular2':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) / (2 ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) * (gamma ** epoch)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose from 'cyclic', 'triangular', 'triangular2', or 'exp_range'.\")\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705bb04-1f9d-49d7-8d45-fd99ca74ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = dynamic_learning_rate(epoch=1000, mode='cyclic')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular2')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='exp_range', gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8cd8b1-69c5-47e7-8b08-568f635887f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a13af6-6827-4fae-adcf-d37f47ae7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.3},\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.3},\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2},\n",
    "    {'units': 16, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2},\n",
    "    {'units': 8, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2}\n",
    "]\n",
    "\n",
    "model = SimpleDNN(output_units=args_dnn.num_classes, input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfa22a-1667-4b9e-b0c2-691d71236332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=50)\n",
    "model.compile_model(optimizer = opt, lr_scheduler=scheduler)\n",
    "\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975db5d-b42e-4329-9f11-7f9b70e426ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### model training on training dataset\n",
    "# history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, validation_split=0.2)\n",
    "history = model.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, class_weight = class_weight, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7961f-5799-4074-9760-5f0b6508982a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bba6c6d9-4109-460b-974e-991c3d3e4b0b",
   "metadata": {},
   "source": [
    "* Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2dfb7a-0f46-41e4-9776-e5a11cef658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_test, y_test):\n",
    "\n",
    "    ## predict on model\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_predict_classes = np.argmax(y_predict, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    ## calculate confusion matrix and visualize\n",
    "    cm = confusion_matrix(y_test_classes, y_predict_classes, normalize='pred')\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, fmt='.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    ## evaluation metrics\n",
    "    accuracy = accuracy_score(y_test_classes, y_predict_classes)\n",
    "    precision = precision_score(y_test_classes, y_predict_classes, average='macro')\n",
    "    recall = recall_score(y_test_classes, y_predict_classes, average='micro')\n",
    "    f1 = f1_score(y_test_classes, y_predict_classes, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_predict, multi_class='ovr')\n",
    "    \n",
    "    ## Results\n",
    "    print(\"=============================================\")\n",
    "    print(f\"The overall accuracy is: {accuracy:.4f}\")\n",
    "    print(f\"The precision score is: {precision:.4f}\")\n",
    "    print(f\"The recall score is: {recall:.4f}\")\n",
    "    print(f\"The F1 score is: {f1:.4f}\")\n",
    "    print(f\"The AUC score is: {auc:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    ## Print out the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee306ab-7e58-4dbe-ae53-81288849ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d88a1-b1b6-4386-a3c8-9bdde0cef008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c45e383-4895-4dda-813c-ead039ec9009",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91ad0f-2d8f-49a7-8b97-85cd38696300",
   "metadata": {},
   "source": [
    "## K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f292e-fbce-486e-8e2f-9b87e479afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_kfold:\n",
    "    # arugments\n",
    "    epochs=70\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    split=5\n",
    "    seed=710674\n",
    "\n",
    "args_kfold = Args_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e25cb-fbad-4575-8461-d42ce17af307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kfoldMLP:\n",
    "    def __init__(self, input_dim, layer_configs, output_units, output_activation='softmax'):\n",
    "        self.input_dim = input_dim\n",
    "        self.layer_configs = layer_configs\n",
    "        self.output_units = output_units\n",
    "        self.output_activation = output_activation\n",
    "        self.callbacks = []\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], \n",
    "                               activation=self.layer_configs[0]['activation'], \n",
    "                               input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        \n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        self.model = model\n",
    "\n",
    "    ## model compile\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            self.callbacks.append(tf.keras.callbacks.LearningRateScheduler(lr_scheduler))\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_data=None, class_weight=None, verbose=0):\n",
    "        return self.model.fit(\n",
    "            x_train, y_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=validation_data, \n",
    "            callbacks=self.callbacks, \n",
    "            class_weight=class_weight,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.model.predict(x)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def cross_validate(self, x_data, y_data, n_splits, epochs, batch_size, optimizer=None, class_weight=None):\n",
    "        x_data = np.array(x_data)\n",
    "        y_data = np.array(y_data)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True) ##set fold split and shuffle\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(x_data), 1):\n",
    "            x_train, x_val = x_data[train_index], x_data[val_index]\n",
    "            y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "\n",
    "            ## build new model\n",
    "            self.build_model()\n",
    "            self.compile_model(optimizer=optimizer)\n",
    "            \n",
    "            ## model training\n",
    "            self.fit_model(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), class_weight=class_weight, verbose=0)\n",
    "            \n",
    "            ## evaluate the model\n",
    "            y_val_pred = self.predict(x_val)\n",
    "            if isinstance(y_val_pred, list):\n",
    "                y_val_pred = np.array(y_val_pred)\n",
    "            y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "            y_val_true = np.argmax(y_val, axis=1)\n",
    "            \n",
    "            accuracy = accuracy_score(y_val_true, y_val_pred)\n",
    "            precision = precision_score(y_val_true, y_val_pred, average='macro')\n",
    "            recall = recall_score(y_val_true, y_val_pred, average='macro')\n",
    "            f1 = f1_score(y_val_true, y_val_pred, average='weighted')\n",
    "            auc = roc_auc_score(y_val, self.predict(x_val), multi_class='ovr')\n",
    "            \n",
    "            fold_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'auc': auc\n",
    "            })\n",
    "\n",
    "            print(f\"Fold metrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1 Score={f1:.4f}, AUC={auc:.4f}\")\n",
    "        \n",
    "        avg_metrics = {key: np.mean([m[key] for m in fold_metrics]) for key in fold_metrics[0].keys()}\n",
    "        \n",
    "        print(\"\\nAverage metrics across all folds:\")\n",
    "        for metric, value in avg_metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        return fold_metrics, avg_metrics\n",
    "\n",
    "    ## evaluation on test dataset (for performance check)\n",
    "    def evaluate_test_data(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true_labels, y_pred_labels, normalize='pred')\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        print(\"=============================================\")\n",
    "\n",
    "        # Classification Report\n",
    "        class_report = classification_report(y_true_labels, y_pred_labels, target_names=[f'Class {i}' for i in range(self.output_units)])\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        return cm, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e0691-d62b-481a-86f0-58486dc577b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': False, 'dropout_rate': 0.3},\n",
    "    {'units': 16, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.2}\n",
    "]\n",
    "\n",
    "kfold = kfoldMLP(output_units=args_kfold.num_classes, input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc1dce-e3a0-4b02-a443-8e1c8651136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "\n",
    "## model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67247754-33e6-4083-b5b3-a1bd47e88686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-Fold 교차 검증 실행\n",
    "metrics, avg_metrics = kfold.cross_validate(x_train, y_train, n_splits=args_kfold.split, optimizer=opt, epochs=args_kfold.epochs, batch_size=args_kfold.bs)\n",
    "\n",
    "# # 각 폴드의 성능 지표 출력\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     print(f\"Fold {i+1}: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee90fdf-5b26-4e4c-935f-3780eb5fe944",
   "metadata": {},
   "source": [
    "* Evaluation on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bde173-0483-46c3-9640-62c675a5c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, class_report = kfold.evaluate_test_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e8976-a689-4a0c-b744-0732f1808cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cefa5011-78f0-4c2c-b4de-6df4c4bf26d1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9453da-c2ef-4fb0-9817-68321d13236c",
   "metadata": {},
   "source": [
    "## 1D-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34654d7e-8592-45af-a55c-a4af89366dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_conv:\n",
    "    # arugments\n",
    "    epochs=70\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    split=5\n",
    "    seed=710674\n",
    "\n",
    "args_conv = Args_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12e02c-30c4-4729-8086-7c80830eb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1Dmodel:\n",
    "    def __init__(self, input_shape, conv_layers, dense_layers, output_units, output_activation='softmax'):\n",
    "        self.input_shape = input_shape\n",
    "        self.conv_layers = conv_layers\n",
    "        self.dense_layers = dense_layers\n",
    "        self.output_units = output_units\n",
    "        self.output_activation = output_activation\n",
    "        self.model = None\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add Conv1D layers\n",
    "        for i, config in enumerate(self.conv_layers):\n",
    "            if i == 0:\n",
    "                model.add(Conv1D(\n",
    "                    filters=config['filters'],\n",
    "                    kernel_size=config['kernel_size'],\n",
    "                    activation=config['activation'],\n",
    "                    input_shape=self.input_shape\n",
    "                ))\n",
    "            else:\n",
    "                model.add(Conv1D(\n",
    "                    filters=config['filters'],\n",
    "                    kernel_size=config['kernel_size'],\n",
    "                    activation=config['activation']\n",
    "                ))\n",
    "\n",
    "            if 'pool_size' in config:\n",
    "                model.add(MaxPooling1D(pool_size=config['pool_size']))\n",
    "\n",
    "        # Add Flatten layer\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # Add Dense layers\n",
    "        for config in self.dense_layers:\n",
    "            model.add(Dense(\n",
    "                units=config['units'],\n",
    "                activation=config['activation']\n",
    "            ))\n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(BatchNormalization())\n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(Dropout(rate=config['dropout_rate']))\n",
    "\n",
    "        # Add Output layer\n",
    "        model.add(Dense(self.output_units, activation=self.output_activation))\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_data=None, class_weight=None, verbose=1):\n",
    "\n",
    "        return self.model.fit(\n",
    "            x_train, y_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=validation_data, \n",
    "            callbacks=self.callbacks, \n",
    "            class_weight=class_weight,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        if self.model:\n",
    "            self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a2382-f2a9-4246-bd08-a713d8903944",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define layer configurations\n",
    "conv_layers = [\n",
    "    {'filters': 32, 'kernel_size': 5, 'activation': 'relu', 'pool_size': 3},\n",
    "    {'filters': 16, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 3}\n",
    "]\n",
    "\n",
    "dense_layers = [\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 32, 'activation': 'relu'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8af224-a961-46fb-be8c-295610f17ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train.shape[1], 1)\n",
    "conv1d = Conv1Dmodel(input_shape = input_shape, conv_layers=conv_layers, dense_layers=dense_layers, output_units=args_conv.num_classes)\n",
    "conv1d.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e2a57-f3b5-4406-9308-cb0f6e35c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5556c9-5f1d-4341-80de-f616bf8491c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "\n",
    "conv1d.compile_model(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efc81e-f9af-45ef-927a-836cd57750e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = conv1d.fit_model(x_train, y_train, epochs=args_conv.epochs, batch_size=args_conv.bs, class_weight=class_weight, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff654a-1dbd-4148-8946-83b311ba8db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faa0c856-324b-4e49-aebe-6ce91e9f333e",
   "metadata": {},
   "source": [
    "* Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bf5a0-c722-4a58-b03b-9b745f4f0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(conv1d, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca50a3f-9437-472b-aa31-f2aec5fcaf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d42755-d708-49c3-9a73-802f899ed220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f34a41bc-3458-49be-b41e-f65006482740",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d908ff-13e6-4cca-988a-7c03231a5f36",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c55d39-853c-4c83-b58c-62a1fce5669a",
   "metadata": {},
   "source": [
    "## Sub-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7bd663-8112-4d7e-a58f-08d50c8a1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_multi:\n",
    "    # arugments\n",
    "    epochs=80\n",
    "    enc_epochs = 50\n",
    "    bs=32\n",
    "    enc_bs = 16\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    seed=710674\n",
    "\n",
    "args_multi = Args_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0647e-a851-4618-9a38-31d17d8791a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_, data_y_ = data_x.copy(), data_y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e8d60-457a-4031-a650-8589268462a4",
   "metadata": {},
   "source": [
    "* preprocessing the partial dataframes for subnetwork training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa61aa-aba6-4194-9305-81f001fd746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subnetwork_preprocess:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data_dict = {}\n",
    "\n",
    "    def select_columns(self, prefixes):\n",
    "        self.data_dict = {prefix: self.data.loc[:, self.data.columns.str.startswith(f'{prefix}_')] \n",
    "                          for prefix in prefixes}\n",
    "        return self.data_dict\n",
    "\n",
    "    def create_inputs(self):\n",
    "        self.inputs = {name: Input(shape=(data.shape[1],)) for name, data in self.data_dict.items()}\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f473c-0761-4ad6-99b2-826df3bda8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining prefixies (criteria for variable selection)\n",
    "prefixes = ['b1', 's', 'b2', 'r', 'b3', 'c']\n",
    "\n",
    "## pass dataframe to generate class instance\n",
    "processor = subnetwork_preprocess(data_x_)\n",
    "\n",
    "## data columns selection based on the prefix\n",
    "data_dict = processor.select_columns(prefixes)\n",
    "\n",
    "## create input layers(for keras) with selected prefix inputs\n",
    "inputs = processor.create_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124feef-173c-4f67-9bb1-17adf42ee38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 각 입력 레이어에 접근\n",
    "input_b1, input_s, input_b2, input_r, input_b3, input_c = inputs['b1'], inputs['s'], inputs['b2'], inputs['r'], inputs['b3'], inputs['c']\n",
    "\n",
    "## 각 변수별 데이터셋 추출\n",
    "data_b1, data_s, data_b2, data_r, data_b3, data_c = data_dict['b1'], data_dict['s'], data_dict['b2'], data_dict['r'], data_dict['b3'], data_dict['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0219b3b-8f85-4cd0-bca0-ce19fbc527c7",
   "metadata": {},
   "source": [
    "* Creating commonly using sub-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca303e5-5fd9-4051-b1f2-60968c6b5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subnetModel:\n",
    "    def __init__(self, input_layers):\n",
    "        self.input_layers = input_layers\n",
    "    \n",
    "    ## subnetwork(AE structure) for each variable group\n",
    "    def create_subnetworks(self, input_layer):\n",
    "        x = Dense(32, activation='relu')(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        # x = Dropout(0.3)(x)\n",
    "        x = Dense(8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_subnetworks(self):\n",
    "        return {name: self.create_subnetworks(layer) for name, layer in self.input_layers.items()}\n",
    "\n",
    "    ## combined network: subnetworks + fully-connected layers\n",
    "    def build_combined(self, subnetworks):\n",
    "        combined = Concatenate()(list(subnetworks.values()))\n",
    "        x = Dense(64, activation='relu')(combined)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        x = Dense(8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_final_output(self, combined_x, num_classes):\n",
    "        return Dense(num_classes, activation='softmax')(combined_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dea061-fd48-4380-955f-27fdbcc4d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시:\n",
    "input_layers = {\n",
    "    'b1': input_b1,\n",
    "    's': input_s,\n",
    "    'b2': input_b2,\n",
    "    'r': input_r,\n",
    "    'b3': input_b3,\n",
    "    'c': input_c\n",
    "}\n",
    "\n",
    "model_builder = subnetModel(input_layers)\n",
    "subnetworks = model_builder.build_subnetworks()\n",
    "combined_x = model_builder.build_combined(subnetworks)\n",
    "\n",
    "# 최종 출력 레이어 정의 (4-class 분류, args_multi.num_classes 사용)\n",
    "final_output = model_builder.build_final_output(combined_x, args_multi.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708f99d-3805-45eb-a0a1-1ce5ff361806",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd93910-0132-43b6-9d91-1d62416580e7",
   "metadata": {},
   "source": [
    "* Train-test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404b181-9eb5-47e5-9bef-6ed4f97b3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing training dataset and test dataset\n",
    "b1_train, b1_test, s_train, s_test, b2_train, b2_test, r_train, r_test, b3_train, b3_test, c_train, c_test, y_train, y_test = train_test_split(\n",
    "    data_b1, data_s, data_b2, data_r, data_b3, data_c, data_y_, test_size=0.2, random_state=710674)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4fabf-9aa0-49c5-bbbd-94ce6be46311",
   "metadata": {},
   "source": [
    "* Optimization function and learning rate scheduler settings, model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93018846-c282-4d97-96cc-dd965c0aa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = dynamic_learning_rate(args_multi.epochs, mode='cyclic')\n",
    "opt = keras.optimizers.SGD(learning_rate = args_multi.lr, decay = 1e-6, momentum = args_multi.momentum)\n",
    "\n",
    "model = Model(inputs=[input_b1, input_s, input_b2, input_r, input_b3, input_c], outputs=final_output)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c091a37-6cca-4e05-8c6d-8f4c1ce35366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([b1_train, s_train, b2_train, r_train, b3_train, c_train], y_train, \n",
    "          epochs=args_multi.epochs, batch_size=args_multi.bs, \n",
    "          validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee81e24-7aec-4f05-af84-4116dee53148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981cab1a-51e9-4094-a57e-4b056719afde",
   "metadata": {},
   "source": [
    "* Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a774cf0-562f-4ba3-a1eb-8c0d20d19cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, [b1_test, s_test, b2_test, r_test, b3_test, c_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd1831-ae0c-4038-b217-788fb383295a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b132d0d4-f53c-4b36-a37a-8fdd8213fdae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860653a-0fb4-4e00-a68f-d9dbd78bdc8c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9db2f-857c-4987-b499-181a10e7775a",
   "metadata": {},
   "source": [
    "## Using VAE/AE as a dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851c3fd-5bbb-4112-b86e-714312bb0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5ab25-a6c5-4f49-85df-8f53b1b7787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=200\n",
    "    enc_epochs = 50\n",
    "    bs=32\n",
    "    enc_bs = 16\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    encoding_dim = 16\n",
    "    num_classes= 2\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e194c-cc97-4dc6-b308-891b91b85917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3fb1ae9-612e-4890-92d3-1c031a05acb2",
   "metadata": {},
   "source": [
    "### Autoencoder for dim reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc9659-d907-4e33-a8bd-31967db5c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset layer\n",
    "input_layer = Input(shape=(x.shape[1],))\n",
    "\n",
    "# encoder layers\n",
    "encoder = Dense(64, activation='relu')(input_layer)\n",
    "encoder = Dense(32, activation='relu')(encoder)\n",
    "encoder_out = Dense(args.encoding_dim, activation='relu')(encoder)\n",
    "\n",
    "# decoder layers\n",
    "decoder = Dense(32, activation='relu')(encoder_out)\n",
    "decoder = Dense(64, activation='relu')(decoder)\n",
    "decoder_out = Dense(x.shape[1], activation='sigmoid')(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d75fa-94bf-4ab7-ae14-dfc0697e16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder_out)\n",
    "\n",
    "# Encoder model (convert input dataset into latent space)\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder_out)\n",
    "\n",
    "# Decoder model (recover latent space/vector into original dataset format)\n",
    "encoded_input = Input(shape=(args.encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-3](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_out = autoencoder.layers[-1](decoder_layer)\n",
    "decoder_model = Model(inputs=encoded_input, outputs=decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e4df0-f1fe-49d7-88ec-5d4743aa6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compile\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Checking each model\n",
    "autoencoder.summary()\n",
    "# encoder_model.summary()\n",
    "# decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6f004-27f3-4a19-ba75-d8735ae82c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Model training\n",
    "autoencoder.fit(x, x, epochs = args.enc_epochs, batch_size = args.enc_bs, shuffle=True, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d73949-5fdf-457e-b1f5-4d174ad54b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = encoder_model.predict(x)\n",
    "decoded_data = decoder_model.predict(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de485d-9b8c-4dbd-b4e3-4cecf71b62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df8ac2-ebe6-47a1-8759-afbe05c3d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f95697-d1d9-4861-a1ce-5b0d46132998",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63bc96-65f7-431d-a581-d9f8e51e9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb603d7-37a9-4af7-8ae9-9e3cc4613ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31eb85-686a-4948-a775-160528f4794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=200\n",
    "    bs=32\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# np.random.seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bbad8-7456-4f91-a531-c7c38e9a4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset, x_test, y_trainset, y_test = train_test_split(decoded_data, y, test_size = 0.1, random_state = 710674)\n",
    "x_train, x_vali, y_train, y_vali = train_test_split(x_trainset, y_trainset, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda29a3-3701-47b3-888c-adab1c263fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((x_train, x_vali), axis = 0)\n",
    "targets = np.concatenate((y_train, y_vali), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e9ade-fab5-48c0-861e-82dd1621e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "split_num = 5\n",
    "opt = keras.optimizers.SGD(learning_rate = args.lr, decay = 1e-6, momentum = args.momentum)\n",
    "kfold = KFold(n_splits = split_num, shuffle = True)\n",
    "# kfold = StratifiedKFold(n_splits = split_num, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d5391-c090-41d2-bf12-62d4c5b9808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## FOR FOUR-GROUP CLASSIFICATION ###############\n",
    "class_weight = {0:1, 1: 1.68}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5713989-8eab-4d96-afb3-f7ad9197c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_fold = []\n",
    "loss_per_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafbb424-a9f9-4318-9ab1-3a85bc98e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in kfold.split(inputs, targets):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim = x_train.shape[1], activation = 'relu'))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dropout(0.5)) #drop out\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(args.num_classes, activation = 'softmax'))\n",
    "    \n",
    "    ## model compile\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print(f'Training or fold {fold_num} ... ')\n",
    "    \n",
    "    ## fit data to model\n",
    "    history = model.fit(inputs[train], targets[train], batch_size = args.bs, epochs = args.epochs, verbose = 0, class_weight = class_weight)\n",
    "    \n",
    "    ## generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test])\n",
    "    print(f'Score for fold {fold_num}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    ## increasing fold number\n",
    "    fold_num = fold_num + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "## Summarizing the results\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'>> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'>>> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'>>> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7d67c-2c9d-4320-84b8-9f451d9b4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "y_predict = np.argmax(y_predict, axis = 1)\n",
    "y_test = np.argmax(y_test, axis = 1)\n",
    "\n",
    "result = confusion_matrix(y_test, y_predict, normalize = 'pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca930a-2f73-4fc8-a7c8-d908c8b8b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(result, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962116a-e761-4059-adbf-e613c02a6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "precision = metrics.precision_score(y_test, y_predict, average = 'macro')\n",
    "recall = metrics.recall_score(y_test, y_predict, average = 'micro')\n",
    "f1 = metrics.f1_score(y_test, y_predict, average = 'weighted')\n",
    "auc = roc_auc_score(y_test, model.predict(x_test, verbose=0), multi_class='ovr')\n",
    "\n",
    "print(\"=============================================\")\n",
    "print(\"The overall accuracy is:\", round(accuracy, 4))\n",
    "print(\"The precision score is:\", round(precision, 4))\n",
    "print(\"The recall score is:\", round(recall, 4))\n",
    "print(\"The f1 score is:\", round(f1, 4))\n",
    "print(\"The AUC score is:\", round(auc, 4))\n",
    "print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e63ff-ce5a-4d03-94ad-e6c7fb4ded06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aeef4b-07fc-409b-a6e0-d37ee522f463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edc141-29ab-4439-9a08-f1fb51cf4a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "602e460b-655c-42a7-a42d-11f6e9d0425b",
   "metadata": {},
   "source": [
    "## EV input & MV output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7db1f0-6786-4f26-a693-02fa2067e79c",
   "metadata": {},
   "source": [
    "> What if we use external variables(ex. demographics) as input variables and set the DNN model to find Medical Variables as an output variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881bf1a-9503-4099-84cc-07f498c298b9",
   "metadata": {},
   "source": [
    "### data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf12b17-199f-4269-bf53-9a513cfa7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evinput = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77471e0f-d6c4-4da3-931f-38c50212af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evinput['disorder'] = data_evinput['disorder'].replace({\"mdd\": 0})\n",
    "data_evinput['disorder'] = data_evinput['disorder'].replace({\"pd\": 1})\n",
    "data_evinput['disorder'] = data_evinput['disorder'].replace({\"con\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0605cd-07ce-481a-9e46-3a3e05c04210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_evinput.head()\n",
    "# data_evinput.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3bbce-f059-4138-9696-f474598a3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list = ['age', 'gender', 'disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c88bcb-8b08-471e-bbba-0c657b2d3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_input = data_evinput.loc[:, ('disorder', 'age', 'gender')]\n",
    "ev_output = data_evinput.drop((scale_list + etc_list + demo_list), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f8eba-34d6-4295-84d6-1aebffd7db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)\n",
    "\n",
    "ev_input[:] = (scaler.fit_transform(ev_input[:])).round(decimals=6)\n",
    "ev_output[:] = (scaler.fit_transform(ev_output[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1b374-cb9d-45c0-8c38-7ca63d19cb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1262e1a5-4d6c-4164-a776-8424effcfbad",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20630384-6f8a-4c46-a35b-fc4be1a592d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = ev_input.shape[1]\n",
    "output_dim = ev_output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0462676-74d5-4870-ac0b-45fe11aca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 정의\n",
    "ev_model = Sequential([\n",
    "    Dense(16, input_dim=input_dim, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'), \n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(output_dim)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3b00b-9c03-434e-bfd7-b91755d06939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "ev_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 모델 요약 출력\n",
    "ev_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e2ee0-dce6-4392-9924-7ebd16348525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습 예시 (X_train은 인구학적 변수, Y_train은 생체신호 변수)\n",
    "ev_model.fit(ev_input, ev_output, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bd49b-ce63-457c-928e-75d78badf076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2601d3-7482-4454-870a-55bc404cd1fc",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad863e-95a0-4be2-8595-e5c86ab9654b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466281fa-f244-4cb0-9639-c7c0a27fa4ff",
   "metadata": {},
   "source": [
    "# Class generic/specific feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73be785-5af4-4a10-8fd4-df5cf027f7c1",
   "metadata": {},
   "source": [
    "> Concept: generate seperate class(var) generic feature and class specific feature extraction models. \\\n",
    "> Maybe implement individual autoencoder model and train the subset datasets. \\\n",
    "> Then concatenate the extracted features, and adopt the feature vector for classifier's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7fabe-ea26-4124-a31e-db338881556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed5402-ad24-49db-adef-53c06b0a48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270acd80-6de5-4a42-aeb3-550ab63b3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hrv = ['sub', 'VISIT', 'age', 'gender', 'HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI', 'disorder_mdd']\n",
    "target = ['disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2ee7f-5f8c-457b-a048-117a9a050750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b3962-6b53-4b3f-a755-b11e571f106e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f51f7-7012-4b88-af6c-8d4d8d2e94ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7daf7-d601-41ac-8c3b-9dc0d780b7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbce32-6c1d-42d1-865e-9997fac5b058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cb706-86e5-4af9-a5a5-031c83bad917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786723db-9c5a-4bc3-971e-43ef7ecc4833",
   "metadata": {},
   "source": [
    "## Class-generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fab1a-f0a5-48f4-8b4c-d45693e1a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generic = data.drop(non_hrv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5de72a-499c-450b-af9f-ecebfc8c130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_generic.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6355b8-977a-4b4d-995a-94ca48d15a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823072b8-871a-405d-85cf-8ccaf3d31160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb063c-fe44-456f-8eea-8c61b42a7b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40edd2-482a-4e23-b184-0e7db8cfc787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b688f2-cb01-4b00-8999-04c4cbeb6a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ecbf690-f09b-4125-b27f-76b1fa408da8",
   "metadata": {},
   "source": [
    "## Class-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e3722-c5d9-4d6c-b44a-51b7ced0e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spe_M = data_ori[data_ori['gender']==1]\n",
    "data_spe_F = data_ori[data_ori['gender']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727b8f5-2248-4504-977d-113660e10bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_specific = data_spe_M.drop(non_hrv, axis=1)\n",
    "# data_specific = data_spe_F.drop(non_hrv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b4a87-54ac-47fe-80f7-36abb9ddb8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_specific.drop('disorder', axis=1)\n",
    "y = data_specific.disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd4b55-2c6c-4805-8ba0-6c82f654fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'mdd': 0})\n",
    "label = label.replace({'pd': 1})\n",
    "label = label.replace({'con' : 2})\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649c705-c75d-4dba-8ec0-af97b247ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492606f3-e9dd-4bbd-9d7b-200553ec8e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef602e1-7783-4457-8189-c4d5e3387a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be793bd-87ff-45e5-8236-2e5cb5d78571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class specific_AE:\n",
    "    def __init__(self, input_dim, encoding_dim, hidden_layers=None, batch_norm=False, dropout_rate=0.0):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.hidden_layers = hidden_layers if hidden_layers is not None else []\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        ## building encoder part\n",
    "        input_layer = layers.Input(shape=(self.input_dim,))\n",
    "        x = input_layer\n",
    "\n",
    "        for units in self.hidden_layers:\n",
    "            x = layers.Dense(units, activation='relu')(x)\n",
    "            if self.batch_norm:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "            if self.dropout_rate > 0.0:\n",
    "                x = layers.Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        encoded = layers.Dense(self.encoding_dim, activation='relu')(x)\n",
    "\n",
    "        ## building decodeer part\n",
    "        x = encoded\n",
    "        for units in reversed(self.hidden_layers):\n",
    "            x = layers.Dense(units, activation='relu')(x)\n",
    "            if self.batch_norm:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "            if self.dropout_rate > 0.0:\n",
    "                x = layers.Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        decoded = layers.Dense(self.input_dim, activation='softmax')(x)\n",
    "\n",
    "        # 인코더, 디코더, 오토인코더 모델\n",
    "        self.encoder = models.Model(input_layer, encoded, name='encoder')\n",
    "        self.decoder = models.Model(encoded, decoded, name='decoder')\n",
    "        self.autoencoder = models.Model(input_layer, self.decoder(self.encoder(input_layer)), name='autoencoder')\n",
    "\n",
    "        self.autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    def summary(self):\n",
    "        self.autoencoder.summary()\n",
    "\n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=256):\n",
    "        return self.autoencoder.fit(X_train, X_train, \n",
    "                                    validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "                                    epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "\n",
    "    def decode(self, encoded_data):\n",
    "        return self.decoder.predict(encoded_data)\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        return self.autoencoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf402a0-aa64-4764-88f6-5644727d0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45065102-09a4-41e4-b764-a427df6b48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "input_dim = data_x.shape[1]\n",
    "encoding_dim = 8\n",
    "hidden_layers = [64, 32, 16]\n",
    "\n",
    "autoencoder = specific_AE(input_dim, encoding_dim, hidden_layers, batch_norm=True, dropout_rate=0.2)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49415e-66f9-4e52-a42c-5758150fde16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.train(data_x, data_x, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2469d-55b2-4413-903f-7f9aa71466a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15aac4-d6e6-4a28-968e-4527e92b45ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1033019-48d2-4259-ade2-566e6d9a6e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444b8d4-ae07-403a-b85b-cf694d8be151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a630f-a949-46c8-841b-fcf46ca83e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efe88d9d-795f-4918-b98e-e8d3bbf3e535",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611ccce-6c6a-4265-a2ea-a1a4679ef50b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375ade6-f619-495f-971d-f0beded5e021",
   "metadata": {},
   "source": [
    "# Code practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28f88b-b33a-4352-96dd-c87e18fc3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_synthetic_data_distribution(original_data, synthetic_data):\n",
    "    # 원래 데이터의 평균과 분산을 계산\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_data)\n",
    "    \n",
    "    # 원래 데이터의 평균과 분산\n",
    "    original_mean = scaler.mean_\n",
    "    original_std = scaler.scale_\n",
    "    \n",
    "    # synthetic data의 평균과 분산을 계산\n",
    "    synthetic_scaler = StandardScaler()\n",
    "    synthetic_scaler.fit(synthetic_data)\n",
    "    \n",
    "    # synthetic data를 원래 데이터의 평균과 분산으로 조정\n",
    "    adjusted_synthetic_data = synthetic_scaler.transform(synthetic_data)\n",
    "    adjusted_synthetic_data = adjusted_synthetic_data * original_std + original_mean\n",
    "    \n",
    "    return adjusted_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d712e2f-03f8-45c7-9ac9-4478eef09479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df = (scaler.fit_transform(df)).round(decimals=6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5c842-2106-4d09-927a-e93ba95e98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x0 = data_scaler(data_x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82fb32-891f-4d97-af13-1278acfd3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x0 = data_MDD.drop(drop_list, axis=1)\n",
    "# data_x0[:] = (scaler.fit_transform(data_x0[:])).round(decimals=6) ## scaling x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d97db-d36f-4dec-beb2-f081ec3a6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_synthetic_data = adjust_synthetic_data_distribution(data_x0, synthetic_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a4026-24dd-4f07-8b7b-eaae059386b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03998662-f384-41ed-92e0-5856912966da",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b18293-bb6b-40ea-9840-47e8475c8f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f8fb6-1ef3-4ca7-bc48-f925663803a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a3a88-96ac-4503-834c-e53e284d7d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6516bd-0e74-4c32-a191-00939906d8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6d04d-4833-4f42-8913-3f8ab6beb17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37862d9-9178-4df1-a5ce-e07bf59e9394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dad44-e012-4172-8ba4-46d768057065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f59d64-f92b-4271-b93a-88a2c3c5b75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361b994-2fa3-4015-bb82-93c302d3fb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e416bc-f6e3-4501-bc04-12836c08826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d9e81-27d4-4805-a767-d87d82a20dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
