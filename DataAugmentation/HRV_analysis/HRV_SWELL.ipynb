{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a20b02d-6ef3-4a6e-a45e-69cddd886137",
   "metadata": {},
   "source": [
    "# Data Augmentation- VAE, Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6a98c-2f59-452a-b8b1-ee762e22acf9",
   "metadata": {},
   "source": [
    "> This script adopts HRV dataset for 3-class mental state classification. \\\n",
    "> SWELL public HRV dataset. \\\n",
    "> No stress, time pressure, interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddbef65-77a1-43fe-989a-0a0c7d4ead5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import shap ## for XAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4882e3a3-161d-4d3e-bef3-b83010ca2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from keras.layers import Dense , Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam ,RMSprop\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, ParameterGrid\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, confusion_matrix, roc_auc_score, classification_report, precision_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Lambda, Conv1D, Flatten, Reshape, UpSampling1D, MaxPooling1D, concatenate, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.losses import mse, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf01643-b981-4d57-ae21-6822cf681a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173afa80-706e-4320-b3c5-cabc098f7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)\n",
    "# scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfaf0e-1296-43a9-a547-8dba9168a6ba",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baaf49-19f6-4559-8906-1d8fb2d49ae7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf5bb1-4eca-4da1-b06e-5c62e8549841",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503f2855-10c1-447d-9f0b-cf0bfad482ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_csv('E:/RESEARCH/Datasets/bio_data/hrv_eda/2. final/datasets/hrv/swell/combined/classification/swell_hrv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b7cb3a-dd1e-43a6-9f71-e3d97900f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the original SWELL HRV dataset is: (391638, 35)\n",
      "Index(['MEAN_RR', 'MEDIAN_RR', 'SDRR', 'RMSSD', 'SDSD', 'SDRR_RMSSD', 'HR',\n",
      "       'pNN25', 'pNN50', 'SD1', 'SD2', 'KURT', 'SKEW', 'MEAN_REL_RR',\n",
      "       'MEDIAN_REL_RR', 'SDRR_REL_RR', 'RMSSD_REL_RR', 'SDSD_REL_RR',\n",
      "       'SDRR_RMSSD_REL_RR', 'KURT_REL_RR', 'SKEW_REL_RR', 'VLF', 'VLF_PCT',\n",
      "       'LF', 'LF_PCT', 'LF_NU', 'HF', 'HF_PCT', 'HF_NU', 'TP', 'LF_HF',\n",
      "       'HF_LF', 'sampen', 'higuci', 'condition'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### data shape, variables check\n",
    "print(\"The shape of the original SWELL HRV dataset is:\", data_ori.shape)\n",
    "print(data_ori.columns)\n",
    "# data_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0edc4b1-37db-4c95-ad24-e143b4094011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows that contains at least one missing value: 0\n"
     ]
    }
   ],
   "source": [
    "# 결측값이 하나라도 있는 행의 개수 확인\n",
    "num_missing_rows = data_ori.isna().any(axis=1).sum()\n",
    "print(f\"The number of rows that contains at least one missing value: {num_missing_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e995314-ce75-4c43-864a-414b415d01ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd1236d0-b3d2-4d80-80d6-b2c4273209d4",
   "metadata": {},
   "source": [
    "> Target label is \"condition\" variable. \\\n",
    "> no stress, time pressure, interruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0fea5-8c24-4a93-a507-1e9acbfcc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori[\"condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ccd03-2d74-4229-a17b-9078245c07e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bc236a9-553b-4ac8-8bbd-b68b378d0639",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e28f5-39e3-441b-95f3-827b08f9fbd9",
   "metadata": {},
   "source": [
    "### Reducing dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd208f-2c61-4929-aeed-4d2bfe09e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly select n samples from original dataset (without any condition)\n",
    "df_reduced = data_ori.sample(n=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ed97d-31be-4e76-8971-7f0518793038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = data_ori.groupby('condition').apply(lambda x: x.sample(500)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb190424-51eb-429b-8611-07a52b0b5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8e9e8-341f-44be-832d-3034d5fc337f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f99886c-d984-4989-a955-acfd917dbd27",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7f908-d2a5-4939-b109-2f2f784b0bce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Distribution check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37422c91-54a5-48d7-a3b4-7b9403b7f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vis = data_ori.copy()\n",
    "data_vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de08029-7af1-485c-af10-7ed88ba7eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_ = ['sub', 'VISIT', 'age', 'gender', 'disorder_mdd']\n",
    "# info_ = ['sub', 'VISIT', 'age', 'gender', 'disorder', 'disorder_mdd']\n",
    "scale_ = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "drop_ = info_ + scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49d31d-d3ea-49b5-841b-ab40fdbc47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = data_vis.drop(drop_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8cb63-fac8-4b72-9c4d-884142dce35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2ecae-f6ae-44eb-bbba-490845d1c19e",
   "metadata": {},
   "source": [
    "#### KDE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc513b78-1111-4303-a85b-981be282841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in list(x.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93b026-aed9-4b00-9038-f2f9c0672650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdeplot_features(data, feature, title):\n",
    "    \n",
    "    df0 = data[data['disorder']==\"mdd\"]\n",
    "    df1 = data[data['disorder']==\"pd\"]\n",
    "    df2 = data[data['disorder']==\"con\"]\n",
    "\n",
    "    df0 = df0.drop(['disorder'], axis=1)\n",
    "    df1 = df1.drop(['disorder'], axis=1)\n",
    "    df2 = df2.drop(['disorder'], axis=1)\n",
    "    \n",
    "    df0_values = df0[feature].to_numpy()\n",
    "    df1_values = df1[feature].to_numpy()\n",
    "    df2_values = df2[feature].to_numpy()\n",
    "     \n",
    "    plt.figure(figsize = (6, 2.5))\n",
    "    \n",
    "    sns.kdeplot(df0_values, color = 'y')\n",
    "    sns.kdeplot(df1_values, color = 'b')\n",
    "    sns.kdeplot(df2_values, color = 'g')\n",
    "    \n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    \n",
    "    # del values_train , values_test\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f370a-e7e2-43d8-a852-9d2eecbbc57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in features:\n",
    "    kdeplot_features(x_, feature=var, title = var + \"distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc4e77-3523-4066-986d-bb6f9b24ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a965d6-b485-4eb2-9bab-eb817409832e",
   "metadata": {},
   "source": [
    "#### JSD calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06c5f9-2a6d-4e6c-be28-8e10c7186e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list to append distributional differences\n",
    "variable = []\n",
    "diff01 = []\n",
    "diff02 = []\n",
    "diff03 = []\n",
    "diff12 = []\n",
    "diff13 = []\n",
    "diff23 = []\n",
    "\n",
    "for feature in features:\n",
    "    ## deleting any NA values in each feature column, for distance calculation\n",
    "    data = data_vis.dropna(subset=[feature])\n",
    "\n",
    "    ## split the dataset into each classes\n",
    "    df0 = data[data['target_info']==\"control\"]\n",
    "    df1 = data[data['target_info']==\"mdd\"]\n",
    "    df2 = data[data['target_info']==\"bpi\"]\n",
    "    df3 = data[data['target_info']==\"bpii\"]\n",
    "\n",
    "    df0 = df0.drop(['target_info'], axis=1)\n",
    "    df1 = df1.drop(['target_info'], axis=1)\n",
    "    df2 = df2.drop(['target_info'], axis=1)\n",
    "    df3 = df3.drop(['target_info'], axis=1)\n",
    "    \n",
    "    df0_values = df0[feature]\n",
    "    df1_values = df1[feature]\n",
    "    df2_values = df2[feature]\n",
    "    df3_values = df3[feature]\n",
    "\n",
    "    ## sampling based on the minimum size of the target info.\n",
    "    sample_size = (min(df0_values.shape[0], df1_values.shape[0], df2_values.shape[0], df3_values.shape[0]))\n",
    "    df0_sample = df0_values.sample(n=(sample_size))\n",
    "    df1_sample = df1_values.sample(n=(sample_size))\n",
    "    df2_sample = df2_values.sample(n=(sample_size))\n",
    "    df3_sample = df3_values.sample(n=(sample_size))\n",
    "\n",
    "    ## jensen-shannon divergence calculation and append\n",
    "    variable.append(feature)\n",
    "    diff01.append(jensenshannon(df0_sample, df1_sample))\n",
    "    diff02.append(jensenshannon(df0_sample, df2_sample))\n",
    "    diff03.append(jensenshannon(df0_sample, df3_sample))\n",
    "    diff12.append(jensenshannon(df1_sample, df2_sample))\n",
    "    diff13.append(jensenshannon(df1_sample, df3_sample))\n",
    "    diff23.append(jensenshannon(df2_sample, df3_sample)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76991d9-f00c-4eff-be6e-239c519eaca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990b52e-f0f1-4e37-8e78-52ad9acdc13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a790933-91fd-4c5e-81b8-7fbf723533cf",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cd5d4-ff8e-4c15-b42f-276833e80bec",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a5c67-d878-405a-a9f8-82423bbc0f86",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72688524-49ea-430b-b106-45a383ae0483",
   "metadata": {},
   "source": [
    "## dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997cdb8-24cf-4e5c-97af-15df171e525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e8855-bfb5-4c04-909e-f9863f59deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d7157-98be-41dd-86a7-f5608e50c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_INT = data[data['condition']==\"interruption\"]\n",
    "data_NOS = data[data['condition']==\"no stress\"]\n",
    "data_TIP = data[data['condition']==\"time pressure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b20739-123e-4919-b470-8927a558f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = data.copy()\n",
    "# data_vae = data_INT.copy()\n",
    "# data_vae = data_NOS.copy()\n",
    "# data_vae = data_TIP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387281-62c2-4293-9bba-99636db90593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22dbbc0-3dd7-4fd0-9041-e53865ff384f",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7aefa-d3f3-4d5f-bb6b-5263c40e8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_vae:\n",
    "    # arugments\n",
    "    epochs=150\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 2\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_vae = Args_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e77906-6db1-4e08-a0cf-9f745d2c62d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a9fc98d-616a-4be3-bbce-8e19f3307c9c",
   "metadata": {},
   "source": [
    "### preparing x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f7cda-0dde-42aa-bb01-7ab9cad6114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfc6e8-e64f-4aad-a0e9-b42f3f05a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['condition', 'higuci', 'sampen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a10a29-e8fb-4768-93fe-6168376f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_vae.drop((drop_list), axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_vae.condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ad571-cd55-43dd-88a8-1520cfd717bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'interruption':0})\n",
    "label = label.replace({'no stress': 1})\n",
    "label = label.replace({'time pressure': 2})\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce530e4-07f7-49d5-be98-a0692741743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8254ba3-98e7-446b-b9cb-17c28c07934f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0f958b-c75f-46cf-ba90-aabe6e3b242a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d6516-5f26-4afd-a1ca-21efe1021660",
   "metadata": {},
   "source": [
    "### vae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd6248-3ced-4948-a427-8149c8e180d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self.build_model()\n",
    "\n",
    "    ## VAE model for data augmentation\n",
    "    def build_model(self):\n",
    "        ####################################################\n",
    "        ## defining encoder section\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # shallow model use\n",
    "        # h = Dense(16, activation='relu')(inputs)\n",
    "\n",
    "        # # deeper model use\n",
    "        h = Dense(32, activation='relu')(inputs)\n",
    "        h = Dense(16, activation='relu')(h)\n",
    "        # h = Dense(32, activation='relu')(h)\n",
    "\n",
    "        # calculating mean/var\n",
    "        z_mean = Dense(self.latent_dim)(h)\n",
    "        z_log_var = Dense(self.latent_dim)(h)\n",
    "\n",
    "        ## sampling section (for gaussian distribution)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        ####################################################\n",
    "        ## defining decoder section\n",
    "\n",
    "        # shallower model use\n",
    "        # decoder_h = Dense(16, activation='relu')\n",
    "        # decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "        # h_decoded = decoder_h(z)\n",
    "        # x_decoded_mean = decoder_mean(h_decoded)\n",
    "        \n",
    "        # deeper model use\n",
    "        decoder_h1 = Dense(16, activation='relu')\n",
    "        decoder_h2 = Dense(32, activation='relu')\n",
    "        # decoder_h3 = Dense(32, activation='relu')\n",
    "        decoder_mean = Dense(self.input_dim, activation='sigmoid')\n",
    "\n",
    "        h_decoded = decoder_h1(z)\n",
    "        h_decoded = decoder_h2(h_decoded)\n",
    "        # h_decoded = decoder_h3(h_decoded)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining VAE model\n",
    "        self.vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "        ####################################################\n",
    "        ## defining loss function (reconstruction + KL divergence)\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var + K.epsilon()), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.vae.add_loss(vae_loss)\n",
    "\n",
    "        ####################################################\n",
    "        ## compiling model\n",
    "        self.vae.compile(optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "        ####################################################\n",
    "        ## Extracting encoder and decoder separately\n",
    "        ## encoder section\n",
    "        self.encoder = Model(inputs, z_mean)\n",
    "\n",
    "        # ## decoder section\n",
    "        # decoder_input = Input(shape=(self.latent_dim,))\n",
    "        # _h_decoded = decoder_h(decoder_input)\n",
    "        # _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        # self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "        \n",
    "        #### deeper model\n",
    "        decoder_input = Input(shape=(self.latent_dim,))\n",
    "        _h_decoded = decoder_h1(decoder_input)\n",
    "        _h_decoded = decoder_h2(_h_decoded)\n",
    "        # _h_decoded = decoder_h3(_h_decoded)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    ####################################################\n",
    "    ####################################################\n",
    "    ## Model training\n",
    "    def train(self, data, epochs, batch_size, validation_split):\n",
    "        self.vae.fit(data, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def encode(self, data):\n",
    "        return self.encoder.predict(data)\n",
    "\n",
    "    def decode(self, latent_points):\n",
    "        return self.decoder.predict(latent_points)\n",
    "\n",
    "    def generate_synthetic_data(self, num_samples=1):\n",
    "        latent_samples = np.random.normal(size=(num_samples, self.latent_dim)) ## sampling from latent space\n",
    "        return self.decode(latent_samples) ## synthetic data generation with decoder section\n",
    "\n",
    "    def visualize_latent_space(self, data, labels):\n",
    "        ## encode the dataset into latent space\n",
    "        encoded_data = self.encode(data)\n",
    "\n",
    "        ## latent space visualization with t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=710674)\n",
    "        encoded_data_tsne = tsne.fit_transform(encoded_data)\n",
    "\n",
    "        ## Visualize\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(encoded_data_tsne[:, 0], encoded_data_tsne[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"t-SNE component 1\")\n",
    "        plt.ylabel(\"t-SNE component 2\")\n",
    "        plt.title(\"t-SNE visualization of the latent space\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519795-e972-46dd-aa18-6485cd044e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = VAE(input_dim=data_x.shape[1], latent_dim=args_vae.latent_dim)\n",
    "vae.train(data_x, epochs = args_vae.epochs, batch_size = args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8c330-9e7d-4339-be91-ca88cde8972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d41ae535-2ade-4963-a709-d96f43475e01",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e235a2-97e6-477d-a78e-2d0ab44bd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.visualize_latent_space(data_x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4121a-9bdd-4b39-b752-ad3afe3b8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8de385-71df-496c-ae62-f99725b6f39e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782e6f7-f737-4dc9-9265-df31bd7a7e15",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc58508-4b08-43b7-8a09-b7576b8e65da",
   "metadata": {},
   "source": [
    "> We use two different synthetic data generation functions.\n",
    "> 1. Adopting single VAE model(trained on all data classes)\n",
    "> 2. Adopting individual VAE model for each data classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60dfc3-5254-40e6-b71a-a0a06bd6f34d",
   "metadata": {},
   "source": [
    "#### single VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e7f06-e243-419a-a18d-843058e1aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ori['condition'].value_counts()\n",
    "df_reduced.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ddb5b-733e-456b-bdde-ba86a3d3ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.condition.value_counts()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cc22a-1632-4adb-be4c-4a8b7e36fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_for_classes(vae_model, latent_dim, class_labels, num_samples_per_class):\n",
    "    synthetic_data = {}\n",
    "    for class_label in class_labels:\n",
    "        num_samples = num_samples_per_class[class_label]\n",
    "        ## sampling from latent space\n",
    "        z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "        ## use sampled data to generate synthetic dataset\n",
    "        generated_samples = vae_model.decoder.predict(z_samples)\n",
    "        ## save synthetic dataset with class label\n",
    "        synthetic_data[class_label] = generated_samples\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7835c7-80b2-47c3-949a-5a81f6a1b999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 생성할 클래스 라벨들과 각 클래스별 생성할 샘플 수\n",
    "# num_samples_per_class = {0:250, 1:250, 2: 250} ##Set to total 300 for each class\n",
    "num_samples_per_class = {0: (850-df_reduced.condition.value_counts()[1]), 1: (850-df_reduced.condition.value_counts()[0]), 2: (df_reduced.condition.value_counts()[2])}\n",
    "\n",
    "# 각 클래스별 synthetic data 생성\n",
    "synthetic_data = generate_synthetic_data_for_classes(vae, args_vae.latent_dim, label, num_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e251b81-d4f1-4c79-97fc-9c95ab0e9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스에 대한 데이터프레임화\n",
    "gen_int = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_nos = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "gen_tip = pd.DataFrame(synthetic_data[2], columns=data_x.columns)\n",
    "print(f\"The synthetic data size is... \\n\\nSynthetic for  interruption class: {gen_int.shape[0]} \\nSynthetic for no stress class: {gen_nos.shape[0]} \\nSynthetic for time pressure class: {gen_tip.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673df51-45c8-4526-b955-eed2cac89c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1688b2b-cc7c-4ee3-bd7b-d25e917fe6ad",
   "metadata": {},
   "source": [
    "> move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded5443-440f-4ab1-9263-a7243f37b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2dbfadc-32b6-4641-99a5-38cf2e3ca0e0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212692c-5901-4188-aaac-f730acb897b9",
   "metadata": {},
   "source": [
    "#### individual VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce9692-9794-48c4-9dfe-87bcb7f9d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_vae(vae_model, latent_dim, num_samples):\n",
    "    ## sampling from latent space\n",
    "    z_samples = np.random.normal(size=(num_samples, latent_dim))\n",
    "    ## use sampled latent space vector to generate synthetic dataset\n",
    "    synthetic_data = vae_model.decoder.predict(z_samples)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e516a0-0667-4cde-9b53-b19577cdfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int, data_nos, data_tip = (data_INT.drop(drop_list, axis=1), data_NOS.drop(drop_list, axis=1), data_TIP.drop(drop_list, axis=1))\n",
    "\n",
    "data_int[:] = (scaler.fit_transform(data_int[:])).round(decimals=6)\n",
    "data_nos[:] = (scaler.fit_transform(data_nos[:])).round(decimals=6)\n",
    "data_tip[:] = (scaler.fit_transform(data_tip[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce85933-2807-438b-9832-d8b0937e8df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# interruption 클래스에 대한 VAE 모델 학습\n",
    "vae_int = VAE(data_int.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_int.train(data_int, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)\n",
    "\n",
    "# no stress 클래스에 대한 VAE 모델 학습\n",
    "vae_nos = VAE(data_nos.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_nos.train(data_nos, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)\n",
    "\n",
    "# time pressure 클래스에 대한 VAE 모델 학습\n",
    "vae_tip = VAE(data_tip.shape[1], latent_dim = args_vae.latent_dim)\n",
    "vae_tip.train(data_tip, epochs=args_vae.epochs, batch_size=args_vae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9a269-5a81-4140-91c7-a82bfea06018",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = {0:250, 1:250, 2: 250}\n",
    "\n",
    "num_samples_int = num_samples_per_class[0]\n",
    "synthetic_data_int = generate_class_synthetic_data_vae(vae_int, args_vae.latent_dim, num_samples_int)\n",
    "\n",
    "num_samples_nos = num_samples_per_class[1]\n",
    "synthetic_data_nos = generate_class_synthetic_data_vae(vae_nos, args_vae.latent_dim, num_samples_nos)\n",
    "\n",
    "num_samples_tip = num_samples_per_class[2]\n",
    "synthetic_data_tip = generate_class_synthetic_data_vae(vae_tip, args_vae.latent_dim, num_samples_tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abe0d1-b678-4307-b31d-d781681a3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_int = pd.DataFrame(synthetic_data_int, columns=data_x.columns)\n",
    "gen_nos = pd.DataFrame(synthetic_data_nos, columns=data_x.columns)\n",
    "gen_tip = pd.DataFrame(synthetic_data_tip, columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277f2a8-02ad-45f0-bf64-c9e2d5e0f2f2",
   "metadata": {},
   "source": [
    "> now move to \"Classification performance check - Augmented datasets add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86b8f5-2d16-492c-8d71-610d4dd831d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9743ac3-520e-4654-ac30-3534ceff5881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95411c46-079c-4c9c-ab27-ff0efab3ebb8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995646d-680f-4e9b-82de-d2a5c100b308",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e2c7a-98b9-4dd5-bc53-cd541a0abf17",
   "metadata": {},
   "source": [
    "## Conditional VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9b771-ad71-492c-bdc7-ec62558c9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_cvae:\n",
    "    # arugments\n",
    "    epochs=80\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    latent_dim = 2\n",
    "    seed=710674\n",
    "\n",
    "args_cvae = Args_cvae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358116f-32bc-47c1-8a9b-e175f214d8e3",
   "metadata": {},
   "source": [
    "> preparing x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7196dd-d531-4896-b879-0c922ff17877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cvae = data_ori.copy()\n",
    "data_cvae = df_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84290ae5-206d-417c-b293-649ea0977214",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cvae.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be319a-d23a-40b1-862f-8d58a64c9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cvae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b83362-5a42-434b-b671-92b322f99b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## relabeling data_cvae's disorder variable to use them as an conditional input\n",
    "data_cvae['condition'] = data_cvae['condition'].replace({'interruption': 0})\n",
    "data_cvae['condition'] = data_cvae['condition'].replace({'no stress' : 1})\n",
    "data_cvae['condition'] = data_cvae['condition'].replace({'time pressure': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb0103-1f46-46ec-86dd-1a78c5cbbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['condition', 'higuci', 'sampen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ef456-4692-4992-8cf6-900f097323e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## x: input, y: output, c: conditional inputs\n",
    "x = data_cvae.drop(drop_list, axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_cvae.condition\n",
    "# c = data_cvae.loc[:, ('condition')]\n",
    "# c = data_cvae.iloc[:, -1]\n",
    "c = data_cvae[['condition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac578c-a68a-4815-bbd0-d37c1557c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6)\n",
    "###################\n",
    "label = y.copy()\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding\n",
    "###################\n",
    "data_c = c.copy()\n",
    "# data_c = to_categorical(data_c, 3)\n",
    "# data_c[:] = (scaler.fit_transform(data_c[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b262b-66df-4686-87cf-2422fb53b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)\n",
    "print(\"The size of c dataset is:\", data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4400c-554b-4fb8-bff3-3709ae1b0ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4584f46-9dbc-41bb-9b51-4e8dbcae0010",
   "metadata": {},
   "source": [
    "> CVAE model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5dfbd-6c87-42e2-9d55-f7ab6b71b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE:\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        ## initializing model\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        ## generating encoder and decoder section\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.cvae = self.build_cvae()\n",
    "    \n",
    "    ## defining encoder function\n",
    "    def build_encoder(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        ## defining input layer: put x and condition c together\n",
    "        combined_input = layers.Concatenate()([x_input, c_input])\n",
    "        \n",
    "        # encoder layers defining\n",
    "        h = layers.Dense(32, activation=None)(combined_input)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)\n",
    "        \n",
    "        h = layers.Dense(16, activation=None)(h)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)\n",
    "        \n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(h)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(h)\n",
    "        \n",
    "        ## sampling (reparameterization)\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        z = layers.Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        return Model([x_input, c_input], [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    ## defining decoder function\n",
    "    def build_decoder(self):\n",
    "        z_input = layers.Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        ## decoder input defining: put latent space input and conditional c together\n",
    "        combined_input = layers.Concatenate()([z_input, c_input])\n",
    "\n",
    "        # decoder layers defining\n",
    "        h = layers.Dense(16, activation=None)(combined_input)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)  # Dropout with 30% rate\n",
    "        \n",
    "        h = layers.Dense(32, activation=None)(h)\n",
    "        h = layers.BatchNormalization()(h)\n",
    "        h = layers.Activation('relu')(h)\n",
    "        h = layers.Dropout(0.3)(h)  # Dropout with 30% rate\n",
    "        \n",
    "        x_decoded = layers.Dense(self.input_dim, activation='sigmoid', name='output')(h)\n",
    "        \n",
    "        return Model([z_input, c_input], x_decoded, name='decoder')\n",
    "    \n",
    "    ## build and compile the CVAE model\n",
    "    def build_cvae(self):\n",
    "        x_input = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        c_input = layers.Input(shape=(self.condition_dim,), name='condition')\n",
    "        \n",
    "        z_mean, z_log_var, z = self.encoder([x_input, c_input])\n",
    "        x_decoded = self.decoder([z, c_input])\n",
    "        \n",
    "        cvae = Model([x_input, c_input], x_decoded, name='cvae')\n",
    "        \n",
    "        reconstruction_loss = mse(x_input, x_decoded)\n",
    "        reconstruction_loss *= self.input_dim\n",
    "        \n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        cvae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        cvae.add_loss(cvae_loss)\n",
    "        cvae.compile(optimizer='adam')\n",
    "        \n",
    "        return cvae\n",
    "    \n",
    "    def encode(self, data, condition):\n",
    "        z_mean, z_log_var, z = self.encoder.predict([data, condition])\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "    def decode(self, latent, condition):\n",
    "        return self.decoder.predict([latent, condition])\n",
    "    \n",
    "    ## model training\n",
    "    def train(self, x_train, c_train, epochs, batch_size, validation_split=None):\n",
    "        self.cvae.fit([x_train, c_train], epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "    \n",
    "    ## synthetic data generation (for data augmentation)\n",
    "    def generate_synthetic_data(self, condition, n_samples):\n",
    "        z_samples = np.random.normal(size=(n_samples, self.latent_dim))\n",
    "        synthetic_data = self.decoder.predict([z_samples, condition])\n",
    "        return synthetic_data\n",
    "\n",
    "\n",
    "    def visualize_latent_space(self, x_data, c_data, labels, n_samples):\n",
    "        c_data = np.array(data_c)\n",
    "        n_samples = min(n_samples, len(x_data), len(c_data))\n",
    "        indices = np.random.choice(len(x_data), n_samples, replace=False) ## data sampling for n\n",
    "        x_sample = x_data.iloc[indices]\n",
    "        c_sample = c_data[indices]\n",
    "        # Convert labels to numpy array if necessary and sample\n",
    "        if isinstance(labels, pd.Series) or isinstance(labels, pd.DataFrame):\n",
    "            labels_sample = labels.iloc[indices]\n",
    "        else:\n",
    "            labels_sample = labels[indices]\n",
    "        z_mean, _, _ = self.encoder.predict([x_sample, c_sample]) ## calculating latent vector z\n",
    "        tsne = TSNE(n_components=2, random_state=710674) #tsne visualization\n",
    "        z_tsne = tsne.fit_transform(z_mean)\n",
    "\n",
    "        ## visualization\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if labels is not None:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=labels_sample, cmap='viridis')\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            plt.scatter(z_tsne[:, 0], z_tsne[:, 1])\n",
    "        plt.title('t-SNE visualization of the latent space')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb698af1-02a1-42f9-99b2-10b84c29e61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvae = CVAE(input_dim = data_x.shape[1], condition_dim = data_c.shape[1], latent_dim=args_cvae.latent_dim)\n",
    "# cvae = CVAE(input_dim = data_x.shape[1], condition_dim = 1, latent_dim=args_cvae.latent_dim)\n",
    "cvae.train(data_x, data_c, epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e43ee8-4e44-40ac-8641-1d9e1deb386f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b307b6f-6507-4b5c-9a48-5189c76de85d",
   "metadata": {},
   "source": [
    "> Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a88eb1-3c0a-4ed7-a960-b4f28db3d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.visualize_latent_space(data_x, data_c, label, n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60811c06-abb8-4ae6-8ffc-3a1d1c034c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "250d704d-a4bc-466d-923b-3ba46eb6adf2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28d859-6da8-4689-9015-64a6ed57f6e6",
   "metadata": {},
   "source": [
    "> Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c22a4-2c6b-4fd8-8a2d-4777c2ea060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd3c54-22b0-451a-b700-7f72ea906575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a859cc-609e-4221-ba57-a36c67cae09e",
   "metadata": {},
   "source": [
    "#### Using single CVAE model (integrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327c748-b69d-48f8-bbf9-b17b8a6fdcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data(cvae_model, c_data, n_samples_per_class, labels, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## filtering the condition for each class\n",
    "        class_indices = labels == class_label\n",
    "        class_conditions = c_data[class_indices]\n",
    "        \n",
    "        if class_conditions.empty:\n",
    "            print(f\"Warning: No data found for class label {class_label}. Skipping this class.\")\n",
    "            continue\n",
    "        \n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            class_new_conditions = class_conditions.sample(n=n_samples, replace=True).values\n",
    "        ## calculate mean and variance from original condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = class_conditions.mean(axis=0).values\n",
    "            std = class_conditions.std(axis=0).values\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953103f1-47aa-4fcc-9149-3741d58ae38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the number of samples to augment for each data class\n",
    "n_samples_per_class = {0: (850-data_cvae.condition.value_counts()[1]), 1: (850-data_cvae.condition.value_counts()[0]), 2: (data_cvae.condition.value_counts()[2])}\n",
    "\n",
    "## synthetic data generation (augmentation)\n",
    "synthetic_data = generate_class_synthetic_data(cvae, data_c, n_samples_per_class, label, method='random')\n",
    "# synthetic_data = generate_class_synthetic_data(cvae, data_c, n_samples_per_class, label, method='gaussian')\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0(MDD) - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(PD) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(CON) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0ccfc-39f5-4aac-b8d9-bb2d77da638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_int = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_nos = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "gen_tip = pd.DataFrame(synthetic_data[2], columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9597c3-20d3-4d2f-92d9-906a63768324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ae357a-e499-4c63-b960-127dc31cc4be",
   "metadata": {},
   "source": [
    "====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79db0e-193b-49f1-903d-efb16df100a7",
   "metadata": {},
   "source": [
    "#### Using individual CVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c11fdb-1e09-4d8f-bf39-b719c207c08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## we should train individual CVAE model for each class\n",
    "cvae_models = {}\n",
    "\n",
    "for class_label in np.unique(label):\n",
    "    ## data filtering to find dataset with that class label\n",
    "    class_indices = np.where(label == class_label)[0]\n",
    "    class_data = data_x.iloc[class_indices]\n",
    "    \n",
    "    ## initialize and train CVAE model\n",
    "    cvae = CVAE(input_dim=data_x.shape[1], latent_dim=args_cvae.latent_dim, condition_dim=data_y.shape[1])\n",
    "    cvae.train(class_data, data_y[class_indices], epochs=args_cvae.epochs, batch_size=args_cvae.bs, validation_split=0.2)\n",
    "    \n",
    "    ## save the trained model\n",
    "    cvae_models[class_label] = cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a008b0-8826-4d9a-8644-106754f472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_synthetic_data_seperately(cvae_models, c_data, n_samples_per_class, labels, method):\n",
    "    synthetic_data = {}\n",
    "    new_conditions = {}\n",
    "\n",
    "    for class_label, n_samples in n_samples_per_class.items():\n",
    "        ## loading trained CVAE model for each class\n",
    "        cvae_model = cvae_models[class_label]\n",
    "        \n",
    "        ## filtering the condition for the class\n",
    "        class_indices = labels == class_label\n",
    "        class_conditions = c_data[class_indices]\n",
    "\n",
    "        ## generating new condition for conditional VAE data augmentation\n",
    "        ## randomly sampling from the original condition dataset\n",
    "        if method == 'random':\n",
    "            class_new_conditions = class_conditions.sample(n=n_samples, replace=True).values\n",
    "        ## calculate mean and variance from origianl condition dataset to sampling\n",
    "        elif method == 'gaussian':\n",
    "            mean = class_conditions.mean(axis=0).values\n",
    "            std = class_conditions.std(axis=0).values\n",
    "            class_new_conditions = np.random.normal(loc=mean, scale=std, size=(n_samples, class_conditions.shape[1]))\n",
    "        \n",
    "        ## generate synthetic dataset for each class with new conditions\n",
    "        class_synthetic_data = cvae_model.generate_synthetic_data(class_new_conditions, n_samples=n_samples)\n",
    "        \n",
    "        ## save the synthesized results\n",
    "        synthetic_data[class_label] = class_synthetic_data\n",
    "        new_conditions[class_label] = class_new_conditions\n",
    "    \n",
    "    # return synthetic_data, new_conditions\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059828f-ccbb-4741-b0ee-dbc284ca9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030048a-e7d7-4460-a8ce-96cbf8c41b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88a138-0a8b-46f6-9bda-378c13f8d8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7fa2e-004d-460a-be48-541bdc9515c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21353-7556-4997-8e54-caa1a82657fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = {0: (850-data_cvae.condition.value_counts()[1]), 1: (850-data_cvae.condition.value_counts()[0]), 2: (data_cvae.condition.value_counts()[2])}\n",
    "\n",
    "# synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_c, n_samples_per_class, label, method=\"random\")\n",
    "synthetic_data = generate_class_synthetic_data_seperately(cvae_models, data_c, n_samples_per_class, label, method=\"gaussian\")\n",
    "\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"Class 0(MDD) - Generated Synthetic Data: \")\n",
    "# print(synthetic_data[0])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(PD) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[1])\n",
    "# print(\"\\n=====================================\")\n",
    "# print(\"\\nClass 1(CON) - Generated Synthetic Data:\")\n",
    "# print(synthetic_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be256307-b7a2-46f4-b9dc-f802cabed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into the dataframe format\n",
    "gen_int = pd.DataFrame(synthetic_data[0], columns=data_x.columns)\n",
    "gen_nos = pd.DataFrame(synthetic_data[1], columns=data_x.columns)\n",
    "gen_tip = pd.DataFrame(synthetic_data[2], columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7304bf-bf37-45e6-b45c-6c5d6a100bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc6eb8f-52e9-4376-83f1-078ccd4f12f9",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674ff09-5db3-4fed-bc2a-73e3e52014c5",
   "metadata": {},
   "source": [
    "> Reconstruct original dataset using trained model to adopt augmented trainig dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69f6e-c4a0-40b0-a1d3-28a848beb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, z_latent = cvae.encode(data_x, data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff0d62-6fc1-49f3-a942-4de6e40a1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = cvae.decode(z_latent, data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ad27b-0745-49aa-aa0a-321622529aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81749b-fbbc-427d-aa50-42597c245810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159035-db8f-414b-a9d2-d322c21044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8491406-0098-4a01-bfe1-68caacbaefb3",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c356d3-bcaa-4389-a583-4c5fab1dce89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264f9bb-bc66-40a9-90c9-0bb4bdc25965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2aad34-7583-4996-aaee-cd00c2241feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50635f7-443e-45af-8a07-b55c9ced68cf",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd232e5c-c1a7-4d8f-8144-14dc2bf64f50",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8736b-d296-4c8d-aace-9dd5d109a0cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Latent Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788b712-d306-4b80-9017-b89025cd26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_diffusion:\n",
    "    # arugments\n",
    "    epochs=80\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    latent_dim = 4\n",
    "    seed=710674\n",
    "\n",
    "args_diffusion = Args_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e0517-bac5-41f1-9ea5-574aa3a05e4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "* preparing x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51d8f6-f44c-4a88-bda9-5f174523aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diffusion = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234de504-6572-4337-8ecc-ac194e367ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CON = data_diffusion[data_diffusion['disorder']==\"con\"]\n",
    "data_PD = data_diffusion[data_diffusion['disorder']==\"pd\"]\n",
    "data_MDD = data_diffusion[data_diffusion['disorder']==\"mdd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55cb8e-fc72-4bd4-bf13-c6ef1f5e3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list = ['age', 'gender', 'disorder']\n",
    "drop_list = scale_list + etc_list + demo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0577fb-4861-4013-ae60-d300183aa2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_diffusion.drop((drop_list), axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_diffusion.disorder# mdd / pd / con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043a2b4-d4de-4aad-b1ab-3e114940842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'mdd': 0})\n",
    "label = label.replace({'pd': 1})\n",
    "label = label.replace({'con' : 2})\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61450ab-71aa-4ac2-99f1-43d3520a5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of x dataset is:\", data_x.shape)\n",
    "print(\"The size of y dataset is:\", data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ed243-4f13-448b-9c57-3d834db8bc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6323d725-767c-4267-81df-536e5ded49a1",
   "metadata": {},
   "source": [
    "* 1 Dimensional latent diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad250c5-f929-4c28-bb6f-57c3c603bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionVAE:\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.vae = self.build_vae()\n",
    "        \n",
    "    ## sampling - reparameterization\n",
    "    def sampling(self, repara):\n",
    "        z_mean, z_log_var = repara\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        z_mean = layers.Dense(self.latent_dim)(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim)(x)\n",
    "        return Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "    def build_decoder(self):\n",
    "        latent_inputs = Input(shape=(self.latent_dim,))\n",
    "        x = layers.Dense(128, activation='relu')(latent_inputs)\n",
    "        x = layers.Dense(np.prod(self.input_shape), activation='relu')(x)\n",
    "        x = layers.Reshape(self.input_shape)(x)\n",
    "        outputs = layers.Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "        return Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    def build_vae(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = layers.Lambda(self.sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "        outputs = self.decoder(z)\n",
    "\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "        # Define the VAE loss\n",
    "        reconstruction_loss = MeanSquaredError()(inputs, outputs)\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        vae_loss = reconstruction_loss + kl_loss\n",
    "        vae.add_loss(vae_loss)\n",
    "        vae.compile(optimizer='adam')\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def train(self, data_x, epochs, batch_size, validation_split=0.2, verbose=2):\n",
    "        self.vae.fit(data_x, epochs=epochs, validation_split=validation_split, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    def generate(self, latent_sample):\n",
    "        return self.decoder.predict(latent_sample)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming data_x and args_diffusion are already defined\n",
    "\n",
    "input_shape = (data_x.shape[1], 1)\n",
    "latent_dim = args_diffusion.latent_dim\n",
    "\n",
    "vae_model = LatentDiffusionVAE(input_shape, latent_dim)\n",
    "vae_model.train(data_x, epochs=args_diffusion.epochs, batch_size=args_diffusion.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749719a-3384-4aa1-8339-1c6e0f6b98c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49bee9-771d-45ae-80be-214694620a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e4ec70-9398-4957-9e9f-3afcf76cf4c7",
   "metadata": {},
   "source": [
    "--- working code (start) --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe80ac-7856-4761-aa36-e580cf9e024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the latent space\n",
    "def sampling(repara):\n",
    "    z_mean, z_log_var = repara\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define encoder\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    return Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "# Define decoder\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(128, activation='relu')(latent_inputs)\n",
    "    x = Dense(np.prod(output_shape), activation='relu')(x)\n",
    "    x = Reshape(output_shape)(x)\n",
    "    outputs = Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "    return Model(latent_inputs, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dba56-edca-46c7-a855-1d38223010ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (data_x.shape[1], 1)  # Example input shape\n",
    "latent_dim = args_diffusion.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b2e74-1c33-412d-8922-6172b7923bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define the VAE\n",
    "encoder = build_encoder(input_shape, latent_dim)\n",
    "decoder = build_decoder(input_shape, latent_dim)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "z_mean, z_log_var = encoder(inputs)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "outputs = decoder(z)\n",
    "\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "# Define the VAE loss\n",
    "reconstruction_loss = MeanSquaredError()(inputs, outputs)\n",
    "kl_loss = -0.5 * tf.reduce_mean(\n",
    "    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dc914-b23c-427a-b90f-cc5e3a0dfc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb65314-efc8-4b76-b8a9-780e86f5123a",
   "metadata": {},
   "source": [
    "* model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e2eab-5606-4188-9d1b-23fdad6c2c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "vae.fit(data_x, epochs=args_diffusion.epochs, validation_split=0.2, batch_size=args_diffusion.bs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22559155-2211-4879-ad88-819bc3a56e34",
   "metadata": {},
   "source": [
    "--- working code (end) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56313a0-e1e4-41c3-8043-3083af56d4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75487eb-4981-48dc-8057-21a92f2f7285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a3418-2b88-4a91-95b8-52c1292ab49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905fb40-d68d-4a01-b73d-60acfbd944bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff6241-7e4b-4827-a1f6-44ba65133932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb965bcb-a722-454f-b3ad-498d54d08094",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981c57e-13b4-4b5e-aa19-c6cfe91c989a",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b774b0d-996a-492a-8930-56b8498703c4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d74df3-ad0e-4d1e-903f-ba88f4613b8e",
   "metadata": {},
   "source": [
    "# Classification performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ea892-6b25-42dc-a749-143b32efbea8",
   "metadata": {},
   "source": [
    "## Original dataset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32baa1-3aad-4685-adcd-06aebe9b98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dnn = data_ori.copy()\n",
    "data_dnn = df_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2788ee0-421c-42c0-a42e-bbaa7e783ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['condition', 'higuci', 'sampen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3899c-0c1f-4ac2-8fc2-a0ce231a6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dnn.drop(drop_list, axis=1)\n",
    "x = x.fillna(x.mean())\n",
    "y = data_dnn.condition# mdd / pd / con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ecabb-7609-45d1-8e73-af98ef90962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = x.copy()\n",
    "data_x[:] = (scaler.fit_transform(data_x[:])).round(decimals=6) ## scaling x values\n",
    "\n",
    "label = y.copy()\n",
    "label = label.replace({'interruption':0})\n",
    "label = label.replace({'no stress': 1})\n",
    "label = label.replace({'time pressure': 2})\n",
    "data_y = to_categorical(label, 3) ## into the format of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e78f59-c643-476e-9f0c-37c3fec6b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, random_state = 710674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8c93e-3b36-44db-be80-ba7b082b58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## definition for computing class weight (to solve class imbalance issue)\n",
    "def compute_class_weights(y):\n",
    "    class_counts = np.bincount(y) ## calculating data point for each class\n",
    "    total_samples = len(y) ## total data points\n",
    "    ## computing each class weight\n",
    "    class_weights = {i: total_samples / (len(class_counts) * class_count) \n",
    "                     for i, class_count in enumerate(class_counts)}\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385e133-6da1-4443-936c-c26a81b9c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weights(label)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04a4cb-6974-4cca-8409-33654a335435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa41a9dd-4361-49e2-bd64-ea3cb5c71d67",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaaf7d3-f1da-4632-bb89-c31e45e79ee4",
   "metadata": {},
   "source": [
    "## Augmented dataset add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23165eff-832d-482a-b4eb-7915a4c29b26",
   "metadata": {},
   "source": [
    "> we generate synthesized dataset using VAE from above. \\\n",
    "> adopted original datset: data_mdd, data_bpi, data_bpii \\\n",
    "> synthesized into: gen_mdd, gen_bpi, gen_bpii \\\n",
    "> gen_control is not generated since \"control group\" has biggest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daafef-9b75-4899-8111-f44278b6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class synthetic_data_preparation:\n",
    "    def __init__(self, original_data, gen_data, label_col, id_cols=None, drop_cols=None):\n",
    "        \"\"\"\n",
    "        original_data (pandas.DataFrame): original dataset\n",
    "        gen_data (dict): data dictionary for synthetic dataset (ex: {0: gen_A, 1: gen_B})\n",
    "        label_col (str): label column (target variable)\n",
    "        id_cols (list): column list to drop (예: ['Unnamed: 32', 'id'])\n",
    "        drop_cols (list): column list to drop from original dataset\n",
    "        \"\"\"\n",
    "        self.original_data = original_data\n",
    "        self.gen_data = gen_data\n",
    "        self.label_col = label_col\n",
    "        self.id_cols = id_cols if id_cols is not None else []\n",
    "        self.drop_cols = drop_cols if drop_cols is not None else []\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        ## preprocessing original dataset\n",
    "        data_dnn = self.original_data.copy()\n",
    "        for col in self.id_cols:\n",
    "            if col in data_dnn.columns:\n",
    "                data_dnn = data_dnn.drop(col, axis=1)\n",
    "        \n",
    "        data_dnn = data_dnn.drop(self.drop_cols, axis=1)\n",
    "        data_classes = {label: data_dnn[data_dnn[self.label_col] == label] for label in data_dnn[self.label_col].unique()}\n",
    "        \n",
    "        ## preprocessing synthetic generated dataset\n",
    "        for label, df in self.gen_data.items():\n",
    "            df[self.label_col] = label\n",
    "            if self.drop_cols:\n",
    "                df = df.drop(self.drop_cols, axis=1)\n",
    "            self.gen_data[label] = df\n",
    "        \n",
    "        ## concat original dataset and generated dataset\n",
    "        ori_df_concat = pd.concat(list(data_classes.values()), ignore_index=True)\n",
    "        gen_df_concat = pd.concat(list(self.gen_data.values()), ignore_index=True)\n",
    "        \n",
    "        ## separating feature and label(target)\n",
    "        ori_x = ori_df_concat.drop([self.label_col], axis=1)\n",
    "        ori_y = ori_df_concat[[self.label_col]]\n",
    "        \n",
    "        gen_x = gen_df_concat.drop([self.label_col], axis=1)\n",
    "        gen_y = gen_df_concat[[self.label_col]]\n",
    "        \n",
    "        ## filling missing values and scaling\n",
    "        ori_x = ori_x.fillna(ori_x.mean())\n",
    "        ori_x[:] = (scaler.fit_transform(ori_x[:])).round(decimals=6) ## scaling x values\n",
    "        \n",
    "        # 레이블 인코딩 및 원-핫 인코딩\n",
    "        ori_y = ori_y.replace({self.label_col: {label: idx for idx, label in enumerate(ori_y[self.label_col].unique())}})\n",
    "        y_ori = to_categorical(ori_y, num_classes=len(ori_y[self.label_col].unique()))\n",
    "        \n",
    "        gen_y = gen_y.replace({self.label_col: {label: idx for idx, label in enumerate(gen_y[self.label_col].unique())}})\n",
    "        y_gen = to_categorical(gen_y, num_classes=len(gen_y[self.label_col].unique()))\n",
    "        \n",
    "        ## separating training and test dataset\n",
    "        x_trainset, x_test, y_trainset, y_test = train_test_split(ori_x, y_ori, test_size=0.35, random_state=710674)\n",
    "        \n",
    "        ## add generated dataset to training dataset\n",
    "        x_train_concat = pd.concat([x_trainset, gen_x], ignore_index=True)\n",
    "        y_train_concat = np.concatenate([y_trainset, y_gen])\n",
    "        \n",
    "        ## separating training dataset and validation dataset\n",
    "        x_train, x_vali, y_train, y_vali = train_test_split(x_train_concat, y_train_concat, test_size=0.2, random_state=710674)\n",
    "        \n",
    "        return x_train, x_vali, x_test, y_train, y_vali, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9528204-5e13-47a1-9266-dd4488568e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215632f-f314-4022-84a0-1d6769e779ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list_ = ['higuci', 'sampen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f4218-b67d-46d3-86ed-47cc98ac34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a dictionary for generated dataset\n",
    "gen_data = {\"interruption\":gen_int, \"no stress\":gen_nos, \"time pressure\":gen_tip}\n",
    "\n",
    "## data_preparation class initialize and data prepare\n",
    "data_prep = synthetic_data_preparation(df_reduced, gen_data, label_col='condition', id_cols=drop_list_)\n",
    "x_train, x_vali, x_test, y_train, y_vali, y_test = data_prep.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39ae97-c519-495a-b9cb-8774160b7cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9ddb173-633a-43d0-9f51-32ae7cb6fec5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c2ee7-3221-4217-8794-c19a904f91d5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2bfda-069a-47e9-9ce5-d579238f3abd",
   "metadata": {},
   "source": [
    "## Simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedc655-0b50-4bc7-b3d6-0832d939efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_dnn:\n",
    "    # arugments\n",
    "    epochs=120\n",
    "    bs=32\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    seed=710674\n",
    "\n",
    "args_dnn = Args_dnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bade4d-02ae-4a3b-ae78-3441f5c033b7",
   "metadata": {},
   "source": [
    "* DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859ec14-825f-4665-8dcc-4c4054076f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDNN:\n",
    "    def __init__(self, input_dim, layer_configs, output_units, output_activation='softmax'):\n",
    "        self.input_dim = input_dim ## input data dimensionality (# variables)\n",
    "        self.layer_configs = layer_configs ## hidden layer/sequential layer lists (units, activation, batch_norm, dropout_rate)\n",
    "        self.output_units = output_units ## output unit no.\n",
    "        self.output_activation = output_activation ## activation function for output layer\n",
    "        self.model = self.build_model()\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], activation=self.layer_configs[0]['activation'], input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            ## AddLearningRateScheduler callback\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "        \n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_split=None, class_weight=None, validation_data=None):\n",
    "        return self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split,\n",
    "                             class_weight=class_weight, validation_data = validation_data, callbacks=self.callbacks)\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8723a2-895c-43b4-b031-c869d0f2af3b",
   "metadata": {},
   "source": [
    "* optimization function, model compile, and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28545248-f087-4bc1-9a84-7262b9179abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_learning_rate(epoch, mode='cyclic', base_lr=0.001, max_lr=0.006, step_size=2000, gamma=0.99994):\n",
    "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
    "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
    "    \n",
    "    if mode == 'cyclic' or mode == 'triangular':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n",
    "    elif mode == 'triangular2':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) / (2 ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x)) * (gamma ** epoch)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose from 'cyclic', 'triangular', 'triangular2', or 'exp_range'.\")\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705bb04-1f9d-49d7-8d45-fd99ca74ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = dynamic_learning_rate(epoch=1000, mode='cyclic')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='triangular2')\n",
    "# lr = dynamic_learning_rate(epoch=1000, mode='exp_range', gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a13af6-6827-4fae-adcf-d37f47ae7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True},\n",
    "    {'units': 128, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 128, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True},\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': True}\n",
    "]\n",
    "\n",
    "model_dnn = SimpleDNN(output_units=args_dnn.num_classes, input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be7953-f7eb-4b66-9e38-06290a75d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=4.):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "#         y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "#         alpha_t = y_true * alpha + (K.ones_like(y_true) - y_true) * (1 - alpha)\n",
    "#         p_t = y_true * y_pred + (K.ones_like(y_true) - y_true) * (1 - y_pred)\n",
    "#         fl = - alpha_t * K.pow((K.ones_like(y_true) - p_t), gamma) * K.log(p_t)\n",
    "#         return K.mean(fl)\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfa22a-1667-4b9e-b0c2-691d71236332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "model_dnn.compile_model(optimizer = opt, lr_scheduler=scheduler)\n",
    "# model_dnn.compile_model(optimizer=opt, loss = focal_loss())\n",
    "\n",
    "## model summary\n",
    "model_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975db5d-b42e-4329-9f11-7f9b70e426ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### model training on training dataset\n",
    "history = model_dnn.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, validation_split=0.2)\n",
    "# history = model_dnn.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, validation_data=(x_vali, y_vali))\n",
    "# history = model_dnn.fit_model(x_train, y_train, epochs=args_dnn.epochs, batch_size=args_dnn.bs, class_weight = class_weight, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d932e1-1b33-41e4-b971-fbed0e368450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bba6c6d9-4109-460b-974e-991c3d3e4b0b",
   "metadata": {},
   "source": [
    "* Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2dfb7a-0f46-41e4-9776-e5a11cef658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_test, y_test):\n",
    "\n",
    "    ## predict on model\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_predict_classes = np.argmax(y_predict, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    ## calculate confusion matrix and visualize\n",
    "    cm = confusion_matrix(y_test_classes, y_predict_classes, normalize='pred')\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, fmt='.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    ## evaluation metrics\n",
    "    accuracy = accuracy_score(y_test_classes, y_predict_classes)\n",
    "    precision = precision_score(y_test_classes, y_predict_classes, average='macro')\n",
    "    recall = recall_score(y_test_classes, y_predict_classes, average='micro')\n",
    "    f1 = f1_score(y_test_classes, y_predict_classes, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_predict, multi_class='ovr')\n",
    "    \n",
    "    ## Results\n",
    "    print(\"=============================================\")\n",
    "    print(f\"The overall accuracy is: {accuracy:.4f}\")\n",
    "    print(f\"The precision score is: {precision:.4f}\")\n",
    "    print(f\"The recall score is: {recall:.4f}\")\n",
    "    print(f\"The F1 score is: {f1:.4f}\")\n",
    "    print(f\"The AUC score is: {auc:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    ## Print out the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee306ab-7e58-4dbe-ae53-81288849ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model_dnn, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d88a1-b1b6-4386-a3c8-9bdde0cef008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c45e383-4895-4dda-813c-ead039ec9009",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91ad0f-2d8f-49a7-8b97-85cd38696300",
   "metadata": {},
   "source": [
    "## K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f292e-fbce-486e-8e2f-9b87e479afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_kfold:\n",
    "    # arugments\n",
    "    epochs=70\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    split=5\n",
    "    seed=710674\n",
    "\n",
    "args_kfold = Args_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e25cb-fbad-4575-8461-d42ce17af307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kfoldMLP:\n",
    "    def __init__(self, input_dim, layer_configs, output_units, output_activation='softmax'):\n",
    "        self.input_dim = input_dim\n",
    "        self.layer_configs = layer_configs\n",
    "        self.output_units = output_units\n",
    "        self.output_activation = output_activation\n",
    "        self.callbacks = []\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        ## add first hidden layer\n",
    "        model.add(layers.Dense(units=self.layer_configs[0]['units'], \n",
    "                               activation=self.layer_configs[0]['activation'], \n",
    "                               input_shape=(self.input_dim,)))\n",
    "        \n",
    "        ## batch normalization and dropout for first layer\n",
    "        if self.layer_configs[0].get('batch_norm', False):\n",
    "            model.add(layers.BatchNormalization())\n",
    "        \n",
    "        if self.layer_configs[0].get('dropout_rate', None) is not None:\n",
    "            model.add(layers.Dropout(rate=self.layer_configs[0]['dropout_rate']))\n",
    "        \n",
    "        ## do same for rest hidden layers (except for the last)\n",
    "        for config in self.layer_configs[1:]:\n",
    "            model.add(layers.Dense(units=config['units'], activation=config['activation']))\n",
    "            \n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(layers.Dropout(rate=config['dropout_rate']))\n",
    "        \n",
    "        ## add output layer\n",
    "        model.add(layers.Dense(units=self.output_units, activation=self.output_activation))\n",
    "        self.model = model\n",
    "\n",
    "    ## model compile\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            self.callbacks.append(tf.keras.callbacks.LearningRateScheduler(lr_scheduler))\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_data=None, class_weight=None, verbose=0):\n",
    "        return self.model.fit(\n",
    "            x_train, y_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=validation_data, \n",
    "            callbacks=self.callbacks, \n",
    "            class_weight=class_weight,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.model.predict(x)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def cross_validate(self, x_data, y_data, n_splits, epochs, batch_size, optimizer=None, class_weight=None):\n",
    "        x_data = np.array(x_data)\n",
    "        y_data = np.array(y_data)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True) ##set fold split and shuffle\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(x_data), 1):\n",
    "            x_train, x_val = x_data[train_index], x_data[val_index]\n",
    "            y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "\n",
    "            ## build new model\n",
    "            self.build_model()\n",
    "            self.compile_model(optimizer=optimizer)\n",
    "            \n",
    "            ## model training\n",
    "            self.fit_model(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), class_weight=class_weight, verbose=0)\n",
    "            \n",
    "            ## evaluate the model\n",
    "            y_val_pred = self.predict(x_val)\n",
    "            if isinstance(y_val_pred, list):\n",
    "                y_val_pred = np.array(y_val_pred)\n",
    "            y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "            y_val_true = np.argmax(y_val, axis=1)\n",
    "            \n",
    "            accuracy = accuracy_score(y_val_true, y_val_pred)\n",
    "            precision = precision_score(y_val_true, y_val_pred, average='macro')\n",
    "            recall = recall_score(y_val_true, y_val_pred, average='macro')\n",
    "            f1 = f1_score(y_val_true, y_val_pred, average='weighted')\n",
    "            auc = roc_auc_score(y_val, self.predict(x_val), multi_class='ovr')\n",
    "            \n",
    "            fold_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'auc': auc\n",
    "            })\n",
    "\n",
    "            print(f\"Fold metrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1 Score={f1:.4f}, AUC={auc:.4f}\")\n",
    "        \n",
    "        avg_metrics = {key: np.mean([m[key] for m in fold_metrics]) for key in fold_metrics[0].keys()}\n",
    "        \n",
    "        print(\"\\nAverage metrics across all folds:\")\n",
    "        for metric, value in avg_metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        return fold_metrics, avg_metrics\n",
    "\n",
    "    ## evaluation on test dataset (for performance check)\n",
    "    def evaluate_test_data(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true_labels, y_pred_labels, normalize='pred')\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        print(\"=============================================\")\n",
    "\n",
    "        # Classification Report\n",
    "        class_report = classification_report(y_true_labels, y_pred_labels, target_names=[f'Class {i}' for i in range(self.output_units)])\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        return cm, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e0691-d62b-481a-86f0-58486dc577b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model initialization with hidden layer list below\n",
    "layer_configs = [\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True},\n",
    "    {'units': 128, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 128, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True},\n",
    "    {'units': 32, 'activation': 'relu', 'batch_norm': True}\n",
    "]\n",
    "\n",
    "kfold = kfoldMLP(output_units=args_kfold.num_classes, input_dim=x_train.shape[1], layer_configs=layer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc1dce-e3a0-4b02-a443-8e1c8651136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile our model\n",
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "\n",
    "## model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67247754-33e6-4083-b5b3-a1bd47e88686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-Fold 교차 검증 실행\n",
    "metrics, avg_metrics = kfold.cross_validate(x_train, y_train, n_splits=args_kfold.split, optimizer=opt, epochs=args_kfold.epochs, batch_size=args_kfold.bs)\n",
    "\n",
    "# # 각 폴드의 성능 지표 출력\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     print(f\"Fold {i+1}: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee90fdf-5b26-4e4c-935f-3780eb5fe944",
   "metadata": {},
   "source": [
    "* Evaluation on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bde173-0483-46c3-9640-62c675a5c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, class_report = kfold.evaluate_test_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e8976-a689-4a0c-b744-0732f1808cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cefa5011-78f0-4c2c-b4de-6df4c4bf26d1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9453da-c2ef-4fb0-9817-68321d13236c",
   "metadata": {},
   "source": [
    "## 1D-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34654d7e-8592-45af-a55c-a4af89366dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_conv:\n",
    "    # arugments\n",
    "    epochs=120\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    seed=710674\n",
    "\n",
    "args_conv = Args_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12e02c-30c4-4729-8086-7c80830eb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1Dmodel:\n",
    "    def __init__(self, input_shape, conv_layers, dense_layers, output_units, output_activation='softmax'):\n",
    "        self.input_shape = input_shape\n",
    "        self.conv_layers = conv_layers\n",
    "        self.dense_layers = dense_layers\n",
    "        self.output_units = output_units\n",
    "        self.output_activation = output_activation\n",
    "        self.model = None\n",
    "        self.callbacks = []\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add Conv1D layers\n",
    "        for i, config in enumerate(self.conv_layers):\n",
    "            if i == 0:\n",
    "                model.add(Conv1D(\n",
    "                    filters=config['filters'],\n",
    "                    kernel_size=config['kernel_size'],\n",
    "                    activation=config['activation'],\n",
    "                    input_shape=self.input_shape\n",
    "                ))\n",
    "            else:\n",
    "                model.add(Conv1D(\n",
    "                    filters=config['filters'],\n",
    "                    kernel_size=config['kernel_size'],\n",
    "                    activation=config['activation']\n",
    "                ))\n",
    "\n",
    "            if 'pool_size' in config:\n",
    "                model.add(MaxPooling1D(pool_size=config['pool_size']))\n",
    "\n",
    "        # Add Flatten layer\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # Add Dense layers\n",
    "        for config in self.dense_layers:\n",
    "            model.add(Dense(\n",
    "                units=config['units'],\n",
    "                activation=config['activation']\n",
    "            ))\n",
    "            if config.get('batch_norm', False):\n",
    "                model.add(BatchNormalization())\n",
    "            if config.get('dropout_rate', None) is not None:\n",
    "                model.add(Dropout(rate=config['dropout_rate']))\n",
    "\n",
    "        # Add Output layer\n",
    "        model.add(Dense(self.output_units, activation=self.output_activation))\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def compile_model(self, optimizer, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=None):\n",
    "        if lr_scheduler:\n",
    "            self.callbacks.append(LearningRateScheduler(lr_scheduler))\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit_model(self, x_train, y_train, epochs, batch_size, validation_data = None, validation_split=None, class_weight=None, verbose=1):\n",
    "\n",
    "        return self.model.fit(\n",
    "            x_train, y_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            validation_data = validation_data,\n",
    "            validation_split=validation_split, \n",
    "            callbacks=self.callbacks, \n",
    "            class_weight=class_weight,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def evaluate_model(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def summary(self):\n",
    "        if self.model:\n",
    "            self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a2382-f2a9-4246-bd08-a713d8903944",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define layer configurations\n",
    "conv_layers = [\n",
    "    {'filters': 32, 'kernel_size': 5, 'activation': 'relu', 'pool_size': 3},\n",
    "    {'filters': 16, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 3}\n",
    "]\n",
    "\n",
    "dense_layers = [\n",
    "    {'units': 64, 'activation': 'relu', 'batch_norm': True, 'dropout_rate': 0.5},\n",
    "    {'units': 32, 'activation': 'relu'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8af224-a961-46fb-be8c-295610f17ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train.shape[1], 1)\n",
    "conv1d = Conv1Dmodel(input_shape = input_shape, conv_layers=conv_layers, dense_layers=dense_layers, output_units=args_conv.num_classes)\n",
    "conv1d.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e2a57-f3b5-4406-9308-cb0f6e35c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5556c9-5f1d-4341-80de-f616bf8491c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate = 0.001, decay = 1e-5, momentum = 0.9)\n",
    "scheduler = lambda epoch: dynamic_learning_rate(epoch, mode='triangular2', base_lr=0.001, max_lr=0.009, step_size=25)\n",
    "\n",
    "conv1d.compile_model(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'], lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efc81e-f9af-45ef-927a-836cd57750e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# history = conv1d.fit_model(x_train, y_train, epochs=args_conv.epochs, batch_size=args_conv.bs, class_weight=class_weight, validation_split=0.2, verbose=2)\n",
    "history = conv1d.fit_model(x_train, y_train, epochs=args_conv.epochs, batch_size=args_conv.bs, validation_data=(x_vali, y_vali), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff654a-1dbd-4148-8946-83b311ba8db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faa0c856-324b-4e49-aebe-6ce91e9f333e",
   "metadata": {},
   "source": [
    "* Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bf5a0-c722-4a58-b03b-9b745f4f0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(conv1d, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca50a3f-9437-472b-aa31-f2aec5fcaf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d42755-d708-49c3-9a73-802f899ed220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f34a41bc-3458-49be-b41e-f65006482740",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d908ff-13e6-4cca-988a-7c03231a5f36",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c55d39-853c-4c83-b58c-62a1fce5669a",
   "metadata": {},
   "source": [
    "## Sub-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7bd663-8112-4d7e-a58f-08d50c8a1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_multi:\n",
    "    # arugments\n",
    "    epochs=120\n",
    "    enc_epochs = 50\n",
    "    bs=32\n",
    "    enc_bs = 16\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    num_classes= 3\n",
    "    seed=710674\n",
    "\n",
    "args_multi = Args_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0647e-a851-4618-9a38-31d17d8791a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_, data_y_ = data_x.copy(), data_y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e8d60-457a-4031-a650-8589268462a4",
   "metadata": {},
   "source": [
    "* preprocessing the partial dataframes for subnetwork training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa61aa-aba6-4194-9305-81f001fd746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subnetwork_preprocess:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data_dict = {}\n",
    "\n",
    "    def select_columns(self, prefixes):\n",
    "        self.data_dict = {prefix: self.data.loc[:, self.data.columns.str.startswith(f'{prefix}_')] \n",
    "                          for prefix in prefixes}\n",
    "        return self.data_dict\n",
    "\n",
    "    def create_inputs(self):\n",
    "        self.inputs = {name: Input(shape=(data.shape[1],)) for name, data in self.data_dict.items()}\n",
    "        return self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f473c-0761-4ad6-99b2-826df3bda8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining prefixies (criteria for variable selection)\n",
    "prefixes = ['b1', 's', 'b2', 'r', 'b3', 'c']\n",
    "\n",
    "## pass dataframe to generate class instance\n",
    "processor = subnetwork_preprocess(data_x_)\n",
    "\n",
    "## data columns selection based on the prefix\n",
    "data_dict = processor.select_columns(prefixes)\n",
    "\n",
    "## create input layers(for keras) with selected prefix inputs\n",
    "inputs = processor.create_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124feef-173c-4f67-9bb1-17adf42ee38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 각 입력 레이어에 접근\n",
    "input_b1, input_s, input_b2, input_r, input_b3, input_c = inputs['b1'], inputs['s'], inputs['b2'], inputs['r'], inputs['b3'], inputs['c']\n",
    "\n",
    "## 각 변수별 데이터셋 추출\n",
    "data_b1, data_s, data_b2, data_r, data_b3, data_c = data_dict['b1'], data_dict['s'], data_dict['b2'], data_dict['r'], data_dict['b3'], data_dict['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0219b3b-8f85-4cd0-bca0-ce19fbc527c7",
   "metadata": {},
   "source": [
    "* Creating commonly using sub-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca303e5-5fd9-4051-b1f2-60968c6b5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subnetModel:\n",
    "    def __init__(self, input_layers):\n",
    "        self.input_layers = input_layers\n",
    "    \n",
    "    ## subnetwork(AE structure) for each variable group\n",
    "    def create_subnetworks(self, input_layer):\n",
    "        x = Dense(32, activation='relu')(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        # x = Dropout(0.3)(x)\n",
    "        x = Dense(8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_subnetworks(self):\n",
    "        return {name: self.create_subnetworks(layer) for name, layer in self.input_layers.items()}\n",
    "\n",
    "    ## combined network: subnetworks + fully-connected layers\n",
    "    def build_combined(self, subnetworks):\n",
    "        combined = Concatenate()(list(subnetworks.values()))\n",
    "        x = Dense(64, activation='relu')(combined)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        x = Dense(8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_final_output(self, combined_x, num_classes):\n",
    "        return Dense(num_classes, activation='softmax')(combined_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dea061-fd48-4380-955f-27fdbcc4d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시:\n",
    "input_layers = {\n",
    "    'b1': input_b1,\n",
    "    's': input_s,\n",
    "    'b2': input_b2,\n",
    "    'r': input_r,\n",
    "    'b3': input_b3,\n",
    "    'c': input_c\n",
    "}\n",
    "\n",
    "model_builder = subnetModel(input_layers)\n",
    "subnetworks = model_builder.build_subnetworks()\n",
    "combined_x = model_builder.build_combined(subnetworks)\n",
    "\n",
    "# 최종 출력 레이어 정의 (4-class 분류, args_multi.num_classes 사용)\n",
    "final_output = model_builder.build_final_output(combined_x, args_multi.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708f99d-3805-45eb-a0a1-1ce5ff361806",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd93910-0132-43b6-9d91-1d62416580e7",
   "metadata": {},
   "source": [
    "* Train-test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404b181-9eb5-47e5-9bef-6ed4f97b3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing training dataset and test dataset\n",
    "b1_train, b1_test, s_train, s_test, b2_train, b2_test, r_train, r_test, b3_train, b3_test, c_train, c_test, y_train, y_test = train_test_split(\n",
    "    data_b1, data_s, data_b2, data_r, data_b3, data_c, data_y_, test_size=0.2, random_state=710674)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4fabf-9aa0-49c5-bbbd-94ce6be46311",
   "metadata": {},
   "source": [
    "* Optimization function and learning rate scheduler settings, model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93018846-c282-4d97-96cc-dd965c0aa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = dynamic_learning_rate(args_multi.epochs, mode='cyclic')\n",
    "opt = keras.optimizers.SGD(learning_rate = args_multi.lr, decay = 1e-6, momentum = args_multi.momentum)\n",
    "\n",
    "model = Model(inputs=[input_b1, input_s, input_b2, input_r, input_b3, input_c], outputs=final_output)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c091a37-6cca-4e05-8c6d-8f4c1ce35366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([b1_train, s_train, b2_train, r_train, b3_train, c_train], y_train, \n",
    "          epochs=args_multi.epochs, batch_size=args_multi.bs, \n",
    "          validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee81e24-7aec-4f05-af84-4116dee53148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981cab1a-51e9-4094-a57e-4b056719afde",
   "metadata": {},
   "source": [
    "* Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a774cf0-562f-4ba3-a1eb-8c0d20d19cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, [b1_test, s_test, b2_test, r_test, b3_test, c_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd1831-ae0c-4038-b217-788fb383295a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b132d0d4-f53c-4b36-a37a-8fdd8213fdae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860653a-0fb4-4e00-a68f-d9dbd78bdc8c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e460b-655c-42a7-a42d-11f6e9d0425b",
   "metadata": {},
   "source": [
    "## EV input & MV output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7db1f0-6786-4f26-a693-02fa2067e79c",
   "metadata": {},
   "source": [
    "> What if we use external variables(ex. demographics) as input variables and set the DNN model to find Medical Variables as an output variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881bf1a-9503-4099-84cc-07f498c298b9",
   "metadata": {},
   "source": [
    "### data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf12b17-199f-4269-bf53-9a513cfa7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evinput = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77471e0f-d6c4-4da3-931f-38c50212af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evinput['disorder'] = data_evinput['disorder'].replace({\"mdd\": 0})\n",
    "data_evinput['disorder'] = data_evinput['disorder'].replace({\"pd\": 1})\n",
    "data_evinput['disorder'] = data_evinput['disorder'].replace({\"con\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0605cd-07ce-481a-9e46-3a3e05c04210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_evinput.head()\n",
    "# data_evinput.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3bbce-f059-4138-9696-f474598a3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = ['HAMD', 'HAMA', 'PDSS', 'ASI', 'APPQ', 'PSWQ', 'SPI', 'PSS', 'BIS', 'SSI']\n",
    "etc_list = ['sub', 'VISIT', 'disorder_mdd']\n",
    "demo_list = ['age', 'gender', 'disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c88bcb-8b08-471e-bbba-0c657b2d3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_input = data_evinput.loc[:, ('disorder', 'age', 'gender')]\n",
    "ev_output = data_evinput.drop((scale_list + etc_list + demo_list), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f8eba-34d6-4295-84d6-1aebffd7db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() #set the scaler (between 0 and 1)\n",
    "\n",
    "ev_input[:] = (scaler.fit_transform(ev_input[:])).round(decimals=6)\n",
    "ev_output[:] = (scaler.fit_transform(ev_output[:])).round(decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1b374-cb9d-45c0-8c38-7ca63d19cb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1262e1a5-4d6c-4164-a776-8424effcfbad",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20630384-6f8a-4c46-a35b-fc4be1a592d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = ev_input.shape[1]\n",
    "output_dim = ev_output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0462676-74d5-4870-ac0b-45fe11aca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 정의\n",
    "ev_model = Sequential([\n",
    "    Dense(16, input_dim=input_dim, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'), \n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(output_dim)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3b00b-9c03-434e-bfd7-b91755d06939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "ev_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 모델 요약 출력\n",
    "ev_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e2ee0-dce6-4392-9924-7ebd16348525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습 예시 (X_train은 인구학적 변수, Y_train은 생체신호 변수)\n",
    "ev_model.fit(ev_input, ev_output, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bd49b-ce63-457c-928e-75d78badf076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7a717-bdc0-47c4-b1b7-f5569a16e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2611ccce-6c6a-4265-a2ea-a1a4679ef50b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375ade6-f619-495f-971d-f0beded5e021",
   "metadata": {},
   "source": [
    "# Code practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28f88b-b33a-4352-96dd-c87e18fc3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_synthetic_data_distribution(original_data, synthetic_data):\n",
    "    # 원래 데이터의 평균과 분산을 계산\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_data)\n",
    "    \n",
    "    # 원래 데이터의 평균과 분산\n",
    "    original_mean = scaler.mean_\n",
    "    original_std = scaler.scale_\n",
    "    \n",
    "    # synthetic data의 평균과 분산을 계산\n",
    "    synthetic_scaler = StandardScaler()\n",
    "    synthetic_scaler.fit(synthetic_data)\n",
    "    \n",
    "    # synthetic data를 원래 데이터의 평균과 분산으로 조정\n",
    "    adjusted_synthetic_data = synthetic_scaler.transform(synthetic_data)\n",
    "    adjusted_synthetic_data = adjusted_synthetic_data * original_std + original_mean\n",
    "    \n",
    "    return adjusted_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d712e2f-03f8-45c7-9ac9-4478eef09479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df = (scaler.fit_transform(df)).round(decimals=6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5c842-2106-4d09-927a-e93ba95e98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x0 = data_scaler(data_x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82fb32-891f-4d97-af13-1278acfd3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x0 = data_MDD.drop(drop_list, axis=1)\n",
    "# data_x0[:] = (scaler.fit_transform(data_x0[:])).round(decimals=6) ## scaling x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d97db-d36f-4dec-beb2-f081ec3a6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_synthetic_data = adjust_synthetic_data_distribution(data_x0, synthetic_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a4026-24dd-4f07-8b7b-eaae059386b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03998662-f384-41ed-92e0-5856912966da",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b18293-bb6b-40ea-9840-47e8475c8f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f8fb6-1ef3-4ca7-bc48-f925663803a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a3a88-96ac-4503-834c-e53e284d7d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6516bd-0e74-4c32-a191-00939906d8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6d04d-4833-4f42-8913-3f8ab6beb17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37862d9-9178-4df1-a5ce-e07bf59e9394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dad44-e012-4172-8ba4-46d768057065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f59d64-f92b-4271-b93a-88a2c3c5b75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361b994-2fa3-4015-bb82-93c302d3fb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e416bc-f6e3-4501-bc04-12836c08826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d9e81-27d4-4805-a767-d87d82a20dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
